[
  {
    "objectID": "Chapter2e.html",
    "href": "Chapter2e.html",
    "title": "Chapter 2 (Exercises)",
    "section": "",
    "text": "When sample size \\(n\\) is extremely large, and \\(p\\) is small, a flexible statistical learning method will perform better than a non-flexible method, because\n\nThe low \\(p\\) allows us to avoid the curse of dimensionality.\nLarge number of \\(n\\) allows us to better predict with a flexible method, lowering the chance of overfitting.\n\n\n\n\nWhen the sample size \\(n\\) is very small, and number of predictors \\(p\\) is very large, a flexible statistical learning method will perform worse than a non-flexible one because a large number of predictors increase the , making it difficult to identify nearest-neighbors. A small number of observations also means that a highly flexible model to could lead to high variance, or overfitting.\n\n\n\nWhen the relationship between predictors and response is highly non-linear, a flexible model will perform much better because it can better allow for non-linear \\(f(x)\\), and thus will better approximate a highly non-linear relationship.\n\n\n\nWhen the variance of error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high, both flexible and non-flexible methods will lead to inaccurate predictions. However, comparatively, a non-flexible method will perform better as it is less likely to overfit or mis-read noise for actual relationship.\n\n\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. CEO Salary.\n\nHere, we are primarily interested in inference, rather than prediction, because we want to understand which factors affect CEO salary, rather than predicting the salary for any given CEO.\n\nIn this scenario, \\(n = 500\\) and \\(p = 3\\) .\n\n\n\n\n\nThis scenario is a classification problem, as the primary response is a qualitative variable (success vs. failure).\n\nHere, we are primarily interested in prediction because we wish to know the response our test data, i.e. new product being launched, rather than understanding the factors behind the response.\n\nIn this scenario, \\(n = 20\\) and \\(p = 13\\) .\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. percentage change in US dollar.\n\nHere, we are primarily interested in prediction instead of inference since our intended purpose to predict % change in US dollar. We are not concerned with the exact factors that cause such a change.\n\nIn this scenario, \\(n = 52\\) (i.e. number of weeks in 2012) and \\(p = 3\\) .\n\n\n\n\n\n\n\nThe sketch is displayed below: —\n \n\n\n\nIn this figure, the curve (C) is the Irreducible Error which is a feature of the data-set, and thus stays constant. The curve(D) represents the Training Error which will always decrease monotonously with increasing flexibility of the fitted model because almost all techniques directly or indirectly aim at reducing the MSE in the training data-set. As flexibility of the method increases, it’s variance will increase [curve (B)] and bias will decrease [curve(D)] because a flexible model is highly variable (change in a single observation will change the fit) and less biased (each observation is closely covered). The sum of curves B, C and E represents the Test Error i.e. Curve A.\n\n\n\n\n\n\nThree real-life applications in which classification is useful are: —\nEmail Spam\n1. Response: A qualitative binary indicator whether the email is spam or not spam.\n2. Predictors: Presence of common words in subject of the email (such as offer, lottery etc.), Presence of name of email account holder in text of email, Unverified attachments, Relative frequency of commonly used words.\n3. Goal of the application: Prediction.\n\nHandwriting recognition : ZIP codes on postal envelopes\n1. Response: Each of the 5 digits (0-9) - categorical outcome\n2. Predictors: A matrix corresponding to an image where each pixel corresponds is an entry in the matrix, and its pixel intensity ranges from 0 (black) to 255 (white).\n3. Goal of the application: Prediction\n\nFactors Affecting Consumer purchase amongst competing goods : Which factors most affect the consumers’ purchase of a particular product amongst competing brands.\n1. Response: Which one of the competing products does a consumer buy?\n2. Predictors: Area, demographics of buyer, advertising revenues etc.\n3. Goal of the application: Inference.\n\n\n\nThree real-life applications in which regression is useful are: —\n1. Predicting response to dosage of BP medication\ni) Response: Blood Pressure (in mmHg)\nii) Predictors: Dose of a given medication, age, vital indicators etc.\niii) Goal of the application: Prediction\n\n2. Factors affecting Crop Yields\ni) Response: Crop Yield in units per area\nii) Predictors: Amount of fertilizer applied, Soil pH, Temperature, Machines used, etc.\niii) Goal of the application: Both inference (which inputs yield maximum benefits) and prediction (expected crop yield in a particular season)\n\n3. Increasing operational efficiency in a production line by identifying low-hanging fruits\ni) Response: Factory output of a good\nii) Predictors: Various raw materials, number of workers, capital investment, working hours, etc.\niii) Goal of the application: Inference (spending on which input / predictor) will cause maximum increase in output.\n\n\n\nThree real-life applications in which cluster analysis is useful are: —\n1. Social Network Clusters: Identifying like-minded twitter users based on their activity.\n2. Finding Similar books for a book recommendation software based on past purchase data from users.\n3. Gene mapping in evolutionary biology to find clusters of similar genes within a genome / evolutionary lineage.\n\n\n\n\n\nA very flexible approach has following advantages and disadvantages :–\n\n\n\nAdvantages: Allows fitting to highly non-linear relationships. Better performance when \\(n\\) is large and \\(p\\) is small.\nDisadvantages: Requires estimation of a large number of parameters. Low level of interpretability. Can lead to overfitting. Does not perform well when \\(n\\) is very small or data is high-dimensional i.e. large \\(p\\).\n\n\nA more flexible approach may be considered when we have a large data (high \\(n\\)), low dimensionality (low \\(p\\)), or when we are mainly interested in prediction in a non-linear relationship.\nA less flexible approach is preferred when out objective is inference and interpretability of the results. (Or when data is high dimensional or low \\(n\\).)\n\n\n\n\nA parametric statistical learning approach assumes a \\(f(x)\\) i.e. it pre-supposes a functional form for relationship between predictors and response. On the other hand, a non-parametric approach does not assume any functional form for \\(f\\) at all. It just tries to best fit the available data. Thus, it requires a very large amount of data.\n\nParametric approach advantages:\n1) Easier to compute and evaluate\n2) Interpretation is simpler\n3) Better for inference tasks.\n4) Can be used with a low number of observations.\n\nParametric approach disadvantages:\n1) If the underlying relationship is far off from assumed functional form, the method will lead to high error rate.\n2) If too flexible a model fitted, it will lead to overfitting.\n3) Real-life relations are rarely simple functional forms.\n\n\n\n\n\n\ndata &lt;- read.csv(\"docs/ISLRCh2Ex2-4Q7.csv\") |&gt;\n  mutate(y = as.factor(y)) |&gt;\n  mutate(dist_000 = sqrt(x1^2 + x2^2 + x3^2))\ndata |&gt; kbl() |&gt; kable_classic_2(full_width = F)\n\n\n\n\nx1\nx2\nx3\ny\ndist_000\n\n\n\n\n0\n3\n0\nred\n3.000000\n\n\n2\n0\n0\nred\n2.000000\n\n\n0\n1\n3\nred\n3.162278\n\n\n0\n1\n2\ngreen\n2.236068\n\n\n-1\n0\n1\ngreen\n1.414214\n\n\n1\n1\n1\nred\n1.732051\n\n\n\n\n\n\n\nThe distance between each test point and the test point (0,0,0) is displayed above, and for the six points is\n3.00 2.00 3.16 2.24 1.41 1.73\n\n\n\nWith \\(K=1\\), our prediction for \\(y\\) is “green”, as the nearest neighbor is “green”.\n\n\n\nWith \\(K=3\\), our prediction for \\(y\\) is “red”, as the nearest neighbors are 1 “green” and 2 “red”.\n\n\n\nIf the Bayes Decision Boundary is highly non-linear, then we would expect the best value of \\(K\\) to be small, because a smaller \\(K\\) allows us more flexibility in the estimated decision boundary.\n\n\n\n\n\n\n\n\n\n\n\ndata(College)\ncollege &lt;- College\nrm(College)\n\n\n\n\n\nrownames(college) &lt;- college[,1]\nfix(college)\n\n\n\n\n\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\n\n\n\n\n\n\n\n\nAs per the summary() function, there are 78 elite universities. The box-plot of Outstate vs. Elite is as below :—\n\n\nwith(college, plot(x=Elite, y=Outstate, ylab = \"Out-of-State Tuition\", xlab = \"Whether Elite College?\", main = \"Question 8(c)(iv)\"))\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nhist(college$Room.Board, main = \"\", sub = \"Room Boarding Charges\")\nhist(college$Room.Board, breaks = 100, main=\"\", sub = \"Room Boarding Charges(100 bins)\")\nhist(college$Books, main = \"\", sub = \"Expenses on Books\")\nhist(college$Books, breaks = 100, main=\"\", sub = \"Expenses on Books\")\n\n\n\n\n\n\n\n\n\n\n\ndata(Auto)\nAuto &lt;- na.omit(Auto)\n\nThe predictors Origin and Name in the Auto data set are qualitative, while all the other variables are quantitative, i.e. mpg, cylinders, displacement, horsepower, weight, acceleration, year.\n\n\n\n\nMin &lt;- rep(0, 7)\nMax &lt;- rep(0, 7)\nStDev &lt;- rep(0, 7)\nMean &lt;- rep(0, 7)\nName &lt;- colnames(Auto)[-c(8, 9)]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n9\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1613\n5140\n\n\nacceleration\n8\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n23.45\n7.81\n\n\ncylinders\n5.47\n1.71\n\n\ndisplacement\n194.41\n104.64\n\n\nhorsepower\n104.47\n38.49\n\n\nweight\n2977.58\n849.4\n\n\nacceleration\n15.54\n2.76\n\n\nyear\n75.98\n3.68\n\n\n\n\n\n\n\n\n\n\n\nAuto1 &lt;- Auto[-c(10:85), ]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto1[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto1[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto1[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto1[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n11\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1649\n4997\n\n\nacceleration\n8.5\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n24.4\n7.87\n\n\ncylinders\n5.37\n1.65\n\n\ndisplacement\n187.24\n99.68\n\n\nhorsepower\n100.72\n35.71\n\n\nweight\n2935.97\n811.3\n\n\nacceleration\n15.73\n2.69\n\n\nyear\n77.15\n3.11\n\n\n\n\n\n\n\n\n\n\n\npairs(Auto[, c(1, 3, 4, 5, 6)])\n\n\n\nmodel &lt;- lm(mpg ~ . - name, data = Auto)\ncor &lt;- as.data.frame(model$coefficients[-1])\ncolnames(cor) &lt;- \"Coefficient of Regression\"\ncor |&gt;\n  kbl() |&gt;\n  kable_classic_2()\n\n\n\n\n\nCoefficient of Regression\n\n\n\n\ncylinders\n-0.4933763\n\n\ndisplacement\n0.0198956\n\n\nhorsepower\n-0.0169511\n\n\nweight\n-0.0064740\n\n\nacceleration\n0.0805758\n\n\nyear\n0.7507727\n\n\norigin\n1.4261405\n\n\n\n\n\n\n\nThe above table shows regression coefficients of other variables with mpg.\n\n\n\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n# VarDescrip &lt;- read.csv(\"BostonVarDescrip.csv\")\nnrow(Boston)\n\n[1] 506\n\nncol(Boston)\n\n[1] 14\n\n\nThere are total 506 rows in the data-set and 14 columns. The rows represent the 506 neighborhoods and each column represents the following :—\n\n# VarDescrip |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\nThe pairwise plots of some important variables is as below : —\n\nBoston1 &lt;- Boston |&gt; dplyr::select(crim, nox, dis, tax, lstat)\npairs(Boston1)\n\n\n\n\n\n\n\nThe linear regression shows us the following result :—\n\nmdl &lt;- lm(crim ~ ., data = Boston)\nsummary(mdl)\n\n\nCall:\nlm(formula = crim ~ ., data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.924 -2.120 -0.353  1.019 75.051 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.033228   7.234903   2.354 0.018949 *  \nzn            0.044855   0.018734   2.394 0.017025 *  \nindus        -0.063855   0.083407  -0.766 0.444294    \nchas         -0.749134   1.180147  -0.635 0.525867    \nnox         -10.313535   5.275536  -1.955 0.051152 .  \nrm            0.430131   0.612830   0.702 0.483089    \nage           0.001452   0.017925   0.081 0.935488    \ndis          -0.987176   0.281817  -3.503 0.000502 ***\nrad           0.588209   0.088049   6.680 6.46e-11 ***\ntax          -0.003780   0.005156  -0.733 0.463793    \nptratio      -0.271081   0.186450  -1.454 0.146611    \nblack        -0.007538   0.003673  -2.052 0.040702 *  \nlstat         0.126211   0.075725   1.667 0.096208 .  \nmedv         -0.198887   0.060516  -3.287 0.001087 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.439 on 492 degrees of freedom\nMultiple R-squared:  0.454, Adjusted R-squared:  0.4396 \nF-statistic: 31.47 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nThus, per capital crime rate is significantly related with the variables zn, dis, rad, black and medv.\n\n\n\nYes, there are some suburbs with abnormally high per-capita crime rate. But no such trend is visible in Tax-Rates or Pupil-Teacher Ratio. The plots of these three variables are shown below:—\n\nattach(Boston)\npar(mfrow=c(2,3))\nplot(crim)\nplot(tax)\nplot(ptratio)\nboxplot(crim)\nboxplot(tax)\nboxplot(ptratio)\n\n\n\n\n\n\n\n35 suburbs in this data set bound the Charles river. This is found using the code:—\n\nnrow(subset(Boston, chas == 1))\n\n[1] 35\n\n\n\n\n\nThe median pupil-teacher ratio is 19.05, found using the code:—\n\nmedian(Boston$ptratio)\n\n\n\n\nThe suburb of Boston with lowest median value of ownership occupied homes is 399. Its values for other variables are:—\n\nas.data.frame(Boston[which.min(Boston$medv),]) |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n399\n38.3518\n0\n18.1\n0\n0.693\n5.453\n100\n1.4896\n24\n666\n20.2\n396.9\n30.59\n5\n\n\n\n\n\n\n\n\n\n\nIn this data set, 13 suburbs average more than 8 rooms per dwelling and 64 suburbs average more than 7 rooms per dwelling. The code to find these values is :—\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;8))\n\n[1] 13\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;7))\n\n[1] 64"
  },
  {
    "objectID": "Chapter2e.html#conceptual",
    "href": "Chapter2e.html#conceptual",
    "title": "Chapter 2 (Exercises)",
    "section": "",
    "text": "When sample size \\(n\\) is extremely large, and \\(p\\) is small, a flexible statistical learning method will perform better than a non-flexible method, because\n\nThe low \\(p\\) allows us to avoid the curse of dimensionality.\nLarge number of \\(n\\) allows us to better predict with a flexible method, lowering the chance of overfitting.\n\n\n\n\nWhen the sample size \\(n\\) is very small, and number of predictors \\(p\\) is very large, a flexible statistical learning method will perform worse than a non-flexible one because a large number of predictors increase the , making it difficult to identify nearest-neighbors. A small number of observations also means that a highly flexible model to could lead to high variance, or overfitting.\n\n\n\nWhen the relationship between predictors and response is highly non-linear, a flexible model will perform much better because it can better allow for non-linear \\(f(x)\\), and thus will better approximate a highly non-linear relationship.\n\n\n\nWhen the variance of error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high, both flexible and non-flexible methods will lead to inaccurate predictions. However, comparatively, a non-flexible method will perform better as it is less likely to overfit or mis-read noise for actual relationship.\n\n\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. CEO Salary.\n\nHere, we are primarily interested in inference, rather than prediction, because we want to understand which factors affect CEO salary, rather than predicting the salary for any given CEO.\n\nIn this scenario, \\(n = 500\\) and \\(p = 3\\) .\n\n\n\n\n\nThis scenario is a classification problem, as the primary response is a qualitative variable (success vs. failure).\n\nHere, we are primarily interested in prediction because we wish to know the response our test data, i.e. new product being launched, rather than understanding the factors behind the response.\n\nIn this scenario, \\(n = 20\\) and \\(p = 13\\) .\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. percentage change in US dollar.\n\nHere, we are primarily interested in prediction instead of inference since our intended purpose to predict % change in US dollar. We are not concerned with the exact factors that cause such a change.\n\nIn this scenario, \\(n = 52\\) (i.e. number of weeks in 2012) and \\(p = 3\\) .\n\n\n\n\n\n\n\nThe sketch is displayed below: —\n \n\n\n\nIn this figure, the curve (C) is the Irreducible Error which is a feature of the data-set, and thus stays constant. The curve(D) represents the Training Error which will always decrease monotonously with increasing flexibility of the fitted model because almost all techniques directly or indirectly aim at reducing the MSE in the training data-set. As flexibility of the method increases, it’s variance will increase [curve (B)] and bias will decrease [curve(D)] because a flexible model is highly variable (change in a single observation will change the fit) and less biased (each observation is closely covered). The sum of curves B, C and E represents the Test Error i.e. Curve A.\n\n\n\n\n\n\nThree real-life applications in which classification is useful are: —\nEmail Spam\n1. Response: A qualitative binary indicator whether the email is spam or not spam.\n2. Predictors: Presence of common words in subject of the email (such as offer, lottery etc.), Presence of name of email account holder in text of email, Unverified attachments, Relative frequency of commonly used words.\n3. Goal of the application: Prediction.\n\nHandwriting recognition : ZIP codes on postal envelopes\n1. Response: Each of the 5 digits (0-9) - categorical outcome\n2. Predictors: A matrix corresponding to an image where each pixel corresponds is an entry in the matrix, and its pixel intensity ranges from 0 (black) to 255 (white).\n3. Goal of the application: Prediction\n\nFactors Affecting Consumer purchase amongst competing goods : Which factors most affect the consumers’ purchase of a particular product amongst competing brands.\n1. Response: Which one of the competing products does a consumer buy?\n2. Predictors: Area, demographics of buyer, advertising revenues etc.\n3. Goal of the application: Inference.\n\n\n\nThree real-life applications in which regression is useful are: —\n1. Predicting response to dosage of BP medication\ni) Response: Blood Pressure (in mmHg)\nii) Predictors: Dose of a given medication, age, vital indicators etc.\niii) Goal of the application: Prediction\n\n2. Factors affecting Crop Yields\ni) Response: Crop Yield in units per area\nii) Predictors: Amount of fertilizer applied, Soil pH, Temperature, Machines used, etc.\niii) Goal of the application: Both inference (which inputs yield maximum benefits) and prediction (expected crop yield in a particular season)\n\n3. Increasing operational efficiency in a production line by identifying low-hanging fruits\ni) Response: Factory output of a good\nii) Predictors: Various raw materials, number of workers, capital investment, working hours, etc.\niii) Goal of the application: Inference (spending on which input / predictor) will cause maximum increase in output.\n\n\n\nThree real-life applications in which cluster analysis is useful are: —\n1. Social Network Clusters: Identifying like-minded twitter users based on their activity.\n2. Finding Similar books for a book recommendation software based on past purchase data from users.\n3. Gene mapping in evolutionary biology to find clusters of similar genes within a genome / evolutionary lineage.\n\n\n\n\n\nA very flexible approach has following advantages and disadvantages :–\n\n\n\nAdvantages: Allows fitting to highly non-linear relationships. Better performance when \\(n\\) is large and \\(p\\) is small.\nDisadvantages: Requires estimation of a large number of parameters. Low level of interpretability. Can lead to overfitting. Does not perform well when \\(n\\) is very small or data is high-dimensional i.e. large \\(p\\).\n\n\nA more flexible approach may be considered when we have a large data (high \\(n\\)), low dimensionality (low \\(p\\)), or when we are mainly interested in prediction in a non-linear relationship.\nA less flexible approach is preferred when out objective is inference and interpretability of the results. (Or when data is high dimensional or low \\(n\\).)\n\n\n\n\nA parametric statistical learning approach assumes a \\(f(x)\\) i.e. it pre-supposes a functional form for relationship between predictors and response. On the other hand, a non-parametric approach does not assume any functional form for \\(f\\) at all. It just tries to best fit the available data. Thus, it requires a very large amount of data.\n\nParametric approach advantages:\n1) Easier to compute and evaluate\n2) Interpretation is simpler\n3) Better for inference tasks.\n4) Can be used with a low number of observations.\n\nParametric approach disadvantages:\n1) If the underlying relationship is far off from assumed functional form, the method will lead to high error rate.\n2) If too flexible a model fitted, it will lead to overfitting.\n3) Real-life relations are rarely simple functional forms.\n\n\n\n\n\n\ndata &lt;- read.csv(\"docs/ISLRCh2Ex2-4Q7.csv\") |&gt;\n  mutate(y = as.factor(y)) |&gt;\n  mutate(dist_000 = sqrt(x1^2 + x2^2 + x3^2))\ndata |&gt; kbl() |&gt; kable_classic_2(full_width = F)\n\n\n\n\nx1\nx2\nx3\ny\ndist_000\n\n\n\n\n0\n3\n0\nred\n3.000000\n\n\n2\n0\n0\nred\n2.000000\n\n\n0\n1\n3\nred\n3.162278\n\n\n0\n1\n2\ngreen\n2.236068\n\n\n-1\n0\n1\ngreen\n1.414214\n\n\n1\n1\n1\nred\n1.732051\n\n\n\n\n\n\n\nThe distance between each test point and the test point (0,0,0) is displayed above, and for the six points is\n3.00 2.00 3.16 2.24 1.41 1.73\n\n\n\nWith \\(K=1\\), our prediction for \\(y\\) is “green”, as the nearest neighbor is “green”.\n\n\n\nWith \\(K=3\\), our prediction for \\(y\\) is “red”, as the nearest neighbors are 1 “green” and 2 “red”.\n\n\n\nIf the Bayes Decision Boundary is highly non-linear, then we would expect the best value of \\(K\\) to be small, because a smaller \\(K\\) allows us more flexibility in the estimated decision boundary."
  },
  {
    "objectID": "Chapter2e.html#applied",
    "href": "Chapter2e.html#applied",
    "title": "Chapter 2 (Exercises)",
    "section": "",
    "text": "data(College)\ncollege &lt;- College\nrm(College)\n\n\n\n\n\nrownames(college) &lt;- college[,1]\nfix(college)\n\n\n\n\n\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\n\n\n\n\n\n\n\n\nAs per the summary() function, there are 78 elite universities. The box-plot of Outstate vs. Elite is as below :—\n\n\nwith(college, plot(x=Elite, y=Outstate, ylab = \"Out-of-State Tuition\", xlab = \"Whether Elite College?\", main = \"Question 8(c)(iv)\"))\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nhist(college$Room.Board, main = \"\", sub = \"Room Boarding Charges\")\nhist(college$Room.Board, breaks = 100, main=\"\", sub = \"Room Boarding Charges(100 bins)\")\nhist(college$Books, main = \"\", sub = \"Expenses on Books\")\nhist(college$Books, breaks = 100, main=\"\", sub = \"Expenses on Books\")\n\n\n\n\n\n\n\n\n\n\n\ndata(Auto)\nAuto &lt;- na.omit(Auto)\n\nThe predictors Origin and Name in the Auto data set are qualitative, while all the other variables are quantitative, i.e. mpg, cylinders, displacement, horsepower, weight, acceleration, year.\n\n\n\n\nMin &lt;- rep(0, 7)\nMax &lt;- rep(0, 7)\nStDev &lt;- rep(0, 7)\nMean &lt;- rep(0, 7)\nName &lt;- colnames(Auto)[-c(8, 9)]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n9\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1613\n5140\n\n\nacceleration\n8\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n23.45\n7.81\n\n\ncylinders\n5.47\n1.71\n\n\ndisplacement\n194.41\n104.64\n\n\nhorsepower\n104.47\n38.49\n\n\nweight\n2977.58\n849.4\n\n\nacceleration\n15.54\n2.76\n\n\nyear\n75.98\n3.68\n\n\n\n\n\n\n\n\n\n\n\nAuto1 &lt;- Auto[-c(10:85), ]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto1[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto1[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto1[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto1[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n11\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1649\n4997\n\n\nacceleration\n8.5\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n24.4\n7.87\n\n\ncylinders\n5.37\n1.65\n\n\ndisplacement\n187.24\n99.68\n\n\nhorsepower\n100.72\n35.71\n\n\nweight\n2935.97\n811.3\n\n\nacceleration\n15.73\n2.69\n\n\nyear\n77.15\n3.11\n\n\n\n\n\n\n\n\n\n\n\npairs(Auto[, c(1, 3, 4, 5, 6)])\n\n\n\nmodel &lt;- lm(mpg ~ . - name, data = Auto)\ncor &lt;- as.data.frame(model$coefficients[-1])\ncolnames(cor) &lt;- \"Coefficient of Regression\"\ncor |&gt;\n  kbl() |&gt;\n  kable_classic_2()\n\n\n\n\n\nCoefficient of Regression\n\n\n\n\ncylinders\n-0.4933763\n\n\ndisplacement\n0.0198956\n\n\nhorsepower\n-0.0169511\n\n\nweight\n-0.0064740\n\n\nacceleration\n0.0805758\n\n\nyear\n0.7507727\n\n\norigin\n1.4261405\n\n\n\n\n\n\n\nThe above table shows regression coefficients of other variables with mpg.\n\n\n\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n# VarDescrip &lt;- read.csv(\"BostonVarDescrip.csv\")\nnrow(Boston)\n\n[1] 506\n\nncol(Boston)\n\n[1] 14\n\n\nThere are total 506 rows in the data-set and 14 columns. The rows represent the 506 neighborhoods and each column represents the following :—\n\n# VarDescrip |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\nThe pairwise plots of some important variables is as below : —\n\nBoston1 &lt;- Boston |&gt; dplyr::select(crim, nox, dis, tax, lstat)\npairs(Boston1)\n\n\n\n\n\n\n\nThe linear regression shows us the following result :—\n\nmdl &lt;- lm(crim ~ ., data = Boston)\nsummary(mdl)\n\n\nCall:\nlm(formula = crim ~ ., data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.924 -2.120 -0.353  1.019 75.051 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.033228   7.234903   2.354 0.018949 *  \nzn            0.044855   0.018734   2.394 0.017025 *  \nindus        -0.063855   0.083407  -0.766 0.444294    \nchas         -0.749134   1.180147  -0.635 0.525867    \nnox         -10.313535   5.275536  -1.955 0.051152 .  \nrm            0.430131   0.612830   0.702 0.483089    \nage           0.001452   0.017925   0.081 0.935488    \ndis          -0.987176   0.281817  -3.503 0.000502 ***\nrad           0.588209   0.088049   6.680 6.46e-11 ***\ntax          -0.003780   0.005156  -0.733 0.463793    \nptratio      -0.271081   0.186450  -1.454 0.146611    \nblack        -0.007538   0.003673  -2.052 0.040702 *  \nlstat         0.126211   0.075725   1.667 0.096208 .  \nmedv         -0.198887   0.060516  -3.287 0.001087 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.439 on 492 degrees of freedom\nMultiple R-squared:  0.454, Adjusted R-squared:  0.4396 \nF-statistic: 31.47 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nThus, per capital crime rate is significantly related with the variables zn, dis, rad, black and medv.\n\n\n\nYes, there are some suburbs with abnormally high per-capita crime rate. But no such trend is visible in Tax-Rates or Pupil-Teacher Ratio. The plots of these three variables are shown below:—\n\nattach(Boston)\npar(mfrow=c(2,3))\nplot(crim)\nplot(tax)\nplot(ptratio)\nboxplot(crim)\nboxplot(tax)\nboxplot(ptratio)\n\n\n\n\n\n\n\n35 suburbs in this data set bound the Charles river. This is found using the code:—\n\nnrow(subset(Boston, chas == 1))\n\n[1] 35\n\n\n\n\n\nThe median pupil-teacher ratio is 19.05, found using the code:—\n\nmedian(Boston$ptratio)\n\n\n\n\nThe suburb of Boston with lowest median value of ownership occupied homes is 399. Its values for other variables are:—\n\nas.data.frame(Boston[which.min(Boston$medv),]) |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n399\n38.3518\n0\n18.1\n0\n0.693\n5.453\n100\n1.4896\n24\n666\n20.2\n396.9\n30.59\n5\n\n\n\n\n\n\n\n\n\n\nIn this data set, 13 suburbs average more than 8 rooms per dwelling and 64 suburbs average more than 7 rooms per dwelling. The code to find these values is :—\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;8))\n\n[1] 13\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;7))\n\n[1] 64"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my corner of the web! I’m Aditya Dahiya, an IAS (Indian Administrative Service) officer navigating the intricate labyrinth of the Indian Administrative Service in the Government of Haryana. While my day job might involve more paperwork than I care to admit, my true passion lies in the world of health data visualization.\nMy journey began with a stethoscope around my neck and a medical degree from the AIIMS, New Delhi, earned from 2005 to 2010. But in 2011, I decided to swap the scrubs for a suit and tie and jumped headfirst into the world of civil service. Fast forward to 2021-22, and I found myself on the hallowed grounds of Harvard University, where I earned an MPH degree, because why not?\nNow, you might wonder how someone with a day job in bureaucracy and a penchant for health data visualization ended up creating solutions for the first edition of “An Introduction to Statistical Learning: With Applications in R.” Well, my secret lies in seizing those precious moments of free time. And during one such moment, I embarked on a journey of statistical enlightenment by enrolling in Johns Hopkins University’s 140.644.01 - Statistical Machine Learning course.\nThe result? A treasure trove of solutions that I’m delighted to share with you. Whether you’re a fellow learner, an aspiring statistician, or just a curious soul, this website is your go-to resource for unraveling the mysteries of statistical learning, sprinkled with a dash of wit and a pinch of my unique perspective.\nSo, join me on this data-driven adventure at LinkedIn or shoot me an email. Even bureaucrats can have a little statistical swagger!"
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nChapter 2 (Exercises)\n\n\nStatistical Learning\n\n\nJan 31, 2021\n\n\n\n\nChapter 3 (Exercises)\n\n\nLinear Regression\n\n\nFeb 1, 2021\n\n\n\n\nChapter 3 (R Lab)\n\n\nLinear Regression (Lab)\n\n\nFeb 1, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Solutions: ISLR 1e",
    "section": "",
    "text": "Author: Aditya Dahiya\nThis website displays the solutions for the exercises in the book An Introduction to Statistical Learning: with Applications in R (1st Edition)."
  },
  {
    "objectID": "index.html#an-introduction-to-statistical-learning-with-applications-in-r-1st-edition",
    "href": "index.html#an-introduction-to-statistical-learning-with-applications-in-r-1st-edition",
    "title": "Solutions: ISLR 1e",
    "section": "",
    "text": "Author: Aditya Dahiya\nThis website displays the solutions for the exercises in the book An Introduction to Statistical Learning: with Applications in R (1st Edition)."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nHarvard University | Boston, MA MPH in Global Health | Aug 2021 - May 2022\nAll India Institute of Medical Sciences | New Delhi, India MBBS in Medicine | Aug 2005-Dec 2010\nIndira Gandhi National Open University | New Delhi, India MA in Public Policy | Jan 2012 - Dec 2013"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nIndian Administrative Service | Director | Aug 2011 - present\nNeuro-Radiology, AIIMS New Delhi | Junior Resident Doctor | Jan 2011 - Aug 2011"
  },
  {
    "objectID": "Chapter3e.html",
    "href": "Chapter3e.html",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The figure is as below:—\n\n\n\n\nThe p-values in this table refer to the null hypothesis \\(H_o : \\beta_i = 0\\), where \\(\\beta_i\\) refers to the Coefficient relating to the concerned variable.\nBased on these p-values, I conclude that TV and radio are significant predictors of variation in sales, whereas newspaper is not a statistically significant predictor of variation in sales.\n\n\n\n\n\n\nKNN Classifier is a method used for modeling categorical or qualitative responses. Here, the classifier returns the category of response which is most likely, given the values of predictors in \\(N_o\\) i.e. K-nearest neighbors. KNN classifier first identifies the K points in the training data that are closest to \\(x_o\\), represented by \\(\\mathcal{N}_o\\). It then estimates the conditional probability for class \\(j\\) as the fraction of points in \\(\\mathcal{N}_o\\) whose response values equal \\(j\\)\n\\[Pr(Y = j |x=x_o) =  \\frac{1}{K} \\sum_{x_i \\in N_o} I(y_i = j)\\]\nFinally, KNN applies Bayes rule and classifies the test observation \\(x_o\\) to the class with the largest probability. - KNN Regression is a method which models continuous outcomes or responses based on predictors. It calculates the predicted outcome as the average of responses for the K-nearest neighbors of the given \\(x_o\\), i.e. :—\n\\[\\hat{f}(x_o) = \\frac{1}{K} \\sum_{x_i \\in N_o} y_i \\] \n\n\n\n\n\n\n\nThis statement is incorrect because coefficient \\(\\hat{\\beta}_3\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0)\n\nThis statement is incorrect because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0), but only if the GPA is less than 3.5. This is because the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\). Thus above a certain value, males will have higher earnings.\n\nThis statement is correct because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive but the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\), meaning thereby that females (coded as 1) earn more than males (coded as 0) only up to a certain GPA level, above which the males earn more.\n\nThis statement is incorrect for reasons explained above in (iii).\n\n\n\n\nThe salary of a female with IQ of 110 and a GPA of 4.0 is 137.1, as calculated below :–\n\n\\(Salary = \\beta_0 + (\\beta_1*4) + (\\beta_2 * 110) + (\\beta_3*1) + (\\beta_4*110*4) + (\\beta_5*4*1)\\)\n\n\\(Salary = 50 + (20*4) + (0.07 * 110) + (35*1) + (0.01*110*4) + (-10*4*1)\\)\n\n\\(Salary = 137.1\\)\n\n\n\n\nFalse. The magnitude of a coefficient does not tell us anything about the significance of the interaction amongst GPA and IQ. This is because the magnitude / value of a coefficient depends on the units of predictor variables chosen. Rather, the standard error and \\(p\\)-value associated with the coefficient will tell us whether there is any evidence of an interaction or not.\n\n\n\n\n\n\n\nThe Training Residual Sum of Squares (RSS) for the cubic regression will always be lower than the simple linear regression, even if the true population model is linear i.e. \\(Y = \\beta_o + \\beta_1(X) + \\epsilon\\). This is because more terms in the cubic regression provide more flexibility and thus allows the model to fit each data point more closely. Thus, the RSS is reduced. This is not good thing for us though, as the cubic regression will lead to overfitting.\n\n\n\n\nFor the Test Residual Sum of Squares, the simple linear regression will have a lower RSS than cubin regression. This happens because our true population model is linear, so cubic regression leads to overfitting. This causes Test RSS to rise, as compared to linear regression.\n\n\n\n\nEven if we do not know the form of true population relation between \\(Y\\) and \\(x\\), the cubic regression will always have a lower Training RSS. This is because the cubic regression is a more flexible model, and thus always fits the training data more closely leading to a lower Training RSS.\n\n\n\n\nFor the Test RSS, when the form true relationship between \\(Y\\) and \\(x\\) is unknown, we cannot predict whether the linear regression or cubic regression will perform better. This will be an empirical question. Which of the two models has a lower Test RSS will depend on the exact relation between \\(Y\\) and \\(x\\), and how closely it is modeled by the linear or cubic regression.\n\n\n\n\nIn this question, we are considering the fitted values from a linear regression without an intercept, where\n\\[ \\hat{y_i} = x_i \\hat{\\beta_i} \\]\n\nand,\n\n\\[\\hat{\\beta}_{i} = \\frac{\\sum_{i=1}^{n}x_{i} y_{i}}{\\sum_{i'=1}^{n} x_{i'}^{2}}\\]\n\nThus, we can easily replace the value of \\(\\hat{\\beta_i}\\) in the equation of \\(y_i\\) and solve for \\(a_i\\) as follows —\n\\[ \\hat{y_i} = x_i \\times \\hat{\\beta_i} \\] \\[ \\hat{y}_{i} = x_{i} \\times \\frac{\\sum_{i'=1}^{n} x_{i'} y_{i'}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\frac{ x_{i'} y_{i'} \\times x_{i}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\left ( \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\times y_{i'} \\right ) \\]\nThus, we can easily re-write the expression as\n\\[ \\hat{y_i} = \\sum_{i'=1}^{n} a_{i'}y_i  \\]\nAs a result, the value of \\(a_i\\) is —\n\\[ a_{i'} = \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\]\nAlso, this means that we can interpret this result by saying that the fitted values (\\(\\hat{y_i}\\)) from a linear regression are linear combination of the response values (\\(y_{i'}\\)).\n\n\n\nThe least squares regression line is\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i \\]\nwhere, \\(beta_o\\) and \\(beta_1\\) are defined as —\n\\[\\hat{\\beta_o} = \\overline{y} - \\hat{\\beta_1} \\overline{x}\\] \\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\overline{x}) (y_{i}-\\overline{y})}{\\sum_{i=1}^{n} (x_{i}-\\overline{x})^{2}}\\]\nNow, we need to prove that when \\(x_i = \\overline{x}\\), then \\(\\hat{y_i} = \\overline{y}\\). For this, we substitute the value of \\(\\beta_o\\) with \\(\\overline{y} - \\hat{\\beta_1} \\overline{x}\\) and set the value of \\(x_i\\) as \\(\\overline{x}\\),\n\\[ \\hat{y_i} = \\hat{\\beta_o} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline{y} - \\hat{\\beta_1} \\overline{x} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline {y}\\]\nThus, the point (\\(\\overline{x},\\overline{y}\\)) will always lie on the least squares regression line.\n\n\n\n\nKeeping the values of \\(\\overline{x}\\) and \\(\\overline{y}\\) as 0, the terms \\(R^2\\) and \\(Cor(X,Y)\\) are defined as follows \\[ R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum(y_j-\\overline{y})^2} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum{y_j}^2}\\]\n\\[ Cor(X,Y) = \\frac{\\sum_{n}(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum{x_j} \\sum{y_j}} = \\frac{\\sum_{n}x_iy_i}{\\sum{x_j} \\sum{y_j}}\\]\nNow, keeping \\(\\overline{y}\\) and \\(\\overline{x}\\) as zero, we can substitute the value of \\(\\hat{y_i}\\) in \\(R^2\\) as:—\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i = 0+ \\frac{\\sum(x_{i} - 0) (y_{i}-0)}{\\sum(x_{i}-0)^{2}} = \\frac{\\sum{x_iy_i}}{\\sum{x_i}^2}\\]\nThus, we have the following expression for \\(R^2\\), \\[R^2 = 1 - \\frac{\\sum_i(y_i - \\sum_jx_jy_j/\\sum_jx_j^2 x_i)^2}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{\\sum_jy_j^2 - (\\sum_iy_i^2 - 2\\sum_iy_i(\\sum_jx_jy_j/\\sum_jx_j^2)x_i + \\sum_i(\\sum_jx_jy_j/\\sum_jx_j^2)^2x_i^2)}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{2(\\sum_ix_iy_i)^2/\\sum_jx_j^2 - (\\sum_ix_iy_i)^2/\\sum_jx_j^2}{\\sum_jy_j^2}\\] \\[ R^2 = \\frac{(\\sum_ix_iy_i)^2}{\\sum_jx_j^2\\sum_jy_j^2} = Cor(X, Y)^2.\\]"
  },
  {
    "objectID": "Chapter3e.html#question-1",
    "href": "Chapter3e.html#question-1",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The figure is as below:—\n\n\n\n\nThe p-values in this table refer to the null hypothesis \\(H_o : \\beta_i = 0\\), where \\(\\beta_i\\) refers to the Coefficient relating to the concerned variable.\nBased on these p-values, I conclude that TV and radio are significant predictors of variation in sales, whereas newspaper is not a statistically significant predictor of variation in sales."
  },
  {
    "objectID": "Chapter3e.html#question-2",
    "href": "Chapter3e.html#question-2",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "KNN Classifier is a method used for modeling categorical or qualitative responses. Here, the classifier returns the category of response which is most likely, given the values of predictors in \\(N_o\\) i.e. K-nearest neighbors. KNN classifier first identifies the K points in the training data that are closest to \\(x_o\\), represented by \\(\\mathcal{N}_o\\). It then estimates the conditional probability for class \\(j\\) as the fraction of points in \\(\\mathcal{N}_o\\) whose response values equal \\(j\\)\n\\[Pr(Y = j |x=x_o) =  \\frac{1}{K} \\sum_{x_i \\in N_o} I(y_i = j)\\]\nFinally, KNN applies Bayes rule and classifies the test observation \\(x_o\\) to the class with the largest probability. - KNN Regression is a method which models continuous outcomes or responses based on predictors. It calculates the predicted outcome as the average of responses for the K-nearest neighbors of the given \\(x_o\\), i.e. :—\n\\[\\hat{f}(x_o) = \\frac{1}{K} \\sum_{x_i \\in N_o} y_i \\]"
  },
  {
    "objectID": "Chapter3e.html#question-3",
    "href": "Chapter3e.html#question-3",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "This statement is incorrect because coefficient \\(\\hat{\\beta}_3\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0)\n\nThis statement is incorrect because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0), but only if the GPA is less than 3.5. This is because the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\). Thus above a certain value, males will have higher earnings.\n\nThis statement is correct because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive but the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\), meaning thereby that females (coded as 1) earn more than males (coded as 0) only up to a certain GPA level, above which the males earn more.\n\nThis statement is incorrect for reasons explained above in (iii).\n\n\n\n\nThe salary of a female with IQ of 110 and a GPA of 4.0 is 137.1, as calculated below :–\n\n\\(Salary = \\beta_0 + (\\beta_1*4) + (\\beta_2 * 110) + (\\beta_3*1) + (\\beta_4*110*4) + (\\beta_5*4*1)\\)\n\n\\(Salary = 50 + (20*4) + (0.07 * 110) + (35*1) + (0.01*110*4) + (-10*4*1)\\)\n\n\\(Salary = 137.1\\)\n\n\n\n\nFalse. The magnitude of a coefficient does not tell us anything about the significance of the interaction amongst GPA and IQ. This is because the magnitude / value of a coefficient depends on the units of predictor variables chosen. Rather, the standard error and \\(p\\)-value associated with the coefficient will tell us whether there is any evidence of an interaction or not."
  },
  {
    "objectID": "Chapter3e.html#question-4",
    "href": "Chapter3e.html#question-4",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The Training Residual Sum of Squares (RSS) for the cubic regression will always be lower than the simple linear regression, even if the true population model is linear i.e. \\(Y = \\beta_o + \\beta_1(X) + \\epsilon\\). This is because more terms in the cubic regression provide more flexibility and thus allows the model to fit each data point more closely. Thus, the RSS is reduced. This is not good thing for us though, as the cubic regression will lead to overfitting.\n\n\n\n\nFor the Test Residual Sum of Squares, the simple linear regression will have a lower RSS than cubin regression. This happens because our true population model is linear, so cubic regression leads to overfitting. This causes Test RSS to rise, as compared to linear regression.\n\n\n\n\nEven if we do not know the form of true population relation between \\(Y\\) and \\(x\\), the cubic regression will always have a lower Training RSS. This is because the cubic regression is a more flexible model, and thus always fits the training data more closely leading to a lower Training RSS.\n\n\n\n\nFor the Test RSS, when the form true relationship between \\(Y\\) and \\(x\\) is unknown, we cannot predict whether the linear regression or cubic regression will perform better. This will be an empirical question. Which of the two models has a lower Test RSS will depend on the exact relation between \\(Y\\) and \\(x\\), and how closely it is modeled by the linear or cubic regression."
  },
  {
    "objectID": "Chapter3e.html#question-5",
    "href": "Chapter3e.html#question-5",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "In this question, we are considering the fitted values from a linear regression without an intercept, where\n\\[ \\hat{y_i} = x_i \\hat{\\beta_i} \\]\n\nand,\n\n\\[\\hat{\\beta}_{i} = \\frac{\\sum_{i=1}^{n}x_{i} y_{i}}{\\sum_{i'=1}^{n} x_{i'}^{2}}\\]\n\nThus, we can easily replace the value of \\(\\hat{\\beta_i}\\) in the equation of \\(y_i\\) and solve for \\(a_i\\) as follows —\n\\[ \\hat{y_i} = x_i \\times \\hat{\\beta_i} \\] \\[ \\hat{y}_{i} = x_{i} \\times \\frac{\\sum_{i'=1}^{n} x_{i'} y_{i'}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\frac{ x_{i'} y_{i'} \\times x_{i}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\left ( \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\times y_{i'} \\right ) \\]\nThus, we can easily re-write the expression as\n\\[ \\hat{y_i} = \\sum_{i'=1}^{n} a_{i'}y_i  \\]\nAs a result, the value of \\(a_i\\) is —\n\\[ a_{i'} = \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\]\nAlso, this means that we can interpret this result by saying that the fitted values (\\(\\hat{y_i}\\)) from a linear regression are linear combination of the response values (\\(y_{i'}\\))."
  },
  {
    "objectID": "Chapter3e.html#question-6",
    "href": "Chapter3e.html#question-6",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The least squares regression line is\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i \\]\nwhere, \\(beta_o\\) and \\(beta_1\\) are defined as —\n\\[\\hat{\\beta_o} = \\overline{y} - \\hat{\\beta_1} \\overline{x}\\] \\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\overline{x}) (y_{i}-\\overline{y})}{\\sum_{i=1}^{n} (x_{i}-\\overline{x})^{2}}\\]\nNow, we need to prove that when \\(x_i = \\overline{x}\\), then \\(\\hat{y_i} = \\overline{y}\\). For this, we substitute the value of \\(\\beta_o\\) with \\(\\overline{y} - \\hat{\\beta_1} \\overline{x}\\) and set the value of \\(x_i\\) as \\(\\overline{x}\\),\n\\[ \\hat{y_i} = \\hat{\\beta_o} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline{y} - \\hat{\\beta_1} \\overline{x} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline {y}\\]\nThus, the point (\\(\\overline{x},\\overline{y}\\)) will always lie on the least squares regression line."
  },
  {
    "objectID": "Chapter3e.html#question-7",
    "href": "Chapter3e.html#question-7",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "Keeping the values of \\(\\overline{x}\\) and \\(\\overline{y}\\) as 0, the terms \\(R^2\\) and \\(Cor(X,Y)\\) are defined as follows \\[ R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum(y_j-\\overline{y})^2} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum{y_j}^2}\\]\n\\[ Cor(X,Y) = \\frac{\\sum_{n}(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum{x_j} \\sum{y_j}} = \\frac{\\sum_{n}x_iy_i}{\\sum{x_j} \\sum{y_j}}\\]\nNow, keeping \\(\\overline{y}\\) and \\(\\overline{x}\\) as zero, we can substitute the value of \\(\\hat{y_i}\\) in \\(R^2\\) as:—\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i = 0+ \\frac{\\sum(x_{i} - 0) (y_{i}-0)}{\\sum(x_{i}-0)^{2}} = \\frac{\\sum{x_iy_i}}{\\sum{x_i}^2}\\]\nThus, we have the following expression for \\(R^2\\), \\[R^2 = 1 - \\frac{\\sum_i(y_i - \\sum_jx_jy_j/\\sum_jx_j^2 x_i)^2}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{\\sum_jy_j^2 - (\\sum_iy_i^2 - 2\\sum_iy_i(\\sum_jx_jy_j/\\sum_jx_j^2)x_i + \\sum_i(\\sum_jx_jy_j/\\sum_jx_j^2)^2x_i^2)}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{2(\\sum_ix_iy_i)^2/\\sum_jx_j^2 - (\\sum_ix_iy_i)^2/\\sum_jx_j^2}{\\sum_jy_j^2}\\] \\[ R^2 = \\frac{(\\sum_ix_iy_i)^2}{\\sum_jx_j^2\\sum_jy_j^2} = Cor(X, Y)^2.\\]"
  },
  {
    "objectID": "Chapter3e.html#question-8",
    "href": "Chapter3e.html#question-8",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 8",
    "text": "Question 8\n\n(a)\n\nlibrary(ISLR)\ndata(Auto)\nfit1 &lt;- lm(mpg ~ horsepower, data = Auto)\nsummary(fit1)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n\n\nThere is a relationship between response and predictor, as the overall F-statistic and t-value for the predictor horsepower are statistically significant. The p-value is very close to zero.\n\nThe strength of the relationship between the response mpg and predictor horsepower is given by the coefficient of regression i.e. -0.16, with a 95% confidence interval of [-0.17, -0.15].\n\nThe relation between the response and predictor is negative, as the coefficient for horsepower is negative, i.e. -0.16.\n\nThe predicted mpg for a horsepower value of 98, with the confidence and prediction intervals is as follows:\n\n\n\nround(predict(fit1, data.frame(horsepower = c(98)), interval = \"confidence\"), digits = 3)\n\n     fit    lwr    upr\n1 24.467 23.973 24.961\n\nround(predict(fit1, data.frame(horsepower = c(98)), interval = \"prediction\"), digits = 3)\n\n     fit    lwr    upr\n1 24.467 14.809 34.125\n\n\n\n\n(b)\n\nplot(x = Auto$horsepower, y = Auto$mpg)\nabline(fit1, lwd = 3, col = \"red\")\n\n\n\n\n\n\n(c)\nThe diagnostic plots, along with the required code to produce them, are displayed below. The Residuals vs Fitted plot shows that the residuals are not evenly distributed, and thus there is a non-linear relation between the response and predictor. The Normal Q-Q plot also shows evidence on whether the residuals are normal distributed. The current evidence shows that the distribution is somewhat normal. The Spread-Location plot of \\(\\sqrt{standardized \\ residuals}\\) vs. fitted values shows the spread of residuals along the range of predictor values, allowing us to test homoskedasticity. Here, the model fit1 suffers from heteroskedasticity. The last plot of Residuals vs. Leverage shows us whether any observations are high-leverage. Here, none of the observations appear outside the dashed lines and thus there is no influential observation which is altering the results of the regression model.\n\npar(mfrow = c(2, 2))\nplot(fit1)\n\n\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "Chapter3e.html#question-9",
    "href": "Chapter3e.html#question-9",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 9",
    "text": "Question 9\n\n(a)\nA scatter-plot matrix of all the variables in Auto data set is produced using the function pairs() as follows :—\n\n\nlibrary(ISLR)\ndata(Auto)\npairs(Auto)\n\n\n\n\n\n\n(b)\nThe correlation matrix between variables of Auto is shown below, after excluding the variable name. We can use the base R command cor. However, a more beautiful tabular display is shown below using the package corrplot.\n\ncor(Auto[, -9])\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\nlibrary(kableExtra)\nlibrary(corrplot)\ncorrplot(cor(Auto[, -9]), method = \"number\", tl.col = \"black\", sig.level = 0.05)\n\n\n\n\n\n\n(c)\nThe multiple linear regression of mpg on all other variables, except name is displayed below:\n\n\nfit2 &lt;- lm(mpg ~ . - name, data = Auto)\nsummary(fit2)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  &lt; 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  &lt; 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe output shows that there is a relationship between response and predictors. The F-statistic of 252.43 (generated by summary(fit2)$fstatistic[1]) has a p-value of nearly zero.\n\nThe predictors which appear to have a statistically significant relationship to the response are displacement, weight, year and origin.\n\nThe coefficient for year is 0.751 (round(fit2$coef[7],3)). This suggests that with each year, the value of miles per gallon mpg increases by 0.75, provided all other variables remain constant. In other words, with each year, fuel efficiency increases by 0.75 [95% CI = 0.65, 0.85] mpg/year if other variables remain the same.\n\n\n\n\n(d)\nThe diagnostic plots for this multiple linear regression are shown below.\n\n\npar(mfrow = c(2, 2))\nplot(fit2)\n\n\n\n\n\nResiduals vs Fitted Plot shows whether the residuals from multiple linear regression have non-linear patterns. We find that the residuals are not evenly spread around the horizontal line. Perhaps there is some non-linear relationship between the response and predictors, which is not captured by the linear model we have fitted.\nNormal Q-Q Plot shows us whether the residuals are normally distributed. Since, residuals follow the straight line when plotted with theoretical quantiles of a normal distribution, this plot suggests that the residuals are somewhat normally distributed, barring a few observations with high residual values.\nScale-Location Plot checks the assumption of homoskedasticity. The plot suggests some amount of heteroskedasticity.\nResiduals vs Leverage Plot helps us to find influential cases with high leverage and outliers (high absolute standardized residual value). In this data set, around 5 observations like 323, 326, 327, 394 etc. are outliers and observation 14 has a high leverage. But, there is no observation outside of the dashed line, the Cook’s distance, thus there is no observation which is significantly affecting the regression results.\n\n\n\n\n(e)\nTo fit various interaction terms in the Multiple linear regression model, we use two strategies :—\n\n\nFirst, we use three interactions to fit to the model, using the three variable pairs which carry the highest correlation from the correlation matrix shown in part (b) above: \\(displacement \\times cylinders\\) , \\(displacement \\times weight\\) and \\(weight \\times cylinders\\).\nThe results below suggest that only the interaction between displacement and weight is statistically significant. However, using predictor variables with high correlation means that our fitted model may suffer from multicollinearity.\n\n\n\nfit3 &lt;- lm(mpg ~ . - name + displacement:cylinders + displacement:weight + weight:cylinders, data = Auto)\nsummary(fit3)\n\n\nCall:\nlm(formula = mpg ~ . - name + displacement:cylinders + displacement:weight + \n    weight:cylinders, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9771 -1.8142 -0.0589  1.6557 12.1181 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -6.830e+00  6.090e+00  -1.122 0.262778    \ncylinders               5.961e-01  1.536e+00   0.388 0.698109    \ndisplacement           -8.595e-02  3.140e-02  -2.738 0.006480 ** \nhorsepower             -3.552e-02  1.319e-02  -2.693 0.007404 ** \nweight                 -9.252e-03  2.355e-03  -3.929 0.000101 ***\nacceleration            6.027e-02  8.905e-02   0.677 0.498978    \nyear                    7.831e-01  4.574e-02  17.120  &lt; 2e-16 ***\norigin                  5.193e-01  2.706e-01   1.919 0.055685 .  \ncylinders:displacement  1.923e-03  3.094e-03   0.622 0.534615    \ndisplacement:weight     2.386e-05  6.162e-06   3.872 0.000127 ***\ncylinders:weight       -2.515e-04  5.047e-04  -0.498 0.618481    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.97 on 381 degrees of freedom\nMultiple R-squared:  0.8589,    Adjusted R-squared:  0.8552 \nF-statistic:   232 on 10 and 381 DF,  p-value: &lt; 2.2e-16\n\n\n\nSecondly, we can create a lot of permutations of variables to create various interaction terms and use R code to select the interaction terms with significant p-values (&lt;0.05). This is shown below:—\n\n\n\nfit4 &lt;- lm(mpg ~ cylinders * displacement + cylinders * horsepower + cylinders * weight + cylinders * acceleration + cylinders * year + cylinders * origin + displacement * horsepower + displacement * weight + displacement * acceleration + displacement * year + displacement * origin + horsepower * weight + horsepower * acceleration + horsepower * year + horsepower * origin + weight * acceleration + weight * year + weight * origin + acceleration * year + acceleration * origin + year * origin, data = Auto)\n\nround(subset(summary(fit4)$coef[, 4], summary(fit4)$coef[, 4] &lt; 0.05), digits = 4)\n\n       displacement        acceleration              origin   displacement:year \n             0.0119              0.0074              0.0034              0.0135 \n  acceleration:year acceleration:origin \n             0.0303              0.0037 \n\n\n\nThus, as we see above, the interaction terms \\(displacement \\times year\\), \\(acceleration \\times year\\) and \\(acceleration \\times origin\\) are statistically significant predictors of the response mpg.\nIt is evident from above two approaches, that the exact model we use decides which interaction terms are statistically significant predictors of the outcome. Thus, interaction terms should be chosen on theoretical basis for interaction effects and then fitted into the model.\n\n\n\n\n(f)\nNow, we fit different types of models using 3 transformations of the predictor and response variables. We use only one predictor variable horsepower for ease of demonstration. The response variable continues to be mpg. We compare the models using plots with the fitted regression line, along with value of \\(R^2\\) in the top of the graph in red font. The plots how that the log transformation of the predictor variable leads to the best fit and highest \\(R^2\\).\n\n\nfit5 &lt;- lm(mpg ~ horsepower, data = Auto)\nfit6 &lt;- lm(mpg ~ I(horsepower^2), data = Auto)\nfit7 &lt;- lm(mpg ~ log(horsepower), data = Auto)\nfit8 &lt;- lm(mpg ~ sqrt(horsepower), data = Auto)\nattach(Auto)\npar(mfrow = c(2, 2))\nplot(horsepower, mpg)\nabline(fit5, col = \"red\", lwd = 4)\nmtext(round(summary(fit5)$r.squared, 2), col = \"red\")\nplot(horsepower^2, mpg)\nabline(fit6, col = \"red\", lwd = 4)\nmtext(round(summary(fit6)$r.squared, 2), col = \"red\")\nplot(log(horsepower), mpg)\nabline(fit7, col = \"red\", lwd = 4)\nmtext(round(summary(fit7)$r.squared, 2), col = \"red\")\nplot(sqrt(horsepower), mpg)\nabline(fit8, col = \"red\", lwd = 4)\nmtext(round(summary(fit8)$r.squared, 2), col = \"red\")"
  },
  {
    "objectID": "Chapter3e.html#question-10",
    "href": "Chapter3e.html#question-10",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 10",
    "text": "Question 10\n\n(a)\n\nlibrary(ISLR)\nlibrary(tidyverse)\nfit9 &lt;- lm(Sales ~ Price + Urban + US, data = Carseats)\nround(summary(fit9)$coef, 4)\n\n            Estimate Std. Error  t value Pr(&gt;|t|)\n(Intercept)  13.0435     0.6510  20.0357   0.0000\nPrice        -0.0545     0.0052 -10.3892   0.0000\nUrbanYes     -0.0219     0.2717  -0.0807   0.9357\nUSYes         1.2006     0.2590   4.6347   0.0000\n\n\n\n\n(b)\nThe interpretation of each coefficient in the regression model is given below. The response variable is Sales, i.e. sales of car seats in thousands.\n\nFor each unit increase in price of carseats, the sales decrease by 54.4 units.\nOn average, stores located in Urban areas in US sell 21.9 less car seats than stores located in rural areas.\nOn average, stores located in US sell 1200.5 car seats more than stores located in other countries.\n\n\n\n\n(c)\nThis model can be written in equation form as\n\\[ Sales(in \\; thousands) = 13.043 -0.054\\times Price - 0.022 \\times Urban + 1.201 \\times US + \\epsilon \\]\n\n\n(d)\nFor the predictors Price and US, we can reject the null hypothesis \\(H_o: \\beta_j = 0\\) because their associated p-value is less than 0.05.\n\n\n\n(e)\nA smaller model is fit as shown below.\n\n\nfit10 &lt;- lm(Sales ~ Price + US, data = Carseats)\nround(summary(fit10)$coef, 4)\n\n            Estimate Std. Error  t value Pr(&gt;|t|)\n(Intercept)  13.0308     0.6310  20.6518        0\nPrice        -0.0545     0.0052 -10.4161        0\nUSYes         1.1996     0.2585   4.6415        0\n\n\n\n\n(f)\nIf we compare the two models in (a) and (e), we find that an Analysis of Variance tells that the two models are not significantly different from each other. How well each model fits the data is revealed by the adjusted \\(R^2\\), which is 0.23 round(summary(fit9)$adj.r.squared,4)for model in (a) and 0.24 round(summary(fit10)$adj.r.squared,4) for model in (e).\n\n\nanova(fit9, fit10)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ Price + Urban + US\nModel 2: Sales ~ Price + US\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    396 2420.8                           \n2    397 2420.9 -1  -0.03979 0.0065 0.9357\n\n\n\n\n(g)\nThe 95% Confidence Intervals for the coefficients in model from (e) are shown below.\n\n\nround(confint(fit10), 3)\n\n             2.5 % 97.5 %\n(Intercept) 11.790 14.271\nPrice       -0.065 -0.044\nUSYes        0.692  1.708\n\n\n\n\n(h)\nThe evidence for outliers and high leverage points in model from (e) can be seen using the diagnostic plots and R code given below. The plots reveal that there are 23 outliers (sum(abs(rstudent(fit10))&gt;2)) with absolute value of studentized residual greater than 2, but none of these has an absolute studentized residual more than 3. Similarly, there are 20 high leverage observations (sum(hatvalues(fit10) &gt; 2*mean(hatvalues(fit10)))) where \\(h_i &gt; 2\\overline{h}\\). Lastly, sum(cooks.distance(fit10) &gt; 4/length(cooks.distance(fit10))) 19 observations are influential as they Cook’s Distance \\(D_i &gt; 4/n\\).\n\nsum(abs(rstudent(fit10)) &gt; 2)\nsum(hatvalues(fit10) &gt; 2 * mean(hatvalues(fit10)))\nsum(cooks.distance(fit10) &gt; 4 / length(cooks.distance(fit10)))\npar(mfrow = c(2, 3))\nplot(fit10, which = c(1:6))"
  },
  {
    "objectID": "Chapter3e.html#question-11",
    "href": "Chapter3e.html#question-11",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 11",
    "text": "Question 11\n\n(a)\nThe simple linear regression of \\(y\\) onto \\(x\\) is shown below. The coefficient \\(\\hat{\\beta}\\) for \\(x\\) is 1.9939. The standard error is 0.1065. The t-value is 18.73 and p-value is nearly 0. The interpretation is that \\(x\\) is a statistically significant predictor of \\(y\\).\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\nfit11 &lt;- lm(y ~ x + 0)\nsummary(fit11)\n\n\nCall:\nlm(formula = y ~ x + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9154 -0.6472 -0.1771  0.5056  2.3109 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nx   1.9939     0.1065   18.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9586 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n(b)\nThe simple linear regression of \\(x\\) onto \\(y\\) is shown below. The coefficient \\(\\hat{\\beta}\\) for \\(y\\) is 0.3911. The standard error is 0.021. The t-value is 18.73 and p-value is nearly 0. The interpretation is that \\(y\\) is a statistically significant predictor of \\(x\\).\n\n\nfit12 &lt;- lm(x ~ y + 0)\nsummary(fit12)\n\n\nCall:\nlm(formula = x ~ y + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8699 -0.2368  0.1030  0.2858  0.8938 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \ny  0.39111    0.02089   18.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4246 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n(c)\nThe results obtained in (a) and (b) above have the exact same t-statistic, p-value and \\(R^2\\) (both adjusted \\(R^2\\) and multiple \\(R^2\\)). This is because both models fit the same correlation between \\(x\\) and \\(y\\).\n\n\n\n(d)\nIn this question, when regression is performed without an intercept, the value of \\(\\hat{\\beta}\\) and \\(SE(\\hat{\\beta})\\) is given by:\n\\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^{n} x_iy_i}{\\sum_{i'=1}^{n}x_{i'}^2} = \\sum_i{x_i}{y_i}/\\sum_j{x_j^2}\\] \\[SE(\\hat{\\beta}) = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - x_i\\hat{\\beta})^2}{(n - 1)\\sum_{i'=1}^nx_{i'}^2}} = \\sqrt{\\sum_i(y_i - x_i\\hat{\\beta})^2/(n - 1)\\sum_jx_j^2}\\] Thus, we can show that:—\n\\[ t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\\] \\[t = \\frac{\\sum_ix_iy_i/\\sum_jx_j^2}{\\sqrt{\\sum_i(y_i - x_i\\hat{\\beta})^2/(n - 1)\\sum_jx_j^2}} \\]\n\\[t = \\frac{\\sum_ix_iy_i \\sqrt{n - 1}}{\\sum_jx_j^2{\\sqrt{\\sum_i(y_i - x_i\\hat{\\beta})^2/\\sum_jx_j^2}}} \\]\n\\[t = \\frac{\\sum_ix_iy_i \\sqrt{n - 1}}{{\\sqrt{\\sum_j{x_j}^2 \\sum_i(y_i - x_i\\hat{\\beta})^2}}} \\]\n\\[t = \\frac{\\sum_ix_iy_i \\sqrt{n - 1}}{{\\sqrt{\\sum_j{x_j}^2 \\sum_i(y_i - x_i\\hat{\\beta})^2}}} \\]\n\nWe now replace the value of \\(\\hat{\\beta}\\) into this equation and solve:—\n\\[t = \\frac{\\sqrt{n - 1}\\sum_ix_iy_i}{\\sqrt{\\sum_jx_j^2\\sum_i(y_i - x_i\\sum_ix_iy_i/\\sum_jx_j^2)^2}}\\] \\[t = \\frac{\\sqrt{n - 1}\\sum_ix_iy_i}{\\sqrt{(\\sum_jx_j^2)(\\sum_jy_j^2) - (\\sum_jx_jy_j)^2}}\\]\nFurther, this result can be confirmed in R Code given below which shows that the calculated t-value using this formula is the same t-value as shown in models fit11 and fit12 :—\n\nt &lt;- sqrt(length(x) - 1) * (x %*% y) / sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)\nas.numeric(round(t, 2))\n\n[1] 18.73\n\n\n\n\n(e)\nFrom the results in (d) above, it is clear that \\(t-statistic\\) will be the same even if \\(x\\) and \\(y\\) are interchanged. The equation for \\(t-statistic\\) is associative in \\(x\\) and \\(y\\).\n\n\n(f)\nNow, we perform the regressions with an intercept. The t-values of both regressions are displayed using the code below. It is clear that both the t-values are equal.\n\nfit13 &lt;- lm(y ~ x)\nfit14 &lt;- lm(x ~ y)\nsummary(fit13)$coef[2, 3]\n\n[1] 18.5556\n\nsummary(fit14)$coef[2, 3]\n\n[1] 18.5556"
  },
  {
    "objectID": "Chapter3e.html#question-12",
    "href": "Chapter3e.html#question-12",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 12",
    "text": "Question 12\n\n(a)\nThe coefficient for regression of \\(Y\\) onto \\(X\\) is given by:\n\\[\\hat{\\beta} = \\frac{\\sum_ix_iy_i}{\\sum_jx_j^2}\\]\nand, the coefficient for regression of \\(X\\) onto \\(Y\\) is given by:\n\\[\\hat{\\beta'} = \\frac{\\sum_ix_iy_i}{\\sum_jy_j^2}\\]\nThus, \\(\\hat{\\beta} = \\hat{\\beta'}\\) in a special condition when,\n\\[{\\sum_jx_j^2} = {\\sum_jy_j^2}\\] .\n\n\n(b)\nThe following R code generates the example:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\nfit15 &lt;- lm(y ~ x)\nfit16 &lt;- lm(x ~ y)\ndata.frame(\n  sum_x_sq = sum(x^2),\n  sum_y_sq = sum(y^2),\n  Coef_y_x = coef(fit15)[2],\n  Coef_x_y = coef(fit16)[2]\n) %&gt;%\n  kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nsum_x_sq\nsum_y_sq\nCoef_y_x\nCoef_x_y\n\n\n\n\nx\n81.05509\n413.2135\n1.99894\n0.3894245\n\n\n\n\n\n\n\n\n\n(c)\nThe following R code generates an example with \\({\\sum_jx_j^2} = {\\sum_jy_j^2}\\):-\n\nx &lt;- (1:100)\ny &lt;- (100:1)\nfit17 &lt;- lm(y ~ x)\nfit18 &lt;- lm(x ~ y)\ndata.frame(\n  sum_x_sq = sum(x^2),\n  sum_y_sq = sum(y^2),\n  Coef_y_x = coef(fit17)[2],\n  Coef_x_y = coef(fit18)[2]\n) %&gt;%\n  kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nsum_x_sq\nsum_y_sq\nCoef_y_x\nCoef_x_y\n\n\n\n\nx\n338350\n338350\n-1\n-1"
  },
  {
    "objectID": "Chapter3e.html#question-13",
    "href": "Chapter3e.html#question-13",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 13",
    "text": "Question 13\n\n(a), (b) & (c)\nThe R code is given below:\n\nset.seed(1)\nx &lt;- rnorm(100, mean = 0, sd = 1)\neps &lt;- rnorm(100, mean = 0, sd = sqrt(0.25))\ny &lt;- -1 + 0.5 * x + eps\nlength(y)\n\n[1] 100\n\n\nThe length of vector y is 100 (length(y)). The coefficient \\(\\beta_o\\) is -1, and the coefficient \\(\\beta_1\\) is 0.5.\n\n\n\n(d)\nThe scatter-plot between x and y is shown below. We see that there is a linear relationship between x and y, although some random variability is introduced by the eps noise.\n\npar(mfrow = c(1, 1))\nplot(x, y)\n\n\n\n\n\n\n(e)\n\nfit19 &lt;- lm(y ~ x)\ncoef(fit19)\n\n(Intercept)           x \n -1.0188463   0.4994698 \n\n\nThe coefficient \\(\\hat{\\beta_o}\\) is -1.02 round(coef(fit19)[1],2), and the coefficient \\(\\hat{\\beta_1}\\) is 0.5 round(coef(fit19)[2],2). Both these estimates are quite close to the true model coefficients.\n\n\n(f)\n\nplot(x, y)\nabline(fit19, col = \"red\", lty = 2)\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2, 0.5,\n  legend = c(\"Fitted Least Squares Line\", \"True Regression Model\"),\n  col = c(\"red\", \"blue\"),\n  lty = 2:1,\n  cex = 0.8\n)\n\n\n\n\n\n\n(g)\nAfter fitting a quadratic term, we observe that there is no significant improvement in the model fit because the ANOVA comparison shows a p-value &gt; 0.05.\n\nfit20 &lt;- lm(y ~ x + I(x^2))\nanova(fit19, fit20)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ x + I(x^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     98 22.709                           \n2     97 22.257  1   0.45163 1.9682 0.1638\n\n\n\n\n(h)\nNow, we re-create steps in questions (a) to (f), but with a lower variance in the error terms, say \\(var(\\epsilon) = 0.05\\). The results how that now the estimated coefficients are much closer to the true model parameters. And, the fitted least squares line is much closer to the true regression model line. However, the confidence intervals for the estimated coefficients are much narrower.\n\n\nset.seed(1)\nx &lt;- rnorm(100, mean = 0, sd = 1)\neps &lt;- rnorm(100, mean = 0, sd = sqrt(0.05))\ny &lt;- -1 + 0.5 * x + eps\nfit21 &lt;- lm(y ~ x)\nplot(x, y)\nabline(fit19, col = \"red\", lty = 2)\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2, 0.0,\n  legend = c(\"Fitted Least Squares Line\", \"True Regression Model\"),\n  col = c(\"red\", \"blue\"), lty = 2:1, cex = 0.8\n)\n\n\n\n\n\n\n(i)\nNow, we re-create steps in questions (a) to (f), but with higher variance in the error terms, say \\(var(\\epsilon) = 5.0\\). The results how that now the estimated coefficients are further away from the true model parameters. And, the fitted least squares line is still somewhat close to the true regression model line because we have a large number of observations. However, the confidence intervals for the estimated coefficients are very wide.\n\n\nset.seed(1)\nx &lt;- rnorm(100, mean = 0, sd = 1)\neps &lt;- rnorm(100, mean = 0, sd = sqrt(5))\ny &lt;- -1 + 0.5 * x + eps\nfit22 &lt;- lm(y ~ x)\nplot(x, y)\nabline(fit19, col = \"red\", lty = 2)\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2, 4, legend = c(\"Fitted Least Squares Line\", \"True Regression Model\"), col = c(\"red\", \"blue\"), lty = 2:1, cex = 0.8)\n\n\n\n\n\n\n(j)\nThe confidence intervals for the coefficients \\(\\hat{\\beta_o}\\) and \\(\\hat{\\beta_1}\\) are tabulated below. The results show that the confidence intervals for the coefficient estimates are much wider when the variance in error terms \\(\\epsilon\\) is higher.\n\n\nbeta_o &lt;- round(rbind(confint(fit19)[1, ], confint(fit21)[1, ], confint(fit22)[1, ]), 3)\nbeta_1 &lt;- round(rbind(confint(fit19)[2, ], confint(fit21)[2, ], confint(fit22)[2, ]), 3)\ncoefs &lt;- data.frame(cbind(beta_o, beta_1))\ncolnames(coefs) &lt;- c(\"LowerCI_beta_o\", \"UpperCI_beta_o\", \"LowerCI_beta_1\", \"UpperCI_beta_o\")\nrownames(coefs) &lt;- c(\"First Model Var(0.25)\", \"Narrow Model Var(0.05)\", \"Wider Model Var(5.0)\")\ncoefs %&gt;%\n  kbl() %&gt;%\n  kable_classic_2(full_width = F)\n\n\n\n\n\nLowerCI_beta_o\nUpperCI_beta_o\nLowerCI_beta_1\nUpperCI_beta_o\n\n\n\n\nFirst Model Var(0.25)\n-1.115\n-0.923\n0.393\n0.606\n\n\nNarrow Model Var(0.05)\n-1.051\n-0.965\n0.452\n0.548\n\n\nWider Model Var(5.0)\n-1.515\n-0.654\n0.020\n0.976"
  },
  {
    "objectID": "Chapter3e.html#question-14",
    "href": "Chapter3e.html#question-14",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 14",
    "text": "Question 14\n\n(a)\nIn the R-code given by the question, and reproduced below, the linear model is: \\[ y = 2 + 2x_1 \\ + \\ 0.3x_2 \\ + \\ \\epsilon \\] where, \\(\\epsilon \\sim \\mathcal{N}(0,1)\\) is a random error term. The regression coefficients are \\(\\beta_o = 2\\), \\(\\beta_1 = 2\\) and \\(\\beta_2 = 0.3\\).\n\n\nset.seed(1)\nx1 &lt;- runif(100)\nx2 &lt;- 0.5 * x1 + rnorm(100) / 10\ny &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)\n\n\n\n(b)\nBased on the R code given, x1 and x2 seems positively correlated. The correlation coefficient between x1 and x2 calculated using code in R is 0.84 (round(cor(x1, x2),2)). The scatter plot matrix is given below:\n\n\ncor(x1, x2)\n\n[1] 0.8351212\n\nplot(x1, x2)\n\n\n\n\n\n\n(c)\nThe regression of y onto x1 and x2 is fitted below. The results show that around 23% variability in y is explained by this model, and the model is significantly better than a NULL model, i.e. \\(\\hat{y} = \\overline{y}\\). Further, x1 is a statistically significant predictor of y, whereas x2 is not. We can safely reject the two null hypotheses \\(H_o \\ : \\ \\beta_o = 0\\) and \\(H_o \\ : \\ \\beta_1 = 0\\). However, we cannot reject the null hypothesis \\(H_o \\ : \\ \\beta_2 = 0\\). The estimated coefficients of regression are: \\(\\hat{\\beta_o} = 2.01\\), \\(\\hat{\\beta_1} = 2.3\\) and \\(\\hat{\\beta_2} = -0.24\\). The estimated coefficient for intercept is reasonably close to the true parameter, however the estimated coefficients for x1 and x2 is totally off target.\n\n\nfit23 &lt;- lm(y ~ x1 + x2)\nsummary(fit23)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8311 -0.7273 -0.0537  0.6338  2.3359 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1305     0.2319   9.188 7.61e-15 ***\nx1            1.4396     0.7212   1.996   0.0487 *  \nx2            1.0097     1.1337   0.891   0.3754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.056 on 97 degrees of freedom\nMultiple R-squared:  0.2088,    Adjusted R-squared:  0.1925 \nF-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05\n\n\n\n\n(d)\nNow, we fit linear least squares regression using only x1 as a predictor for the response y. The resulting model is statistically significantly better than the null model due to a very low p-value on the F-statistic. Further, it explains 24% of the variability in y. We can safely reject the two null hypotheses \\(H_o \\ : \\ \\beta_o = 0\\) and \\(H_o \\ : \\ \\beta_{x1} = 0\\). The estimated coefficients of regression are: \\(\\hat{\\beta_o} = 2.01\\), \\(\\hat{\\beta_{x1}} = 2.2\\). They are reasonably close to the true parameters.\n\n\nfit24 &lt;- lm(y ~ x1)\nsummary(fit24)\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89495 -0.66874 -0.07785  0.59221  2.45560 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1124     0.2307   9.155 8.27e-15 ***\nx1            1.9759     0.3963   4.986 2.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.055 on 98 degrees of freedom\nMultiple R-squared:  0.2024,    Adjusted R-squared:  0.1942 \nF-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06\n\n\n\n\n(e)\nNow, we fit linear least squares regression using only x2 as a predictor for the response y. The resulting model is statistically significantly better than the null model due to a very low p-value on the F-statistic. Further, it explains only 14.8% of the variability in y. Based on this model, we can safely reject the two null hypotheses \\(H_o \\ : \\ \\beta_o = 0\\) and \\(H_o \\ : \\ \\beta_{x2} = 0\\). The estimated coefficients of regression are: \\(\\hat{\\beta_o} = 2.42\\), \\(\\hat{\\beta_{x2}} = 2.8\\). Surprisingly, the coefficient estimates are widely off target for both the intercept and the predictor. Despite being statistically significant, the estimates do not incorporate the true parameter values even in their 95 % confidence interval.\n\n\nfit25 &lt;- lm(y ~ x2)\nsummary(fit25)\n\n\nCall:\nlm(formula = y ~ x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62687 -0.75156 -0.03598  0.72383  2.44890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.3899     0.1949   12.26  &lt; 2e-16 ***\nx2            2.8996     0.6330    4.58 1.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.072 on 98 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.1679 \nF-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05\n\n\n\n\n(f)\nAt the first sight, the results from (c), (d) and (e) do seem to contradict each other. Firstly, the coefficient for x2 is non-significant and negative in the multiple regression, but turns positive and highly significant in simple regression. Further, there is very high error in estimate of \\(\\hat{\\beta_2}\\) in the multiple regression (because x2 is correlated with x1, and this collinearity widens the confidence interval). Secondly, the simple regression using only x2 as a predictor displays a large positive coefficient estimate, much more than the true parameter. This is perhaps because x2 is acting as a surrogate for x1 in the simple regression of y onto x2.\nThe situation in questions (c) to (e) present a text book example of collinearity. When two predictors (x1 and x2) are highly correlated, the parameter estimates in multiple linear regression tend to have large standard errors of estimation. This leads to imprecise estimates with wide confidence intervals. Thus, they null hypothesis cannot be rejected in the multiple linear regression. However, simple regression performed after dropping one of the correlated predictors can solve the problem of imprecise estimates. Here, the simple linear regression of either predictor shows them to be highly significant predictors of y. But, we must be wary that simple regression involving such predictors can lead to misleading estimates of the true underlying relationship.\n\n\n\n(g)\nNow, we add a new observation as provided in the question and fit the three models again. The new models are checked for leverage and standardized residual (outlier status) of the new observation. The results are compactly produced in the table produced at the end. We compare the models and find the following results: - In the multiple linear regression model, adding the new observation totally throws off the estimated coefficients for x1 and x2. Adding this observation has a major impact on the re-fitted model. The new observation is an outlier, having an absolute standardized residual &gt; 2 (though it is &lt; 3). The new observation has high leverage, and thus a high influence on the re-fitted model. This is shown in the diagnostic plots as well. Lastly, this means that a fitted model with high collinearity is not robust, it is very vulnerable to misread observations. - In the simple linear regression model with x1 as predictor, adding the new observation has very low impact on the coefficients. The new observation is an extreme outlier, with an absolute standardized residual greater than 3. However, the new observation has a very low leverage, and thus, it is not influential on the re-fitted model at all. - In the simple linear regression model with x2 as predictor, adding the new observation has a moderate to low impact on the coefficients. The new observation is not an outlier, and thus even though it has somewhat high leverage, it has no major influence on the fitted model.\n\nx1 &lt;- c(x1, 0.1)\nx2 &lt;- c(x2, 0.8)\ny &lt;- c(y, 6)\nfit23new &lt;- lm(y ~ x1 + x2)\nfit24new &lt;- lm(y ~ x1)\nfit25new &lt;- lm(y ~ x2)\nResult &lt;- data.frame(\n  Model = c(\n    \"y~x1+x2\", \"y~x1+x2 new\", \"y~x1\",\n    \"y~x1 new\", \"y~x2\", \"y~x2 new\"\n  ),\n  Intercept = rep(NA, 6), Beta_x1 = rep(NA, 6),\n  Beta_x2 = rep(NA, 6), Leverage = rep(NA, 6),\n  Standardized_Residual = rep(NA, 6)\n)\nResult[1, 2:4] &lt;- fit23$coefficients\nResult[2, 2:4] &lt;- fit23new$coefficients\nResult[3, 2:3] &lt;- fit24$coefficients\nResult[4, 2:3] &lt;- fit24new$coefficients\nResult[5, c(2, 4)] &lt;- fit25$coefficients\nResult[6, c(2, 4)] &lt;- fit25new$coefficients\n\n# Code to find Leverage of the new observation\nsum(hatvalues(fit23new) &gt; 2 * mean(hatvalues(fit23new)))\n\n[1] 2\n\nwhich.max(hatvalues(fit23new))\n\n101 \n101 \n\nResult[2, 5] &lt;- hatvalues(fit23new)[101]\nwhich(abs(rstudent(fit23new)) &gt; 2)\n\n 16  21  55  82 101 \n 16  21  55  82 101 \n\nResult[2, 6] &lt;- rstudent(fit23new)[101]\nsum(hatvalues(fit24new) &gt; 2 * mean(hatvalues(fit24new)))\n\n[1] 3\n\nwhich.max(hatvalues(fit24new))\n\n27 \n27 \n\nResult[4, 5] &lt;- hatvalues(fit24new)[101]\nwhich(abs(rstudent(fit24new)) &gt; 2)\n\n 21  55  56  82 101 \n 21  55  56  82 101 \n\nResult[4, 6] &lt;- rstudent(fit24new)[101]\nsum(hatvalues(fit25new) &gt; 2 * mean(hatvalues(fit25new)))\n\n[1] 8\n\nwhich.max(hatvalues(fit25new))\n\n101 \n101 \n\nResult[6, 5] &lt;- hatvalues(fit25new)[101]\nwhich(abs(rstudent(fit25new)) &gt; 2)\n\n 5 21 55 82 \n 5 21 55 82 \n\nResult[6, 6] &lt;- rstudent(fit25new)[101]\n\n\nlibrary(kableExtra)\nknitr::kable(Result, digits = 3) %&gt;% kable_classic_2()\n\n\n\n\nModel\nIntercept\nBeta_x1\nBeta_x2\nLeverage\nStandardized_Residual\n\n\n\n\ny~x1+x2\n2.130\n1.440\n1.010\nNA\nNA\n\n\ny~x1+x2 new\n2.227\n0.539\n2.515\n0.415\n2.113\n\n\ny~x1\n2.112\n1.976\nNA\nNA\nNA\n\n\ny~x1 new\n2.257\n1.766\nNA\n0.033\n3.438\n\n\ny~x2\n2.390\nNA\n2.900\nNA\nNA\n\n\ny~x2 new\n2.345\nNA\n3.119\n0.101\n1.141\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(fit23new)\n\n\n\npar(mfrow = c(2, 2))\nplot(fit24new)\n\n\n\npar(mfrow = c(2, 2))\nplot(fit25new)"
  },
  {
    "objectID": "Chapter3l.html",
    "href": "Chapter3l.html",
    "title": "Chapter 3 (R Lab)",
    "section": "",
    "text": "library(MASS)\nlibrary(ISLR)\nlibrary(car)"
  },
  {
    "objectID": "Chapter3l.html#libraries",
    "href": "Chapter3l.html#libraries",
    "title": "Chapter 3 (R Lab)",
    "section": "",
    "text": "library(MASS)\nlibrary(ISLR)\nlibrary(car)"
  },
  {
    "objectID": "Chapter3l.html#simple-linear-regression",
    "href": "Chapter3l.html#simple-linear-regression",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.2 Simple Linear Regression",
    "text": "3.6.2 Simple Linear Regression\n\ndata(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\nlm.fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)\n\nDisplaying the results of simple linear regression of medv on lstat.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nThe contents of lm.fit can be displayed as :—\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nUsing the predict function for predicting the values of medv for a given value(s) of lstat :—\n\npredict(lm.fit, data.frame(lstat=c(5,10,15)), interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat=c(5,10,15)), interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nThe plot of medv with lstat along with least squares regression line is as follows:-\n\npar(mfrow=c(2,2))\nplot(x=lstat, y=medv)\nabline(lm.fit, col=\"red\")\nplot(x=lstat, y=medv, pch=20)\nplot(x=lstat, y=medv, pch=\"+\")\nabline(lm.fit, col=\"red\", lwd=3)\nplot(1:20, 1:20, pch=1:20)\n\n\n\n\nPlotting the 4 diagnostic plots of lm.fit:—\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\nAlternatively, we can plot the residuals vs. fitted values; and studentized residuals vs. fitted values as follows: —\n\npar(mfrow=c(1,2))\nplot(x=predict(lm.fit), y=residuals(lm.fit))\nplot(x=predict(lm.fit), y=rstudent(lm.fit))\n\n\n\n\n\nNow, we calculate the leverage statistics using hatvalues function. The largest leverage is for the observation number 375 (which.max(hatvalues(lm.fit))). Also, I plot the studentized residuals vs. leverage statistic, just like Fig.3.13(right) in the book :–\n\n\npar(mfrow=c(1,2))\nplot(hatvalues(lm.fit))\nplot(x=hatvalues(lm.fit), y=rstudent(lm.fit),\n     xlab = \"Leverage\", ylab=\"Studentized Residuals\")\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Chapter3l.html#multiple-linear-regression",
    "href": "Chapter3l.html#multiple-linear-regression",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.3 Multiple Linear Regression",
    "text": "3.6.3 Multiple Linear Regression\nFitting a multiple linear regression as follows:-\n\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nPerforming regression of medv on all other variables :—\n\nlm.fit &lt;- lm(medv ~ ., data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nThe components of summary(lm.fit) are :–\n\nnames(summary(lm.fit))\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nsummary(lm.fit)$r.squared ; summary(lm.fit)$sigma\n\n[1] 0.7406427\n\n\n[1] 4.745298\n\n\nCalculating V.I.F from car::vif() from the car package :—\n\ncar::vif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\n\nExcluding one variable (age, which has high p-value) from multiple regression :—\n\nlm.fit1 &lt;- lm(medv ~ . - age, data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter3l.html#interaction-terms",
    "href": "Chapter3l.html#interaction-terms",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.4 Interaction Terms",
    "text": "3.6.4 Interaction Terms\nIncluding interaction terms as follows :-\n\nsummary(lm(medv ~ lstat*age, data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter3l.html#non-linear-transformations-of-predictors",
    "href": "Chapter3l.html#non-linear-transformations-of-predictors",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.5 Non-Linear Transformations of Predictors",
    "text": "3.6.5 Non-Linear Transformations of Predictors\n\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2), data=Boston)\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nWe use anova() function to quantify how much the quadratic fit is better than linear fit. This is shown as below:\n\nlm.fit &lt;- lm(medv~lstat, data=Boston)\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2), data=Boston)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHence, the model containing \\(lstat^2\\) is far superior to the simple linear regression model. This is also shown in the the diagnostic plots as below:\n\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\nNow, we use the poly() function with the lm() call to include polynomials of a variable up to any degree.\n\n\nlm.fit5 &lt;- lm(medv ~ poly(lstat,5), data=Boston)\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nLastly, a log transformation of the predictor variable.\n\n\nsummary(lm(medv ~ log(lstat), data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(lstat), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.4599  -3.5006  -0.6686   2.1688  26.0129 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  52.1248     0.9652   54.00   &lt;2e-16 ***\nlog(lstat)  -12.4810     0.3946  -31.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.329 on 504 degrees of freedom\nMultiple R-squared:  0.6649,    Adjusted R-squared:  0.6643 \nF-statistic:  1000 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter3l.html#qualitative-predictor",
    "href": "Chapter3l.html#qualitative-predictor",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.6 Qualitative Predictor",
    "text": "3.6.6 Qualitative Predictor\nLoading the Carseats data set.\n\ndata(\"Carseats\")\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\nattach(Carseats)\n\nNow, we create a multiple linear regression with some interaction terms:-\n\n\nlm.fit &lt;- lm(Sales ~ . + Income*Advertising + Price*Age, data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income * Advertising + Price * Age, \n    data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nWe use the contrasts() function to display the dummy coding that R uses for qualitative variables such as ShelveLoc. We can use contrasts() to change the dummy values for different factor levels.\n\n\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Chapter3l.html#writing-functions",
    "href": "Chapter3l.html#writing-functions",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.7 Writing Functions",
    "text": "3.6.7 Writing Functions\nWe now write the function to load both libraries ISLR and MASS.\n\nLoadLibraries &lt;- function(){\n  library(MASS)\n  library(ISLR)\n  print(\"Libraries MASS and ISLR have been loaded!\")\n}\nLoadLibraries()\n\n[1] \"Libraries MASS and ISLR have been loaded!\""
  }
]