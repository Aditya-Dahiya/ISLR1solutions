[
  {
    "objectID": "Chapter2e.html",
    "href": "Chapter2e.html",
    "title": "Chapter 2 (Exercises)",
    "section": "",
    "text": "When sample size \\(n\\) is extremely large, and \\(p\\) is small, a flexible statistical learning method will perform better than a non-flexible method, because\n\nThe low \\(p\\) allows us to avoid the curse of dimensionality.\nLarge number of \\(n\\) allows us to better predict with a flexible method, lowering the chance of overfitting.\n\n\n\n\nWhen the sample size \\(n\\) is very small, and number of predictors \\(p\\) is very large, a flexible statistical learning method will perform worse than a non-flexible one because a large number of predictors increase the , making it difficult to identify nearest-neighbors. A small number of observations also means that a highly flexible model to could lead to high variance, or overfitting.\n\n\n\nWhen the relationship between predictors and response is highly non-linear, a flexible model will perform much better because it can better allow for non-linear \\(f(x)\\), and thus will better approximate a highly non-linear relationship.\n\n\n\nWhen the variance of error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high, both flexible and non-flexible methods will lead to inaccurate predictions. However, comparatively, a non-flexible method will perform better as it is less likely to overfit or mis-read noise for actual relationship.\n\n\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. CEO Salary.\n\nHere, we are primarily interested in inference, rather than prediction, because we want to understand which factors affect CEO salary, rather than predicting the salary for any given CEO.\n\nIn this scenario, \\(n = 500\\) and \\(p = 3\\) .\n\n\n\n\n\nThis scenario is a classification problem, as the primary response is a qualitative variable (success vs. failure).\n\nHere, we are primarily interested in prediction because we wish to know the response our test data, i.e. new product being launched, rather than understanding the factors behind the response.\n\nIn this scenario, \\(n = 20\\) and \\(p = 13\\) .\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. percentage change in US dollar.\n\nHere, we are primarily interested in prediction instead of inference since our intended purpose to predict % change in US dollar. We are not concerned with the exact factors that cause such a change.\n\nIn this scenario, \\(n = 52\\) (i.e. number of weeks in 2012) and \\(p = 3\\) .\n\n\n\n\n\n\n\nThe sketch is displayed below: —\n \n\n\n\nIn this figure, the curve (C) is the Irreducible Error which is a feature of the data-set, and thus stays constant. The curve(D) represents the Training Error which will always decrease monotonously with increasing flexibility of the fitted model because almost all techniques directly or indirectly aim at reducing the MSE in the training data-set. As flexibility of the method increases, it’s variance will increase [curve (B)] and bias will decrease [curve(D)] because a flexible model is highly variable (change in a single observation will change the fit) and less biased (each observation is closely covered). The sum of curves B, C and E represents the Test Error i.e. Curve A.\n\n\n\n\n\n\nThree real-life applications in which classification is useful are: —\nEmail Spam\n1. Response: A qualitative binary indicator whether the email is spam or not spam.\n2. Predictors: Presence of common words in subject of the email (such as offer, lottery etc.), Presence of name of email account holder in text of email, Unverified attachments, Relative frequency of commonly used words.\n3. Goal of the application: Prediction.\n\nHandwriting recognition : ZIP codes on postal envelopes\n1. Response: Each of the 5 digits (0-9) - categorical outcome\n2. Predictors: A matrix corresponding to an image where each pixel corresponds is an entry in the matrix, and its pixel intensity ranges from 0 (black) to 255 (white).\n3. Goal of the application: Prediction\n\nFactors Affecting Consumer purchase amongst competing goods : Which factors most affect the consumers’ purchase of a particular product amongst competing brands.\n1. Response: Which one of the competing products does a consumer buy?\n2. Predictors: Area, demographics of buyer, advertising revenues etc.\n3. Goal of the application: Inference.\n\n\n\nThree real-life applications in which regression is useful are: —\n1. Predicting response to dosage of BP medication\ni) Response: Blood Pressure (in mmHg)\nii) Predictors: Dose of a given medication, age, vital indicators etc.\niii) Goal of the application: Prediction\n\n2. Factors affecting Crop Yields\ni) Response: Crop Yield in units per area\nii) Predictors: Amount of fertilizer applied, Soil pH, Temperature, Machines used, etc.\niii) Goal of the application: Both inference (which inputs yield maximum benefits) and prediction (expected crop yield in a particular season)\n\n3. Increasing operational efficiency in a production line by identifying low-hanging fruits\ni) Response: Factory output of a good\nii) Predictors: Various raw materials, number of workers, capital investment, working hours, etc.\niii) Goal of the application: Inference (spending on which input / predictor) will cause maximum increase in output.\n\n\n\nThree real-life applications in which cluster analysis is useful are: —\n1. Social Network Clusters: Identifying like-minded twitter users based on their activity.\n2. Finding Similar books for a book recommendation software based on past purchase data from users.\n3. Gene mapping in evolutionary biology to find clusters of similar genes within a genome / evolutionary lineage.\n\n\n\n\n\nA very flexible approach has following advantages and disadvantages :–\n\n\n\nAdvantages: Allows fitting to highly non-linear relationships. Better performance when \\(n\\) is large and \\(p\\) is small.\nDisadvantages: Requires estimation of a large number of parameters. Low level of interpretability. Can lead to overfitting. Does not perform well when \\(n\\) is very small or data is high-dimensional i.e. large \\(p\\).\n\n\nA more flexible approach may be considered when we have a large data (high \\(n\\)), low dimensionality (low \\(p\\)), or when we are mainly interested in prediction in a non-linear relationship.\nA less flexible approach is preferred when out objective is inference and interpretability of the results. (Or when data is high dimensional or low \\(n\\).)\n\n\n\n\nA parametric statistical learning approach assumes a \\(f(x)\\) i.e. it pre-supposes a functional form for relationship between predictors and response. On the other hand, a non-parametric approach does not assume any functional form for \\(f\\) at all. It just tries to best fit the available data. Thus, it requires a very large amount of data.\n\nParametric approach advantages:\n1) Easier to compute and evaluate\n2) Interpretation is simpler\n3) Better for inference tasks.\n4) Can be used with a low number of observations.\n\nParametric approach disadvantages:\n1) If the underlying relationship is far off from assumed functional form, the method will lead to high error rate.\n2) If too flexible a model fitted, it will lead to overfitting.\n3) Real-life relations are rarely simple functional forms.\n\n\n\n\n\n\ndata &lt;- read.csv(\"docs/ISLRCh2Ex2-4Q7.csv\") |&gt;\n  mutate(y = as.factor(y)) |&gt;\n  mutate(dist_000 = sqrt(x1^2 + x2^2 + x3^2))\ndata |&gt; kbl() |&gt; kable_classic_2(full_width = F)\n\n\n\n\nx1\nx2\nx3\ny\ndist_000\n\n\n\n\n0\n3\n0\nred\n3.000000\n\n\n2\n0\n0\nred\n2.000000\n\n\n0\n1\n3\nred\n3.162278\n\n\n0\n1\n2\ngreen\n2.236068\n\n\n-1\n0\n1\ngreen\n1.414214\n\n\n1\n1\n1\nred\n1.732051\n\n\n\n\n\n\n\nThe distance between each test point and the test point (0,0,0) is displayed above, and for the six points is\n3.00 2.00 3.16 2.24 1.41 1.73\n\n\n\nWith \\(K=1\\), our prediction for \\(y\\) is “green”, as the nearest neighbor is “green”.\n\n\n\nWith \\(K=3\\), our prediction for \\(y\\) is “red”, as the nearest neighbors are 1 “green” and 2 “red”.\n\n\n\nIf the Bayes Decision Boundary is highly non-linear, then we would expect the best value of \\(K\\) to be small, because a smaller \\(K\\) allows us more flexibility in the estimated decision boundary.\n\n\n\n\n\n\n\n\n\n\n\ndata(College)\ncollege &lt;- College\nrm(College)\n\n\n\n\n\nrownames(college) &lt;- college[,1]\nfix(college)\n\n\n\n\n\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\n\n\n\n\n\n\n\n\nAs per the summary() function, there are 78 elite universities. The box-plot of Outstate vs. Elite is as below :—\n\n\nwith(college, plot(x=Elite, y=Outstate, ylab = \"Out-of-State Tuition\", xlab = \"Whether Elite College?\", main = \"Question 8(c)(iv)\"))\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nhist(college$Room.Board, main = \"\", sub = \"Room Boarding Charges\")\nhist(college$Room.Board, breaks = 100, main=\"\", sub = \"Room Boarding Charges(100 bins)\")\nhist(college$Books, main = \"\", sub = \"Expenses on Books\")\nhist(college$Books, breaks = 100, main=\"\", sub = \"Expenses on Books\")\n\n\n\n\n\n\n\n\n\n\n\ndata(Auto)\nAuto &lt;- na.omit(Auto)\n\nThe predictors Origin and Name in the Auto data set are qualitative, while all the other variables are quantitative, i.e. mpg, cylinders, displacement, horsepower, weight, acceleration, year.\n\n\n\n\nMin &lt;- rep(0, 7)\nMax &lt;- rep(0, 7)\nStDev &lt;- rep(0, 7)\nMean &lt;- rep(0, 7)\nName &lt;- colnames(Auto)[-c(8, 9)]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n9\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1613\n5140\n\n\nacceleration\n8\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n23.45\n7.81\n\n\ncylinders\n5.47\n1.71\n\n\ndisplacement\n194.41\n104.64\n\n\nhorsepower\n104.47\n38.49\n\n\nweight\n2977.58\n849.4\n\n\nacceleration\n15.54\n2.76\n\n\nyear\n75.98\n3.68\n\n\n\n\n\n\n\n\n\n\n\nAuto1 &lt;- Auto[-c(10:85), ]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto1[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto1[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto1[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto1[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n11\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1649\n4997\n\n\nacceleration\n8.5\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n24.4\n7.87\n\n\ncylinders\n5.37\n1.65\n\n\ndisplacement\n187.24\n99.68\n\n\nhorsepower\n100.72\n35.71\n\n\nweight\n2935.97\n811.3\n\n\nacceleration\n15.73\n2.69\n\n\nyear\n77.15\n3.11\n\n\n\n\n\n\n\n\n\n\n\npairs(Auto[, c(1, 3, 4, 5, 6)])\n\n\n\nmodel &lt;- lm(mpg ~ . - name, data = Auto)\ncor &lt;- as.data.frame(model$coefficients[-1])\ncolnames(cor) &lt;- \"Coefficient of Regression\"\ncor |&gt;\n  kbl() |&gt;\n  kable_classic_2()\n\n\n\n\n\nCoefficient of Regression\n\n\n\n\ncylinders\n-0.4933763\n\n\ndisplacement\n0.0198956\n\n\nhorsepower\n-0.0169511\n\n\nweight\n-0.0064740\n\n\nacceleration\n0.0805758\n\n\nyear\n0.7507727\n\n\norigin\n1.4261405\n\n\n\n\n\n\n\nThe above table shows regression coefficients of other variables with mpg.\n\n\n\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n# VarDescrip &lt;- read.csv(\"BostonVarDescrip.csv\")\nnrow(Boston)\n\n[1] 506\n\nncol(Boston)\n\n[1] 14\n\n\nThere are total 506 rows in the data-set and 14 columns. The rows represent the 506 neighborhoods and each column represents the following :—\n\n# VarDescrip |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\nThe pairwise plots of some important variables is as below : —\n\nBoston1 &lt;- Boston |&gt; dplyr::select(crim, nox, dis, tax, lstat)\npairs(Boston1)\n\n\n\n\n\n\n\nThe linear regression shows us the following result :—\n\nmdl &lt;- lm(crim ~ ., data = Boston)\nsummary(mdl)\n\n\nCall:\nlm(formula = crim ~ ., data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.924 -2.120 -0.353  1.019 75.051 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.033228   7.234903   2.354 0.018949 *  \nzn            0.044855   0.018734   2.394 0.017025 *  \nindus        -0.063855   0.083407  -0.766 0.444294    \nchas         -0.749134   1.180147  -0.635 0.525867    \nnox         -10.313535   5.275536  -1.955 0.051152 .  \nrm            0.430131   0.612830   0.702 0.483089    \nage           0.001452   0.017925   0.081 0.935488    \ndis          -0.987176   0.281817  -3.503 0.000502 ***\nrad           0.588209   0.088049   6.680 6.46e-11 ***\ntax          -0.003780   0.005156  -0.733 0.463793    \nptratio      -0.271081   0.186450  -1.454 0.146611    \nblack        -0.007538   0.003673  -2.052 0.040702 *  \nlstat         0.126211   0.075725   1.667 0.096208 .  \nmedv         -0.198887   0.060516  -3.287 0.001087 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.439 on 492 degrees of freedom\nMultiple R-squared:  0.454, Adjusted R-squared:  0.4396 \nF-statistic: 31.47 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nThus, per capital crime rate is significantly related with the variables zn, dis, rad, black and medv.\n\n\n\nYes, there are some suburbs with abnormally high per-capita crime rate. But no such trend is visible in Tax-Rates or Pupil-Teacher Ratio. The plots of these three variables are shown below:—\n\nattach(Boston)\npar(mfrow=c(2,3))\nplot(crim)\nplot(tax)\nplot(ptratio)\nboxplot(crim)\nboxplot(tax)\nboxplot(ptratio)\n\n\n\n\n\n\n\n35 suburbs in this data set bound the Charles river. This is found using the code:—\n\nnrow(subset(Boston, chas == 1))\n\n[1] 35\n\n\n\n\n\nThe median pupil-teacher ratio is 19.05, found using the code:—\n\nmedian(Boston$ptratio)\n\n\n\n\nThe suburb of Boston with lowest median value of ownership occupied homes is 399. Its values for other variables are:—\n\nas.data.frame(Boston[which.min(Boston$medv),]) |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n399\n38.3518\n0\n18.1\n0\n0.693\n5.453\n100\n1.4896\n24\n666\n20.2\n396.9\n30.59\n5\n\n\n\n\n\n\n\n\n\n\nIn this data set, 13 suburbs average more than 8 rooms per dwelling and 64 suburbs average more than 7 rooms per dwelling. The code to find these values is :—\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;8))\n\n[1] 13\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;7))\n\n[1] 64"
  },
  {
    "objectID": "Chapter2e.html#conceptual",
    "href": "Chapter2e.html#conceptual",
    "title": "Chapter 2 (Exercises)",
    "section": "",
    "text": "When sample size \\(n\\) is extremely large, and \\(p\\) is small, a flexible statistical learning method will perform better than a non-flexible method, because\n\nThe low \\(p\\) allows us to avoid the curse of dimensionality.\nLarge number of \\(n\\) allows us to better predict with a flexible method, lowering the chance of overfitting.\n\n\n\n\nWhen the sample size \\(n\\) is very small, and number of predictors \\(p\\) is very large, a flexible statistical learning method will perform worse than a non-flexible one because a large number of predictors increase the , making it difficult to identify nearest-neighbors. A small number of observations also means that a highly flexible model to could lead to high variance, or overfitting.\n\n\n\nWhen the relationship between predictors and response is highly non-linear, a flexible model will perform much better because it can better allow for non-linear \\(f(x)\\), and thus will better approximate a highly non-linear relationship.\n\n\n\nWhen the variance of error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high, both flexible and non-flexible methods will lead to inaccurate predictions. However, comparatively, a non-flexible method will perform better as it is less likely to overfit or mis-read noise for actual relationship.\n\n\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. CEO Salary.\n\nHere, we are primarily interested in inference, rather than prediction, because we want to understand which factors affect CEO salary, rather than predicting the salary for any given CEO.\n\nIn this scenario, \\(n = 500\\) and \\(p = 3\\) .\n\n\n\n\n\nThis scenario is a classification problem, as the primary response is a qualitative variable (success vs. failure).\n\nHere, we are primarily interested in prediction because we wish to know the response our test data, i.e. new product being launched, rather than understanding the factors behind the response.\n\nIn this scenario, \\(n = 20\\) and \\(p = 13\\) .\n\n\n\n\n\nThis scenario is a regression problem, because the response / outcome is a continuous variable, i.e. percentage change in US dollar.\n\nHere, we are primarily interested in prediction instead of inference since our intended purpose to predict % change in US dollar. We are not concerned with the exact factors that cause such a change.\n\nIn this scenario, \\(n = 52\\) (i.e. number of weeks in 2012) and \\(p = 3\\) .\n\n\n\n\n\n\n\nThe sketch is displayed below: —\n \n\n\n\nIn this figure, the curve (C) is the Irreducible Error which is a feature of the data-set, and thus stays constant. The curve(D) represents the Training Error which will always decrease monotonously with increasing flexibility of the fitted model because almost all techniques directly or indirectly aim at reducing the MSE in the training data-set. As flexibility of the method increases, it’s variance will increase [curve (B)] and bias will decrease [curve(D)] because a flexible model is highly variable (change in a single observation will change the fit) and less biased (each observation is closely covered). The sum of curves B, C and E represents the Test Error i.e. Curve A.\n\n\n\n\n\n\nThree real-life applications in which classification is useful are: —\nEmail Spam\n1. Response: A qualitative binary indicator whether the email is spam or not spam.\n2. Predictors: Presence of common words in subject of the email (such as offer, lottery etc.), Presence of name of email account holder in text of email, Unverified attachments, Relative frequency of commonly used words.\n3. Goal of the application: Prediction.\n\nHandwriting recognition : ZIP codes on postal envelopes\n1. Response: Each of the 5 digits (0-9) - categorical outcome\n2. Predictors: A matrix corresponding to an image where each pixel corresponds is an entry in the matrix, and its pixel intensity ranges from 0 (black) to 255 (white).\n3. Goal of the application: Prediction\n\nFactors Affecting Consumer purchase amongst competing goods : Which factors most affect the consumers’ purchase of a particular product amongst competing brands.\n1. Response: Which one of the competing products does a consumer buy?\n2. Predictors: Area, demographics of buyer, advertising revenues etc.\n3. Goal of the application: Inference.\n\n\n\nThree real-life applications in which regression is useful are: —\n1. Predicting response to dosage of BP medication\ni) Response: Blood Pressure (in mmHg)\nii) Predictors: Dose of a given medication, age, vital indicators etc.\niii) Goal of the application: Prediction\n\n2. Factors affecting Crop Yields\ni) Response: Crop Yield in units per area\nii) Predictors: Amount of fertilizer applied, Soil pH, Temperature, Machines used, etc.\niii) Goal of the application: Both inference (which inputs yield maximum benefits) and prediction (expected crop yield in a particular season)\n\n3. Increasing operational efficiency in a production line by identifying low-hanging fruits\ni) Response: Factory output of a good\nii) Predictors: Various raw materials, number of workers, capital investment, working hours, etc.\niii) Goal of the application: Inference (spending on which input / predictor) will cause maximum increase in output.\n\n\n\nThree real-life applications in which cluster analysis is useful are: —\n1. Social Network Clusters: Identifying like-minded twitter users based on their activity.\n2. Finding Similar books for a book recommendation software based on past purchase data from users.\n3. Gene mapping in evolutionary biology to find clusters of similar genes within a genome / evolutionary lineage.\n\n\n\n\n\nA very flexible approach has following advantages and disadvantages :–\n\n\n\nAdvantages: Allows fitting to highly non-linear relationships. Better performance when \\(n\\) is large and \\(p\\) is small.\nDisadvantages: Requires estimation of a large number of parameters. Low level of interpretability. Can lead to overfitting. Does not perform well when \\(n\\) is very small or data is high-dimensional i.e. large \\(p\\).\n\n\nA more flexible approach may be considered when we have a large data (high \\(n\\)), low dimensionality (low \\(p\\)), or when we are mainly interested in prediction in a non-linear relationship.\nA less flexible approach is preferred when out objective is inference and interpretability of the results. (Or when data is high dimensional or low \\(n\\).)\n\n\n\n\nA parametric statistical learning approach assumes a \\(f(x)\\) i.e. it pre-supposes a functional form for relationship between predictors and response. On the other hand, a non-parametric approach does not assume any functional form for \\(f\\) at all. It just tries to best fit the available data. Thus, it requires a very large amount of data.\n\nParametric approach advantages:\n1) Easier to compute and evaluate\n2) Interpretation is simpler\n3) Better for inference tasks.\n4) Can be used with a low number of observations.\n\nParametric approach disadvantages:\n1) If the underlying relationship is far off from assumed functional form, the method will lead to high error rate.\n2) If too flexible a model fitted, it will lead to overfitting.\n3) Real-life relations are rarely simple functional forms.\n\n\n\n\n\n\ndata &lt;- read.csv(\"docs/ISLRCh2Ex2-4Q7.csv\") |&gt;\n  mutate(y = as.factor(y)) |&gt;\n  mutate(dist_000 = sqrt(x1^2 + x2^2 + x3^2))\ndata |&gt; kbl() |&gt; kable_classic_2(full_width = F)\n\n\n\n\nx1\nx2\nx3\ny\ndist_000\n\n\n\n\n0\n3\n0\nred\n3.000000\n\n\n2\n0\n0\nred\n2.000000\n\n\n0\n1\n3\nred\n3.162278\n\n\n0\n1\n2\ngreen\n2.236068\n\n\n-1\n0\n1\ngreen\n1.414214\n\n\n1\n1\n1\nred\n1.732051\n\n\n\n\n\n\n\nThe distance between each test point and the test point (0,0,0) is displayed above, and for the six points is\n3.00 2.00 3.16 2.24 1.41 1.73\n\n\n\nWith \\(K=1\\), our prediction for \\(y\\) is “green”, as the nearest neighbor is “green”.\n\n\n\nWith \\(K=3\\), our prediction for \\(y\\) is “red”, as the nearest neighbors are 1 “green” and 2 “red”.\n\n\n\nIf the Bayes Decision Boundary is highly non-linear, then we would expect the best value of \\(K\\) to be small, because a smaller \\(K\\) allows us more flexibility in the estimated decision boundary."
  },
  {
    "objectID": "Chapter2e.html#applied",
    "href": "Chapter2e.html#applied",
    "title": "Chapter 2 (Exercises)",
    "section": "",
    "text": "data(College)\ncollege &lt;- College\nrm(College)\n\n\n\n\n\nrownames(college) &lt;- college[,1]\nfix(college)\n\n\n\n\n\n\n Private        Apps           Accept          Enroll       Top10perc    \n No :212   Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00  \n Yes:565   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00  \n           Median : 1558   Median : 1110   Median : 434   Median :23.00  \n           Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56  \n           3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00  \n           Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00  \n   Top25perc      F.Undergrad     P.Undergrad         Outstate    \n Min.   :  9.0   Min.   :  139   Min.   :    1.0   Min.   : 2340  \n 1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320  \n Median : 54.0   Median : 1707   Median :  353.0   Median : 9990  \n Mean   : 55.8   Mean   : 3700   Mean   :  855.3   Mean   :10441  \n 3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925  \n Max.   :100.0   Max.   :31643   Max.   :21836.0   Max.   :21700  \n   Room.Board       Books           Personal         PhD        \n Min.   :1780   Min.   :  96.0   Min.   : 250   Min.   :  8.00  \n 1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00  \n Median :4200   Median : 500.0   Median :1200   Median : 75.00  \n Mean   :4358   Mean   : 549.4   Mean   :1341   Mean   : 72.66  \n 3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00  \n Max.   :8124   Max.   :2340.0   Max.   :6800   Max.   :103.00  \n    Terminal       S.F.Ratio      perc.alumni        Expend     \n Min.   : 24.0   Min.   : 2.50   Min.   : 0.00   Min.   : 3186  \n 1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751  \n Median : 82.0   Median :13.60   Median :21.00   Median : 8377  \n Mean   : 79.7   Mean   :14.09   Mean   :22.74   Mean   : 9660  \n 3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830  \n Max.   :100.0   Max.   :39.80   Max.   :64.00   Max.   :56233  \n   Grad.Rate     \n Min.   : 10.00  \n 1st Qu.: 53.00  \n Median : 65.00  \n Mean   : 65.46  \n 3rd Qu.: 78.00  \n Max.   :118.00  \n\n\n\n\n\n\n\n\n\n\n\nAs per the summary() function, there are 78 elite universities. The box-plot of Outstate vs. Elite is as below :—\n\n\nwith(college, plot(x=Elite, y=Outstate, ylab = \"Out-of-State Tuition\", xlab = \"Whether Elite College?\", main = \"Question 8(c)(iv)\"))\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nhist(college$Room.Board, main = \"\", sub = \"Room Boarding Charges\")\nhist(college$Room.Board, breaks = 100, main=\"\", sub = \"Room Boarding Charges(100 bins)\")\nhist(college$Books, main = \"\", sub = \"Expenses on Books\")\nhist(college$Books, breaks = 100, main=\"\", sub = \"Expenses on Books\")\n\n\n\n\n\n\n\n\n\n\n\ndata(Auto)\nAuto &lt;- na.omit(Auto)\n\nThe predictors Origin and Name in the Auto data set are qualitative, while all the other variables are quantitative, i.e. mpg, cylinders, displacement, horsepower, weight, acceleration, year.\n\n\n\n\nMin &lt;- rep(0, 7)\nMax &lt;- rep(0, 7)\nStDev &lt;- rep(0, 7)\nMean &lt;- rep(0, 7)\nName &lt;- colnames(Auto)[-c(8, 9)]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n9\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1613\n5140\n\n\nacceleration\n8\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n23.45\n7.81\n\n\ncylinders\n5.47\n1.71\n\n\ndisplacement\n194.41\n104.64\n\n\nhorsepower\n104.47\n38.49\n\n\nweight\n2977.58\n849.4\n\n\nacceleration\n15.54\n2.76\n\n\nyear\n75.98\n3.68\n\n\n\n\n\n\n\n\n\n\n\nAuto1 &lt;- Auto[-c(10:85), ]\nfor (i in 1:7) {\n  Min[i] &lt;- min(Auto1[, i])\n}\nfor (i in 1:7) {\n  Max[i] &lt;- max(Auto1[, i])\n}\nfor (i in 1:7) {\n  Mean[i] &lt;- round(mean(Auto1[, i]), digits = 2)\n}\nfor (i in 1:7) {\n  StDev[i] &lt;- round(sd(Auto1[, i]), digits = 2)\n}\nrangetab &lt;- as.data.frame(cbind(Name, Min, Max))\nmsdtab &lt;- as.data.frame(cbind(Name, Mean, StDev))\nrangetab |&gt;\n  kbl() |&gt;\n  kable_classic_2(full_width = FALSE)\n\n\n\n\nName\nMin\nMax\n\n\n\n\nmpg\n11\n46.6\n\n\ncylinders\n3\n8\n\n\ndisplacement\n68\n455\n\n\nhorsepower\n46\n230\n\n\nweight\n1649\n4997\n\n\nacceleration\n8.5\n24.8\n\n\nyear\n70\n82\n\n\n\n\n\n\nmsdtab |&gt;\n  kbl() |&gt;\n  kable_classic(full_width = FALSE)\n\n\n\n\nName\nMean\nStDev\n\n\n\n\nmpg\n24.4\n7.87\n\n\ncylinders\n5.37\n1.65\n\n\ndisplacement\n187.24\n99.68\n\n\nhorsepower\n100.72\n35.71\n\n\nweight\n2935.97\n811.3\n\n\nacceleration\n15.73\n2.69\n\n\nyear\n77.15\n3.11\n\n\n\n\n\n\n\n\n\n\n\npairs(Auto[, c(1, 3, 4, 5, 6)])\n\n\n\nmodel &lt;- lm(mpg ~ . - name, data = Auto)\ncor &lt;- as.data.frame(model$coefficients[-1])\ncolnames(cor) &lt;- \"Coefficient of Regression\"\ncor |&gt;\n  kbl() |&gt;\n  kable_classic_2()\n\n\n\n\n\nCoefficient of Regression\n\n\n\n\ncylinders\n-0.4933763\n\n\ndisplacement\n0.0198956\n\n\nhorsepower\n-0.0169511\n\n\nweight\n-0.0064740\n\n\nacceleration\n0.0805758\n\n\nyear\n0.7507727\n\n\norigin\n1.4261405\n\n\n\n\n\n\n\nThe above table shows regression coefficients of other variables with mpg.\n\n\n\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n# VarDescrip &lt;- read.csv(\"BostonVarDescrip.csv\")\nnrow(Boston)\n\n[1] 506\n\nncol(Boston)\n\n[1] 14\n\n\nThere are total 506 rows in the data-set and 14 columns. The rows represent the 506 neighborhoods and each column represents the following :—\n\n# VarDescrip |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\nThe pairwise plots of some important variables is as below : —\n\nBoston1 &lt;- Boston |&gt; dplyr::select(crim, nox, dis, tax, lstat)\npairs(Boston1)\n\n\n\n\n\n\n\nThe linear regression shows us the following result :—\n\nmdl &lt;- lm(crim ~ ., data = Boston)\nsummary(mdl)\n\n\nCall:\nlm(formula = crim ~ ., data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.924 -2.120 -0.353  1.019 75.051 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.033228   7.234903   2.354 0.018949 *  \nzn            0.044855   0.018734   2.394 0.017025 *  \nindus        -0.063855   0.083407  -0.766 0.444294    \nchas         -0.749134   1.180147  -0.635 0.525867    \nnox         -10.313535   5.275536  -1.955 0.051152 .  \nrm            0.430131   0.612830   0.702 0.483089    \nage           0.001452   0.017925   0.081 0.935488    \ndis          -0.987176   0.281817  -3.503 0.000502 ***\nrad           0.588209   0.088049   6.680 6.46e-11 ***\ntax          -0.003780   0.005156  -0.733 0.463793    \nptratio      -0.271081   0.186450  -1.454 0.146611    \nblack        -0.007538   0.003673  -2.052 0.040702 *  \nlstat         0.126211   0.075725   1.667 0.096208 .  \nmedv         -0.198887   0.060516  -3.287 0.001087 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.439 on 492 degrees of freedom\nMultiple R-squared:  0.454, Adjusted R-squared:  0.4396 \nF-statistic: 31.47 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nThus, per capital crime rate is significantly related with the variables zn, dis, rad, black and medv.\n\n\n\nYes, there are some suburbs with abnormally high per-capita crime rate. But no such trend is visible in Tax-Rates or Pupil-Teacher Ratio. The plots of these three variables are shown below:—\n\nattach(Boston)\npar(mfrow=c(2,3))\nplot(crim)\nplot(tax)\nplot(ptratio)\nboxplot(crim)\nboxplot(tax)\nboxplot(ptratio)\n\n\n\n\n\n\n\n35 suburbs in this data set bound the Charles river. This is found using the code:—\n\nnrow(subset(Boston, chas == 1))\n\n[1] 35\n\n\n\n\n\nThe median pupil-teacher ratio is 19.05, found using the code:—\n\nmedian(Boston$ptratio)\n\n\n\n\nThe suburb of Boston with lowest median value of ownership occupied homes is 399. Its values for other variables are:—\n\nas.data.frame(Boston[which.min(Boston$medv),]) |&gt; kbl() |&gt; kable_classic_2()\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nblack\nlstat\nmedv\n\n\n\n\n399\n38.3518\n0\n18.1\n0\n0.693\n5.453\n100\n1.4896\n24\n666\n20.2\n396.9\n30.59\n5\n\n\n\n\n\n\n\n\n\n\nIn this data set, 13 suburbs average more than 8 rooms per dwelling and 64 suburbs average more than 7 rooms per dwelling. The code to find these values is :—\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;8))\n\n[1] 13\n\nnrow(Boston |&gt; dplyr::filter(rm&gt;7))\n\n[1] 64"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my corner of the web! I’m Aditya Dahiya, an IAS (Indian Administrative Service) officer navigating the intricate labyrinth of the Indian Administrative Service in the Government of Haryana. While my day job might involve more paperwork than I care to admit, my true passion lies in the world of health data visualization.\nMy journey began with a stethoscope around my neck and a medical degree from the AIIMS, New Delhi, earned from 2005 to 2010. But in 2011, I decided to swap the scrubs for a suit and tie and jumped headfirst into the world of civil service. Fast forward to 2021-22, and I found myself on the hallowed grounds of Harvard University, where I earned an MPH degree, because why not?\nNow, you might wonder how someone with a day job in bureaucracy and a penchant for health data visualization ended up creating solutions for the first edition of “An Introduction to Statistical Learning: With Applications in R.” Well, my secret lies in seizing those precious moments of free time. And during one such moment, I embarked on a journey of statistical enlightenment by enrolling in Johns Hopkins University’s 140.644.01 - Statistical Machine Learning course.\nThe result? A treasure trove of solutions that I’m delighted to share with you. Whether you’re a fellow learner, an aspiring statistician, or just a curious soul, this website is your go-to resource for unraveling the mysteries of statistical learning, sprinkled with a dash of wit and a pinch of my unique perspective.\nSo, join me on this data-driven adventure at LinkedIn or shoot me an email. Even bureaucrats can have a little statistical swagger!"
  },
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nChapter 10 (Lab)\n\n\nUnsupervised Learning\n\n\nMar 1, 2021\n\n\n\n\nChapter 2 (Exercises)\n\n\nStatistical Learning\n\n\nJan 31, 2021\n\n\n\n\nChapter 3 (Exercises)\n\n\nLinear Regression\n\n\nFeb 1, 2021\n\n\n\n\nChapter 3 (R Lab)\n\n\nLinear Regression (Lab)\n\n\nFeb 1, 2021\n\n\n\n\nChapter 4 (Exercises)\n\n\nClassification\n\n\nFeb 7, 2021\n\n\n\n\nChapter 4 (Lab)\n\n\nClassification\n\n\nFeb 6, 2021\n\n\n\n\nChapter 5 (Exercises)\n\n\nResampling Methods\n\n\nFeb 14, 2021\n\n\n\n\nChapter 5 (Lab)\n\n\nResampling Methods\n\n\nFeb 13, 2021\n\n\n\n\nChapter 6 (Exercises)\n\n\nLinear Model Selection and Regularization\n\n\nFeb 21, 2021\n\n\n\n\nChapter 6 (Lab)\n\n\nLinear Model Selection and Regularization\n\n\nFeb 20, 2021\n\n\n\n\nChapter 7 (Lab)\n\n\nMoving Beyond Linearity\n\n\nFeb 24, 2021\n\n\n\n\nChapter 8 (Lab)\n\n\nTree-Based Methods\n\n\nMar 11, 2021\n\n\n\n\nChapter 9 (Lab)\n\n\nSupport Vector Machines\n\n\nMar 4, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Solutions: ISLR 1e",
    "section": "",
    "text": "Author: Aditya Dahiya\nThis website displays the solutions for the exercises in the book An Introduction to Statistical Learning: with Applications in R (1st Edition)."
  },
  {
    "objectID": "index.html#an-introduction-to-statistical-learning-with-applications-in-r-1st-edition",
    "href": "index.html#an-introduction-to-statistical-learning-with-applications-in-r-1st-edition",
    "title": "Solutions: ISLR 1e",
    "section": "",
    "text": "Author: Aditya Dahiya\nThis website displays the solutions for the exercises in the book An Introduction to Statistical Learning: with Applications in R (1st Edition)."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nHarvard University | Boston, MA MPH in Global Health | Aug 2021 - May 2022\nAll India Institute of Medical Sciences | New Delhi, India MBBS in Medicine | Aug 2005-Dec 2010\nIndira Gandhi National Open University | New Delhi, India MA in Public Policy | Jan 2012 - Dec 2013"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nIndian Administrative Service | Director | Aug 2011 - present\nNeuro-Radiology, AIIMS New Delhi | Junior Resident Doctor | Jan 2011 - Aug 2011"
  },
  {
    "objectID": "Chapter3e.html",
    "href": "Chapter3e.html",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The figure is as below:—\n\n\n\n\nThe p-values in this table refer to the null hypothesis \\(H_o : \\beta_i = 0\\), where \\(\\beta_i\\) refers to the Coefficient relating to the concerned variable.\nBased on these p-values, I conclude that TV and radio are significant predictors of variation in sales, whereas newspaper is not a statistically significant predictor of variation in sales.\n\n\n\n\n\n\nKNN Classifier is a method used for modeling categorical or qualitative responses. Here, the classifier returns the category of response which is most likely, given the values of predictors in \\(N_o\\) i.e. K-nearest neighbors. KNN classifier first identifies the K points in the training data that are closest to \\(x_o\\), represented by \\(\\mathcal{N}_o\\). It then estimates the conditional probability for class \\(j\\) as the fraction of points in \\(\\mathcal{N}_o\\) whose response values equal \\(j\\)\n\\[Pr(Y = j |x=x_o) =  \\frac{1}{K} \\sum_{x_i \\in N_o} I(y_i = j)\\]\nFinally, KNN applies Bayes rule and classifies the test observation \\(x_o\\) to the class with the largest probability. - KNN Regression is a method which models continuous outcomes or responses based on predictors. It calculates the predicted outcome as the average of responses for the K-nearest neighbors of the given \\(x_o\\), i.e. :—\n\\[\\hat{f}(x_o) = \\frac{1}{K} \\sum_{x_i \\in N_o} y_i \\] \n\n\n\n\n\n\n\nThis statement is incorrect because coefficient \\(\\hat{\\beta}_3\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0)\n\nThis statement is incorrect because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0), but only if the GPA is less than 3.5. This is because the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\). Thus above a certain value, males will have higher earnings.\n\nThis statement is correct because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive but the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\), meaning thereby that females (coded as 1) earn more than males (coded as 0) only up to a certain GPA level, above which the males earn more.\n\nThis statement is incorrect for reasons explained above in (iii).\n\n\n\n\nThe salary of a female with IQ of 110 and a GPA of 4.0 is 137.1, as calculated below :–\n\n\\(Salary = \\beta_0 + (\\beta_1*4) + (\\beta_2 * 110) + (\\beta_3*1) + (\\beta_4*110*4) + (\\beta_5*4*1)\\)\n\n\\(Salary = 50 + (20*4) + (0.07 * 110) + (35*1) + (0.01*110*4) + (-10*4*1)\\)\n\n\\(Salary = 137.1\\)\n\n\n\n\nFalse. The magnitude of a coefficient does not tell us anything about the significance of the interaction amongst GPA and IQ. This is because the magnitude / value of a coefficient depends on the units of predictor variables chosen. Rather, the standard error and \\(p\\)-value associated with the coefficient will tell us whether there is any evidence of an interaction or not.\n\n\n\n\n\n\n\nThe Training Residual Sum of Squares (RSS) for the cubic regression will always be lower than the simple linear regression, even if the true population model is linear i.e. \\(Y = \\beta_o + \\beta_1(X) + \\epsilon\\). This is because more terms in the cubic regression provide more flexibility and thus allows the model to fit each data point more closely. Thus, the RSS is reduced. This is not good thing for us though, as the cubic regression will lead to overfitting.\n\n\n\n\nFor the Test Residual Sum of Squares, the simple linear regression will have a lower RSS than cubin regression. This happens because our true population model is linear, so cubic regression leads to overfitting. This causes Test RSS to rise, as compared to linear regression.\n\n\n\n\nEven if we do not know the form of true population relation between \\(Y\\) and \\(x\\), the cubic regression will always have a lower Training RSS. This is because the cubic regression is a more flexible model, and thus always fits the training data more closely leading to a lower Training RSS.\n\n\n\n\nFor the Test RSS, when the form true relationship between \\(Y\\) and \\(x\\) is unknown, we cannot predict whether the linear regression or cubic regression will perform better. This will be an empirical question. Which of the two models has a lower Test RSS will depend on the exact relation between \\(Y\\) and \\(x\\), and how closely it is modeled by the linear or cubic regression.\n\n\n\n\nIn this question, we are considering the fitted values from a linear regression without an intercept, where\n\\[ \\hat{y_i} = x_i \\hat{\\beta_i} \\]\n\nand,\n\n\\[\\hat{\\beta}_{i} = \\frac{\\sum_{i=1}^{n}x_{i} y_{i}}{\\sum_{i'=1}^{n} x_{i'}^{2}}\\]\n\nThus, we can easily replace the value of \\(\\hat{\\beta_i}\\) in the equation of \\(y_i\\) and solve for \\(a_i\\) as follows —\n\\[ \\hat{y_i} = x_i \\times \\hat{\\beta_i} \\] \\[ \\hat{y}_{i} = x_{i} \\times \\frac{\\sum_{i'=1}^{n} x_{i'} y_{i'}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\frac{ x_{i'} y_{i'} \\times x_{i}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\left ( \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\times y_{i'} \\right ) \\]\nThus, we can easily re-write the expression as\n\\[ \\hat{y_i} = \\sum_{i'=1}^{n} a_{i'}y_i  \\]\nAs a result, the value of \\(a_i\\) is —\n\\[ a_{i'} = \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\]\nAlso, this means that we can interpret this result by saying that the fitted values (\\(\\hat{y_i}\\)) from a linear regression are linear combination of the response values (\\(y_{i'}\\)).\n\n\n\nThe least squares regression line is\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i \\]\nwhere, \\(beta_o\\) and \\(beta_1\\) are defined as —\n\\[\\hat{\\beta_o} = \\overline{y} - \\hat{\\beta_1} \\overline{x}\\] \\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\overline{x}) (y_{i}-\\overline{y})}{\\sum_{i=1}^{n} (x_{i}-\\overline{x})^{2}}\\]\nNow, we need to prove that when \\(x_i = \\overline{x}\\), then \\(\\hat{y_i} = \\overline{y}\\). For this, we substitute the value of \\(\\beta_o\\) with \\(\\overline{y} - \\hat{\\beta_1} \\overline{x}\\) and set the value of \\(x_i\\) as \\(\\overline{x}\\),\n\\[ \\hat{y_i} = \\hat{\\beta_o} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline{y} - \\hat{\\beta_1} \\overline{x} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline {y}\\]\nThus, the point (\\(\\overline{x},\\overline{y}\\)) will always lie on the least squares regression line.\n\n\n\n\nKeeping the values of \\(\\overline{x}\\) and \\(\\overline{y}\\) as 0, the terms \\(R^2\\) and \\(Cor(X,Y)\\) are defined as follows \\[ R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum(y_j-\\overline{y})^2} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum{y_j}^2}\\]\n\\[ Cor(X,Y) = \\frac{\\sum_{n}(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum{x_j} \\sum{y_j}} = \\frac{\\sum_{n}x_iy_i}{\\sum{x_j} \\sum{y_j}}\\]\nNow, keeping \\(\\overline{y}\\) and \\(\\overline{x}\\) as zero, we can substitute the value of \\(\\hat{y_i}\\) in \\(R^2\\) as:—\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i = 0+ \\frac{\\sum(x_{i} - 0) (y_{i}-0)}{\\sum(x_{i}-0)^{2}} = \\frac{\\sum{x_iy_i}}{\\sum{x_i}^2}\\]\nThus, we have the following expression for \\(R^2\\), \\[R^2 = 1 - \\frac{\\sum_i(y_i - \\sum_jx_jy_j/\\sum_jx_j^2 x_i)^2}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{\\sum_jy_j^2 - (\\sum_iy_i^2 - 2\\sum_iy_i(\\sum_jx_jy_j/\\sum_jx_j^2)x_i + \\sum_i(\\sum_jx_jy_j/\\sum_jx_j^2)^2x_i^2)}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{2(\\sum_ix_iy_i)^2/\\sum_jx_j^2 - (\\sum_ix_iy_i)^2/\\sum_jx_j^2}{\\sum_jy_j^2}\\] \\[ R^2 = \\frac{(\\sum_ix_iy_i)^2}{\\sum_jx_j^2\\sum_jy_j^2} = Cor(X, Y)^2.\\]"
  },
  {
    "objectID": "Chapter3e.html#question-1",
    "href": "Chapter3e.html#question-1",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The figure is as below:—\n\n\n\n\nThe p-values in this table refer to the null hypothesis \\(H_o : \\beta_i = 0\\), where \\(\\beta_i\\) refers to the Coefficient relating to the concerned variable.\nBased on these p-values, I conclude that TV and radio are significant predictors of variation in sales, whereas newspaper is not a statistically significant predictor of variation in sales."
  },
  {
    "objectID": "Chapter3e.html#question-2",
    "href": "Chapter3e.html#question-2",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "KNN Classifier is a method used for modeling categorical or qualitative responses. Here, the classifier returns the category of response which is most likely, given the values of predictors in \\(N_o\\) i.e. K-nearest neighbors. KNN classifier first identifies the K points in the training data that are closest to \\(x_o\\), represented by \\(\\mathcal{N}_o\\). It then estimates the conditional probability for class \\(j\\) as the fraction of points in \\(\\mathcal{N}_o\\) whose response values equal \\(j\\)\n\\[Pr(Y = j |x=x_o) =  \\frac{1}{K} \\sum_{x_i \\in N_o} I(y_i = j)\\]\nFinally, KNN applies Bayes rule and classifies the test observation \\(x_o\\) to the class with the largest probability. - KNN Regression is a method which models continuous outcomes or responses based on predictors. It calculates the predicted outcome as the average of responses for the K-nearest neighbors of the given \\(x_o\\), i.e. :—\n\\[\\hat{f}(x_o) = \\frac{1}{K} \\sum_{x_i \\in N_o} y_i \\]"
  },
  {
    "objectID": "Chapter3e.html#question-3",
    "href": "Chapter3e.html#question-3",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "This statement is incorrect because coefficient \\(\\hat{\\beta}_3\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0)\n\nThis statement is incorrect because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive, meaning thereby that females (coded as 1) earn more than males _(coded as 0), but only if the GPA is less than 3.5. This is because the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\). Thus above a certain value, males will have higher earnings.\n\nThis statement is correct because coefficient \\(\\hat{\\beta}_3 = 35\\) is positive but the coefficient of interaction term for Gender and GPA \\(\\hat{\\beta}_5 = -10\\), meaning thereby that females (coded as 1) earn more than males (coded as 0) only up to a certain GPA level, above which the males earn more.\n\nThis statement is incorrect for reasons explained above in (iii).\n\n\n\n\nThe salary of a female with IQ of 110 and a GPA of 4.0 is 137.1, as calculated below :–\n\n\\(Salary = \\beta_0 + (\\beta_1*4) + (\\beta_2 * 110) + (\\beta_3*1) + (\\beta_4*110*4) + (\\beta_5*4*1)\\)\n\n\\(Salary = 50 + (20*4) + (0.07 * 110) + (35*1) + (0.01*110*4) + (-10*4*1)\\)\n\n\\(Salary = 137.1\\)\n\n\n\n\nFalse. The magnitude of a coefficient does not tell us anything about the significance of the interaction amongst GPA and IQ. This is because the magnitude / value of a coefficient depends on the units of predictor variables chosen. Rather, the standard error and \\(p\\)-value associated with the coefficient will tell us whether there is any evidence of an interaction or not."
  },
  {
    "objectID": "Chapter3e.html#question-4",
    "href": "Chapter3e.html#question-4",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The Training Residual Sum of Squares (RSS) for the cubic regression will always be lower than the simple linear regression, even if the true population model is linear i.e. \\(Y = \\beta_o + \\beta_1(X) + \\epsilon\\). This is because more terms in the cubic regression provide more flexibility and thus allows the model to fit each data point more closely. Thus, the RSS is reduced. This is not good thing for us though, as the cubic regression will lead to overfitting.\n\n\n\n\nFor the Test Residual Sum of Squares, the simple linear regression will have a lower RSS than cubin regression. This happens because our true population model is linear, so cubic regression leads to overfitting. This causes Test RSS to rise, as compared to linear regression.\n\n\n\n\nEven if we do not know the form of true population relation between \\(Y\\) and \\(x\\), the cubic regression will always have a lower Training RSS. This is because the cubic regression is a more flexible model, and thus always fits the training data more closely leading to a lower Training RSS.\n\n\n\n\nFor the Test RSS, when the form true relationship between \\(Y\\) and \\(x\\) is unknown, we cannot predict whether the linear regression or cubic regression will perform better. This will be an empirical question. Which of the two models has a lower Test RSS will depend on the exact relation between \\(Y\\) and \\(x\\), and how closely it is modeled by the linear or cubic regression."
  },
  {
    "objectID": "Chapter3e.html#question-5",
    "href": "Chapter3e.html#question-5",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "In this question, we are considering the fitted values from a linear regression without an intercept, where\n\\[ \\hat{y_i} = x_i \\hat{\\beta_i} \\]\n\nand,\n\n\\[\\hat{\\beta}_{i} = \\frac{\\sum_{i=1}^{n}x_{i} y_{i}}{\\sum_{i'=1}^{n} x_{i'}^{2}}\\]\n\nThus, we can easily replace the value of \\(\\hat{\\beta_i}\\) in the equation of \\(y_i\\) and solve for \\(a_i\\) as follows —\n\\[ \\hat{y_i} = x_i \\times \\hat{\\beta_i} \\] \\[ \\hat{y}_{i} = x_{i} \\times \\frac{\\sum_{i'=1}^{n} x_{i'} y_{i'}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\frac{ x_{i'} y_{i'} \\times x_{i}}{\\sum_{j=1}^{n} x_{j}^{2}} \\] \\[ \\hat{y}_{i} = \\sum_{i'=1}^{n} \\left ( \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\times y_{i'} \\right ) \\]\nThus, we can easily re-write the expression as\n\\[ \\hat{y_i} = \\sum_{i'=1}^{n} a_{i'}y_i  \\]\nAs a result, the value of \\(a_i\\) is —\n\\[ a_{i'} = \\frac{ x_{i} x_{i'} } { \\sum_{j=1}^{n} x_{j}^{2} } \\]\nAlso, this means that we can interpret this result by saying that the fitted values (\\(\\hat{y_i}\\)) from a linear regression are linear combination of the response values (\\(y_{i'}\\))."
  },
  {
    "objectID": "Chapter3e.html#question-6",
    "href": "Chapter3e.html#question-6",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "The least squares regression line is\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i \\]\nwhere, \\(beta_o\\) and \\(beta_1\\) are defined as —\n\\[\\hat{\\beta_o} = \\overline{y} - \\hat{\\beta_1} \\overline{x}\\] \\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\overline{x}) (y_{i}-\\overline{y})}{\\sum_{i=1}^{n} (x_{i}-\\overline{x})^{2}}\\]\nNow, we need to prove that when \\(x_i = \\overline{x}\\), then \\(\\hat{y_i} = \\overline{y}\\). For this, we substitute the value of \\(\\beta_o\\) with \\(\\overline{y} - \\hat{\\beta_1} \\overline{x}\\) and set the value of \\(x_i\\) as \\(\\overline{x}\\),\n\\[ \\hat{y_i} = \\hat{\\beta_o} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline{y} - \\hat{\\beta_1} \\overline{x} + \\hat{\\beta_1}\\overline{x} \\]\n\\[ \\hat{y_i} = \\overline {y}\\]\nThus, the point (\\(\\overline{x},\\overline{y}\\)) will always lie on the least squares regression line."
  },
  {
    "objectID": "Chapter3e.html#question-7",
    "href": "Chapter3e.html#question-7",
    "title": "Chapter 3 (Exercises)",
    "section": "",
    "text": "Keeping the values of \\(\\overline{x}\\) and \\(\\overline{y}\\) as 0, the terms \\(R^2\\) and \\(Cor(X,Y)\\) are defined as follows \\[ R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum(y_j-\\overline{y})^2} = 1 - \\frac{\\sum(y_i - \\hat{y_i})^2}{\\sum{y_j}^2}\\]\n\\[ Cor(X,Y) = \\frac{\\sum_{n}(x_i-\\overline{x})(y_i-\\overline{y})}{\\sum{x_j} \\sum{y_j}} = \\frac{\\sum_{n}x_iy_i}{\\sum{x_j} \\sum{y_j}}\\]\nNow, keeping \\(\\overline{y}\\) and \\(\\overline{x}\\) as zero, we can substitute the value of \\(\\hat{y_i}\\) in \\(R^2\\) as:—\n\\[ y_i = \\hat{\\beta_o} + \\hat{\\beta_1}x_i = 0+ \\frac{\\sum(x_{i} - 0) (y_{i}-0)}{\\sum(x_{i}-0)^{2}} = \\frac{\\sum{x_iy_i}}{\\sum{x_i}^2}\\]\nThus, we have the following expression for \\(R^2\\), \\[R^2 = 1 - \\frac{\\sum_i(y_i - \\sum_jx_jy_j/\\sum_jx_j^2 x_i)^2}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{\\sum_jy_j^2 - (\\sum_iy_i^2 - 2\\sum_iy_i(\\sum_jx_jy_j/\\sum_jx_j^2)x_i + \\sum_i(\\sum_jx_jy_j/\\sum_jx_j^2)^2x_i^2)}{\\sum_jy_j^2}\\] \\[R^2 = \\frac{2(\\sum_ix_iy_i)^2/\\sum_jx_j^2 - (\\sum_ix_iy_i)^2/\\sum_jx_j^2}{\\sum_jy_j^2}\\] \\[ R^2 = \\frac{(\\sum_ix_iy_i)^2}{\\sum_jx_j^2\\sum_jy_j^2} = Cor(X, Y)^2.\\]"
  },
  {
    "objectID": "Chapter3e.html#question-8",
    "href": "Chapter3e.html#question-8",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 8",
    "text": "Question 8\n\n(a)\n\nlibrary(ISLR)\ndata(Auto)\nfit1 &lt;- lm(mpg ~ horsepower, data = Auto)\nsummary(fit1)\n\n\nCall:\nlm(formula = mpg ~ horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n\n\nThere is a relationship between response and predictor, as the overall F-statistic and t-value for the predictor horsepower are statistically significant. The p-value is very close to zero.\n\nThe strength of the relationship between the response mpg and predictor horsepower is given by the coefficient of regression i.e. -0.16, with a 95% confidence interval of [-0.17, -0.15].\n\nThe relation between the response and predictor is negative, as the coefficient for horsepower is negative, i.e. -0.16.\n\nThe predicted mpg for a horsepower value of 98, with the confidence and prediction intervals is as follows:\n\n\n\nround(predict(fit1, data.frame(horsepower = c(98)), interval = \"confidence\"), digits = 3)\n\n     fit    lwr    upr\n1 24.467 23.973 24.961\n\nround(predict(fit1, data.frame(horsepower = c(98)), interval = \"prediction\"), digits = 3)\n\n     fit    lwr    upr\n1 24.467 14.809 34.125\n\n\n\n\n(b)\n\nplot(x = Auto$horsepower, y = Auto$mpg)\nabline(fit1, lwd = 3, col = \"red\")\n\n\n\n\n\n\n(c)\nThe diagnostic plots, along with the required code to produce them, are displayed below. The Residuals vs Fitted plot shows that the residuals are not evenly distributed, and thus there is a non-linear relation between the response and predictor. The Normal Q-Q plot also shows evidence on whether the residuals are normal distributed. The current evidence shows that the distribution is somewhat normal. The Spread-Location plot of \\(\\sqrt{standardized \\ residuals}\\) vs. fitted values shows the spread of residuals along the range of predictor values, allowing us to test homoskedasticity. Here, the model fit1 suffers from heteroskedasticity. The last plot of Residuals vs. Leverage shows us whether any observations are high-leverage. Here, none of the observations appear outside the dashed lines and thus there is no influential observation which is altering the results of the regression model.\n\npar(mfrow = c(2, 2))\nplot(fit1)\n\n\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "Chapter3e.html#question-9",
    "href": "Chapter3e.html#question-9",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 9",
    "text": "Question 9\n\n(a)\nA scatter-plot matrix of all the variables in Auto data set is produced using the function pairs() as follows :—\n\n\nlibrary(ISLR)\ndata(Auto)\npairs(Auto)\n\n\n\n\n\n\n(b)\nThe correlation matrix between variables of Auto is shown below, after excluding the variable name. We can use the base R command cor. However, a more beautiful tabular display is shown below using the package corrplot.\n\ncor(Auto[, -9])\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\nlibrary(kableExtra)\nlibrary(corrplot)\ncorrplot(cor(Auto[, -9]), method = \"number\", tl.col = \"black\", sig.level = 0.05)\n\n\n\n\n\n\n(c)\nThe multiple linear regression of mpg on all other variables, except name is displayed below:\n\n\nfit2 &lt;- lm(mpg ~ . - name, data = Auto)\nsummary(fit2)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -17.218435   4.644294  -3.707  0.00024 ***\ncylinders     -0.493376   0.323282  -1.526  0.12780    \ndisplacement   0.019896   0.007515   2.647  0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230  0.21963    \nweight        -0.006474   0.000652  -9.929  &lt; 2e-16 ***\nacceleration   0.080576   0.098845   0.815  0.41548    \nyear           0.750773   0.050973  14.729  &lt; 2e-16 ***\norigin         1.426141   0.278136   5.127 4.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe output shows that there is a relationship between response and predictors. The F-statistic of 252.43 (generated by summary(fit2)$fstatistic[1]) has a p-value of nearly zero.\n\nThe predictors which appear to have a statistically significant relationship to the response are displacement, weight, year and origin.\n\nThe coefficient for year is 0.751 (round(fit2$coef[7],3)). This suggests that with each year, the value of miles per gallon mpg increases by 0.75, provided all other variables remain constant. In other words, with each year, fuel efficiency increases by 0.75 [95% CI = 0.65, 0.85] mpg/year if other variables remain the same.\n\n\n\n\n(d)\nThe diagnostic plots for this multiple linear regression are shown below.\n\n\npar(mfrow = c(2, 2))\nplot(fit2)\n\n\n\n\n\nResiduals vs Fitted Plot shows whether the residuals from multiple linear regression have non-linear patterns. We find that the residuals are not evenly spread around the horizontal line. Perhaps there is some non-linear relationship between the response and predictors, which is not captured by the linear model we have fitted.\nNormal Q-Q Plot shows us whether the residuals are normally distributed. Since, residuals follow the straight line when plotted with theoretical quantiles of a normal distribution, this plot suggests that the residuals are somewhat normally distributed, barring a few observations with high residual values.\nScale-Location Plot checks the assumption of homoskedasticity. The plot suggests some amount of heteroskedasticity.\nResiduals vs Leverage Plot helps us to find influential cases with high leverage and outliers (high absolute standardized residual value). In this data set, around 5 observations like 323, 326, 327, 394 etc. are outliers and observation 14 has a high leverage. But, there is no observation outside of the dashed line, the Cook’s distance, thus there is no observation which is significantly affecting the regression results.\n\n\n\n\n(e)\nTo fit various interaction terms in the Multiple linear regression model, we use two strategies :—\n\n\nFirst, we use three interactions to fit to the model, using the three variable pairs which carry the highest correlation from the correlation matrix shown in part (b) above: \\(displacement \\times cylinders\\) , \\(displacement \\times weight\\) and \\(weight \\times cylinders\\).\nThe results below suggest that only the interaction between displacement and weight is statistically significant. However, using predictor variables with high correlation means that our fitted model may suffer from multicollinearity.\n\n\n\nfit3 &lt;- lm(mpg ~ . - name + displacement:cylinders + displacement:weight + weight:cylinders, data = Auto)\nsummary(fit3)\n\n\nCall:\nlm(formula = mpg ~ . - name + displacement:cylinders + displacement:weight + \n    weight:cylinders, data = Auto)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9771 -1.8142 -0.0589  1.6557 12.1181 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -6.830e+00  6.090e+00  -1.122 0.262778    \ncylinders               5.961e-01  1.536e+00   0.388 0.698109    \ndisplacement           -8.595e-02  3.140e-02  -2.738 0.006480 ** \nhorsepower             -3.552e-02  1.319e-02  -2.693 0.007404 ** \nweight                 -9.252e-03  2.355e-03  -3.929 0.000101 ***\nacceleration            6.027e-02  8.905e-02   0.677 0.498978    \nyear                    7.831e-01  4.574e-02  17.120  &lt; 2e-16 ***\norigin                  5.193e-01  2.706e-01   1.919 0.055685 .  \ncylinders:displacement  1.923e-03  3.094e-03   0.622 0.534615    \ndisplacement:weight     2.386e-05  6.162e-06   3.872 0.000127 ***\ncylinders:weight       -2.515e-04  5.047e-04  -0.498 0.618481    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.97 on 381 degrees of freedom\nMultiple R-squared:  0.8589,    Adjusted R-squared:  0.8552 \nF-statistic:   232 on 10 and 381 DF,  p-value: &lt; 2.2e-16\n\n\n\nSecondly, we can create a lot of permutations of variables to create various interaction terms and use R code to select the interaction terms with significant p-values (&lt;0.05). This is shown below:—\n\n\n\nfit4 &lt;- lm(mpg ~ cylinders * displacement + cylinders * horsepower + cylinders * weight + cylinders * acceleration + cylinders * year + cylinders * origin + displacement * horsepower + displacement * weight + displacement * acceleration + displacement * year + displacement * origin + horsepower * weight + horsepower * acceleration + horsepower * year + horsepower * origin + weight * acceleration + weight * year + weight * origin + acceleration * year + acceleration * origin + year * origin, data = Auto)\n\nround(subset(summary(fit4)$coef[, 4], summary(fit4)$coef[, 4] &lt; 0.05), digits = 4)\n\n       displacement        acceleration              origin   displacement:year \n             0.0119              0.0074              0.0034              0.0135 \n  acceleration:year acceleration:origin \n             0.0303              0.0037 \n\n\n\nThus, as we see above, the interaction terms \\(displacement \\times year\\), \\(acceleration \\times year\\) and \\(acceleration \\times origin\\) are statistically significant predictors of the response mpg.\nIt is evident from above two approaches, that the exact model we use decides which interaction terms are statistically significant predictors of the outcome. Thus, interaction terms should be chosen on theoretical basis for interaction effects and then fitted into the model.\n\n\n\n\n(f)\nNow, we fit different types of models using 3 transformations of the predictor and response variables. We use only one predictor variable horsepower for ease of demonstration. The response variable continues to be mpg. We compare the models using plots with the fitted regression line, along with value of \\(R^2\\) in the top of the graph in red font. The plots how that the log transformation of the predictor variable leads to the best fit and highest \\(R^2\\).\n\n\nfit5 &lt;- lm(mpg ~ horsepower, data = Auto)\nfit6 &lt;- lm(mpg ~ I(horsepower^2), data = Auto)\nfit7 &lt;- lm(mpg ~ log(horsepower), data = Auto)\nfit8 &lt;- lm(mpg ~ sqrt(horsepower), data = Auto)\nattach(Auto)\npar(mfrow = c(2, 2))\nplot(horsepower, mpg)\nabline(fit5, col = \"red\", lwd = 4)\nmtext(round(summary(fit5)$r.squared, 2), col = \"red\")\nplot(horsepower^2, mpg)\nabline(fit6, col = \"red\", lwd = 4)\nmtext(round(summary(fit6)$r.squared, 2), col = \"red\")\nplot(log(horsepower), mpg)\nabline(fit7, col = \"red\", lwd = 4)\nmtext(round(summary(fit7)$r.squared, 2), col = \"red\")\nplot(sqrt(horsepower), mpg)\nabline(fit8, col = \"red\", lwd = 4)\nmtext(round(summary(fit8)$r.squared, 2), col = \"red\")"
  },
  {
    "objectID": "Chapter3e.html#question-10",
    "href": "Chapter3e.html#question-10",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 10",
    "text": "Question 10\n\n(a)\n\nlibrary(ISLR)\nlibrary(tidyverse)\nfit9 &lt;- lm(Sales ~ Price + Urban + US, data = Carseats)\nround(summary(fit9)$coef, 4)\n\n            Estimate Std. Error  t value Pr(&gt;|t|)\n(Intercept)  13.0435     0.6510  20.0357   0.0000\nPrice        -0.0545     0.0052 -10.3892   0.0000\nUrbanYes     -0.0219     0.2717  -0.0807   0.9357\nUSYes         1.2006     0.2590   4.6347   0.0000\n\n\n\n\n(b)\nThe interpretation of each coefficient in the regression model is given below. The response variable is Sales, i.e. sales of car seats in thousands.\n\nFor each unit increase in price of carseats, the sales decrease by 54.4 units.\nOn average, stores located in Urban areas in US sell 21.9 less car seats than stores located in rural areas.\nOn average, stores located in US sell 1200.5 car seats more than stores located in other countries.\n\n\n\n\n(c)\nThis model can be written in equation form as\n\\[ Sales(in \\; thousands) = 13.043 -0.054\\times Price - 0.022 \\times Urban + 1.201 \\times US + \\epsilon \\]\n\n\n(d)\nFor the predictors Price and US, we can reject the null hypothesis \\(H_o: \\beta_j = 0\\) because their associated p-value is less than 0.05.\n\n\n\n(e)\nA smaller model is fit as shown below.\n\n\nfit10 &lt;- lm(Sales ~ Price + US, data = Carseats)\nround(summary(fit10)$coef, 4)\n\n            Estimate Std. Error  t value Pr(&gt;|t|)\n(Intercept)  13.0308     0.6310  20.6518        0\nPrice        -0.0545     0.0052 -10.4161        0\nUSYes         1.1996     0.2585   4.6415        0\n\n\n\n\n(f)\nIf we compare the two models in (a) and (e), we find that an Analysis of Variance tells that the two models are not significantly different from each other. How well each model fits the data is revealed by the adjusted \\(R^2\\), which is 0.23 round(summary(fit9)$adj.r.squared,4)for model in (a) and 0.24 round(summary(fit10)$adj.r.squared,4) for model in (e).\n\n\nanova(fit9, fit10)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ Price + Urban + US\nModel 2: Sales ~ Price + US\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    396 2420.8                           \n2    397 2420.9 -1  -0.03979 0.0065 0.9357\n\n\n\n\n(g)\nThe 95% Confidence Intervals for the coefficients in model from (e) are shown below.\n\n\nround(confint(fit10), 3)\n\n             2.5 % 97.5 %\n(Intercept) 11.790 14.271\nPrice       -0.065 -0.044\nUSYes        0.692  1.708\n\n\n\n\n(h)\nThe evidence for outliers and high leverage points in model from (e) can be seen using the diagnostic plots and R code given below. The plots reveal that there are 23 outliers (sum(abs(rstudent(fit10))&gt;2)) with absolute value of studentized residual greater than 2, but none of these has an absolute studentized residual more than 3. Similarly, there are 20 high leverage observations (sum(hatvalues(fit10) &gt; 2*mean(hatvalues(fit10)))) where \\(h_i &gt; 2\\overline{h}\\). Lastly, sum(cooks.distance(fit10) &gt; 4/length(cooks.distance(fit10))) 19 observations are influential as they Cook’s Distance \\(D_i &gt; 4/n\\).\n\nsum(abs(rstudent(fit10)) &gt; 2)\nsum(hatvalues(fit10) &gt; 2 * mean(hatvalues(fit10)))\nsum(cooks.distance(fit10) &gt; 4 / length(cooks.distance(fit10)))\npar(mfrow = c(2, 3))\nplot(fit10, which = c(1:6))"
  },
  {
    "objectID": "Chapter3e.html#question-11",
    "href": "Chapter3e.html#question-11",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 11",
    "text": "Question 11\n\n(a)\nThe simple linear regression of \\(y\\) onto \\(x\\) is shown below. The coefficient \\(\\hat{\\beta}\\) for \\(x\\) is 1.9939. The standard error is 0.1065. The t-value is 18.73 and p-value is nearly 0. The interpretation is that \\(x\\) is a statistically significant predictor of \\(y\\).\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\nfit11 &lt;- lm(y ~ x + 0)\nsummary(fit11)\n\n\nCall:\nlm(formula = y ~ x + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9154 -0.6472 -0.1771  0.5056  2.3109 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nx   1.9939     0.1065   18.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9586 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n(b)\nThe simple linear regression of \\(x\\) onto \\(y\\) is shown below. The coefficient \\(\\hat{\\beta}\\) for \\(y\\) is 0.3911. The standard error is 0.021. The t-value is 18.73 and p-value is nearly 0. The interpretation is that \\(y\\) is a statistically significant predictor of \\(x\\).\n\n\nfit12 &lt;- lm(x ~ y + 0)\nsummary(fit12)\n\n\nCall:\nlm(formula = x ~ y + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8699 -0.2368  0.1030  0.2858  0.8938 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \ny  0.39111    0.02089   18.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4246 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n(c)\nThe results obtained in (a) and (b) above have the exact same t-statistic, p-value and \\(R^2\\) (both adjusted \\(R^2\\) and multiple \\(R^2\\)). This is because both models fit the same correlation between \\(x\\) and \\(y\\).\n\n\n\n(d)\nIn this question, when regression is performed without an intercept, the value of \\(\\hat{\\beta}\\) and \\(SE(\\hat{\\beta})\\) is given by:\n\\[ \\hat{\\beta} = \\frac{\\sum_{i=1}^{n} x_iy_i}{\\sum_{i'=1}^{n}x_{i'}^2} = \\sum_i{x_i}{y_i}/\\sum_j{x_j^2}\\] \\[SE(\\hat{\\beta}) = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - x_i\\hat{\\beta})^2}{(n - 1)\\sum_{i'=1}^nx_{i'}^2}} = \\sqrt{\\sum_i(y_i - x_i\\hat{\\beta})^2/(n - 1)\\sum_jx_j^2}\\] Thus, we can show that:—\n\\[ t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}\\] \\[t = \\frac{\\sum_ix_iy_i/\\sum_jx_j^2}{\\sqrt{\\sum_i(y_i - x_i\\hat{\\beta})^2/(n - 1)\\sum_jx_j^2}} \\]\n\\[t = \\frac{\\sum_ix_iy_i \\sqrt{n - 1}}{\\sum_jx_j^2{\\sqrt{\\sum_i(y_i - x_i\\hat{\\beta})^2/\\sum_jx_j^2}}} \\]\n\\[t = \\frac{\\sum_ix_iy_i \\sqrt{n - 1}}{{\\sqrt{\\sum_j{x_j}^2 \\sum_i(y_i - x_i\\hat{\\beta})^2}}} \\]\n\\[t = \\frac{\\sum_ix_iy_i \\sqrt{n - 1}}{{\\sqrt{\\sum_j{x_j}^2 \\sum_i(y_i - x_i\\hat{\\beta})^2}}} \\]\n\nWe now replace the value of \\(\\hat{\\beta}\\) into this equation and solve:—\n\\[t = \\frac{\\sqrt{n - 1}\\sum_ix_iy_i}{\\sqrt{\\sum_jx_j^2\\sum_i(y_i - x_i\\sum_ix_iy_i/\\sum_jx_j^2)^2}}\\] \\[t = \\frac{\\sqrt{n - 1}\\sum_ix_iy_i}{\\sqrt{(\\sum_jx_j^2)(\\sum_jy_j^2) - (\\sum_jx_jy_j)^2}}\\]\nFurther, this result can be confirmed in R Code given below which shows that the calculated t-value using this formula is the same t-value as shown in models fit11 and fit12 :—\n\nt &lt;- sqrt(length(x) - 1) * (x %*% y) / sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)\nas.numeric(round(t, 2))\n\n[1] 18.73\n\n\n\n\n(e)\nFrom the results in (d) above, it is clear that \\(t-statistic\\) will be the same even if \\(x\\) and \\(y\\) are interchanged. The equation for \\(t-statistic\\) is associative in \\(x\\) and \\(y\\).\n\n\n(f)\nNow, we perform the regressions with an intercept. The t-values of both regressions are displayed using the code below. It is clear that both the t-values are equal.\n\nfit13 &lt;- lm(y ~ x)\nfit14 &lt;- lm(x ~ y)\nsummary(fit13)$coef[2, 3]\n\n[1] 18.5556\n\nsummary(fit14)$coef[2, 3]\n\n[1] 18.5556"
  },
  {
    "objectID": "Chapter3e.html#question-12",
    "href": "Chapter3e.html#question-12",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 12",
    "text": "Question 12\n\n(a)\nThe coefficient for regression of \\(Y\\) onto \\(X\\) is given by:\n\\[\\hat{\\beta} = \\frac{\\sum_ix_iy_i}{\\sum_jx_j^2}\\]\nand, the coefficient for regression of \\(X\\) onto \\(Y\\) is given by:\n\\[\\hat{\\beta'} = \\frac{\\sum_ix_iy_i}{\\sum_jy_j^2}\\]\nThus, \\(\\hat{\\beta} = \\hat{\\beta'}\\) in a special condition when,\n\\[{\\sum_jx_j^2} = {\\sum_jy_j^2}\\] .\n\n\n(b)\nThe following R code generates the example:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\nfit15 &lt;- lm(y ~ x)\nfit16 &lt;- lm(x ~ y)\ndata.frame(\n  sum_x_sq = sum(x^2),\n  sum_y_sq = sum(y^2),\n  Coef_y_x = coef(fit15)[2],\n  Coef_x_y = coef(fit16)[2]\n) %&gt;%\n  kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nsum_x_sq\nsum_y_sq\nCoef_y_x\nCoef_x_y\n\n\n\n\nx\n81.05509\n413.2135\n1.99894\n0.3894245\n\n\n\n\n\n\n\n\n\n(c)\nThe following R code generates an example with \\({\\sum_jx_j^2} = {\\sum_jy_j^2}\\):-\n\nx &lt;- (1:100)\ny &lt;- (100:1)\nfit17 &lt;- lm(y ~ x)\nfit18 &lt;- lm(x ~ y)\ndata.frame(\n  sum_x_sq = sum(x^2),\n  sum_y_sq = sum(y^2),\n  Coef_y_x = coef(fit17)[2],\n  Coef_x_y = coef(fit18)[2]\n) %&gt;%\n  kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\n\nsum_x_sq\nsum_y_sq\nCoef_y_x\nCoef_x_y\n\n\n\n\nx\n338350\n338350\n-1\n-1"
  },
  {
    "objectID": "Chapter3e.html#question-13",
    "href": "Chapter3e.html#question-13",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 13",
    "text": "Question 13\n\n(a), (b) & (c)\nThe R code is given below:\n\nset.seed(1)\nx &lt;- rnorm(100, mean = 0, sd = 1)\neps &lt;- rnorm(100, mean = 0, sd = sqrt(0.25))\ny &lt;- -1 + 0.5 * x + eps\nlength(y)\n\n[1] 100\n\n\nThe length of vector y is 100 (length(y)). The coefficient \\(\\beta_o\\) is -1, and the coefficient \\(\\beta_1\\) is 0.5.\n\n\n\n(d)\nThe scatter-plot between x and y is shown below. We see that there is a linear relationship between x and y, although some random variability is introduced by the eps noise.\n\npar(mfrow = c(1, 1))\nplot(x, y)\n\n\n\n\n\n\n(e)\n\nfit19 &lt;- lm(y ~ x)\ncoef(fit19)\n\n(Intercept)           x \n -1.0188463   0.4994698 \n\n\nThe coefficient \\(\\hat{\\beta_o}\\) is -1.02 round(coef(fit19)[1],2), and the coefficient \\(\\hat{\\beta_1}\\) is 0.5 round(coef(fit19)[2],2). Both these estimates are quite close to the true model coefficients.\n\n\n(f)\n\nplot(x, y)\nabline(fit19, col = \"red\", lty = 2)\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2, 0.5,\n  legend = c(\"Fitted Least Squares Line\", \"True Regression Model\"),\n  col = c(\"red\", \"blue\"),\n  lty = 2:1,\n  cex = 0.8\n)\n\n\n\n\n\n\n(g)\nAfter fitting a quadratic term, we observe that there is no significant improvement in the model fit because the ANOVA comparison shows a p-value &gt; 0.05.\n\nfit20 &lt;- lm(y ~ x + I(x^2))\nanova(fit19, fit20)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ x + I(x^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     98 22.709                           \n2     97 22.257  1   0.45163 1.9682 0.1638\n\n\n\n\n(h)\nNow, we re-create steps in questions (a) to (f), but with a lower variance in the error terms, say \\(var(\\epsilon) = 0.05\\). The results how that now the estimated coefficients are much closer to the true model parameters. And, the fitted least squares line is much closer to the true regression model line. However, the confidence intervals for the estimated coefficients are much narrower.\n\n\nset.seed(1)\nx &lt;- rnorm(100, mean = 0, sd = 1)\neps &lt;- rnorm(100, mean = 0, sd = sqrt(0.05))\ny &lt;- -1 + 0.5 * x + eps\nfit21 &lt;- lm(y ~ x)\nplot(x, y)\nabline(fit19, col = \"red\", lty = 2)\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2, 0.0,\n  legend = c(\"Fitted Least Squares Line\", \"True Regression Model\"),\n  col = c(\"red\", \"blue\"), lty = 2:1, cex = 0.8\n)\n\n\n\n\n\n\n(i)\nNow, we re-create steps in questions (a) to (f), but with higher variance in the error terms, say \\(var(\\epsilon) = 5.0\\). The results how that now the estimated coefficients are further away from the true model parameters. And, the fitted least squares line is still somewhat close to the true regression model line because we have a large number of observations. However, the confidence intervals for the estimated coefficients are very wide.\n\n\nset.seed(1)\nx &lt;- rnorm(100, mean = 0, sd = 1)\neps &lt;- rnorm(100, mean = 0, sd = sqrt(5))\ny &lt;- -1 + 0.5 * x + eps\nfit22 &lt;- lm(y ~ x)\nplot(x, y)\nabline(fit19, col = \"red\", lty = 2)\nabline(a = -1, b = 0.5, col = \"blue\")\nlegend(-2, 4, legend = c(\"Fitted Least Squares Line\", \"True Regression Model\"), col = c(\"red\", \"blue\"), lty = 2:1, cex = 0.8)\n\n\n\n\n\n\n(j)\nThe confidence intervals for the coefficients \\(\\hat{\\beta_o}\\) and \\(\\hat{\\beta_1}\\) are tabulated below. The results show that the confidence intervals for the coefficient estimates are much wider when the variance in error terms \\(\\epsilon\\) is higher.\n\n\nbeta_o &lt;- round(rbind(confint(fit19)[1, ], confint(fit21)[1, ], confint(fit22)[1, ]), 3)\nbeta_1 &lt;- round(rbind(confint(fit19)[2, ], confint(fit21)[2, ], confint(fit22)[2, ]), 3)\ncoefs &lt;- data.frame(cbind(beta_o, beta_1))\ncolnames(coefs) &lt;- c(\"LowerCI_beta_o\", \"UpperCI_beta_o\", \"LowerCI_beta_1\", \"UpperCI_beta_o\")\nrownames(coefs) &lt;- c(\"First Model Var(0.25)\", \"Narrow Model Var(0.05)\", \"Wider Model Var(5.0)\")\ncoefs %&gt;%\n  kbl() %&gt;%\n  kable_classic_2(full_width = F)\n\n\n\n\n\nLowerCI_beta_o\nUpperCI_beta_o\nLowerCI_beta_1\nUpperCI_beta_o\n\n\n\n\nFirst Model Var(0.25)\n-1.115\n-0.923\n0.393\n0.606\n\n\nNarrow Model Var(0.05)\n-1.051\n-0.965\n0.452\n0.548\n\n\nWider Model Var(5.0)\n-1.515\n-0.654\n0.020\n0.976"
  },
  {
    "objectID": "Chapter3e.html#question-14",
    "href": "Chapter3e.html#question-14",
    "title": "Chapter 3 (Exercises)",
    "section": "Question 14",
    "text": "Question 14\n\n(a)\nIn the R-code given by the question, and reproduced below, the linear model is: \\[ y = 2 + 2x_1 \\ + \\ 0.3x_2 \\ + \\ \\epsilon \\] where, \\(\\epsilon \\sim \\mathcal{N}(0,1)\\) is a random error term. The regression coefficients are \\(\\beta_o = 2\\), \\(\\beta_1 = 2\\) and \\(\\beta_2 = 0.3\\).\n\n\nset.seed(1)\nx1 &lt;- runif(100)\nx2 &lt;- 0.5 * x1 + rnorm(100) / 10\ny &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)\n\n\n\n(b)\nBased on the R code given, x1 and x2 seems positively correlated. The correlation coefficient between x1 and x2 calculated using code in R is 0.84 (round(cor(x1, x2),2)). The scatter plot matrix is given below:\n\n\ncor(x1, x2)\n\n[1] 0.8351212\n\nplot(x1, x2)\n\n\n\n\n\n\n(c)\nThe regression of y onto x1 and x2 is fitted below. The results show that around 23% variability in y is explained by this model, and the model is significantly better than a NULL model, i.e. \\(\\hat{y} = \\overline{y}\\). Further, x1 is a statistically significant predictor of y, whereas x2 is not. We can safely reject the two null hypotheses \\(H_o \\ : \\ \\beta_o = 0\\) and \\(H_o \\ : \\ \\beta_1 = 0\\). However, we cannot reject the null hypothesis \\(H_o \\ : \\ \\beta_2 = 0\\). The estimated coefficients of regression are: \\(\\hat{\\beta_o} = 2.01\\), \\(\\hat{\\beta_1} = 2.3\\) and \\(\\hat{\\beta_2} = -0.24\\). The estimated coefficient for intercept is reasonably close to the true parameter, however the estimated coefficients for x1 and x2 is totally off target.\n\n\nfit23 &lt;- lm(y ~ x1 + x2)\nsummary(fit23)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8311 -0.7273 -0.0537  0.6338  2.3359 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1305     0.2319   9.188 7.61e-15 ***\nx1            1.4396     0.7212   1.996   0.0487 *  \nx2            1.0097     1.1337   0.891   0.3754    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.056 on 97 degrees of freedom\nMultiple R-squared:  0.2088,    Adjusted R-squared:  0.1925 \nF-statistic:  12.8 on 2 and 97 DF,  p-value: 1.164e-05\n\n\n\n\n(d)\nNow, we fit linear least squares regression using only x1 as a predictor for the response y. The resulting model is statistically significantly better than the null model due to a very low p-value on the F-statistic. Further, it explains 24% of the variability in y. We can safely reject the two null hypotheses \\(H_o \\ : \\ \\beta_o = 0\\) and \\(H_o \\ : \\ \\beta_{x1} = 0\\). The estimated coefficients of regression are: \\(\\hat{\\beta_o} = 2.01\\), \\(\\hat{\\beta_{x1}} = 2.2\\). They are reasonably close to the true parameters.\n\n\nfit24 &lt;- lm(y ~ x1)\nsummary(fit24)\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.89495 -0.66874 -0.07785  0.59221  2.45560 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.1124     0.2307   9.155 8.27e-15 ***\nx1            1.9759     0.3963   4.986 2.66e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.055 on 98 degrees of freedom\nMultiple R-squared:  0.2024,    Adjusted R-squared:  0.1942 \nF-statistic: 24.86 on 1 and 98 DF,  p-value: 2.661e-06\n\n\n\n\n(e)\nNow, we fit linear least squares regression using only x2 as a predictor for the response y. The resulting model is statistically significantly better than the null model due to a very low p-value on the F-statistic. Further, it explains only 14.8% of the variability in y. Based on this model, we can safely reject the two null hypotheses \\(H_o \\ : \\ \\beta_o = 0\\) and \\(H_o \\ : \\ \\beta_{x2} = 0\\). The estimated coefficients of regression are: \\(\\hat{\\beta_o} = 2.42\\), \\(\\hat{\\beta_{x2}} = 2.8\\). Surprisingly, the coefficient estimates are widely off target for both the intercept and the predictor. Despite being statistically significant, the estimates do not incorporate the true parameter values even in their 95 % confidence interval.\n\n\nfit25 &lt;- lm(y ~ x2)\nsummary(fit25)\n\n\nCall:\nlm(formula = y ~ x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.62687 -0.75156 -0.03598  0.72383  2.44890 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.3899     0.1949   12.26  &lt; 2e-16 ***\nx2            2.8996     0.6330    4.58 1.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.072 on 98 degrees of freedom\nMultiple R-squared:  0.1763,    Adjusted R-squared:  0.1679 \nF-statistic: 20.98 on 1 and 98 DF,  p-value: 1.366e-05\n\n\n\n\n(f)\nAt the first sight, the results from (c), (d) and (e) do seem to contradict each other. Firstly, the coefficient for x2 is non-significant and negative in the multiple regression, but turns positive and highly significant in simple regression. Further, there is very high error in estimate of \\(\\hat{\\beta_2}\\) in the multiple regression (because x2 is correlated with x1, and this collinearity widens the confidence interval). Secondly, the simple regression using only x2 as a predictor displays a large positive coefficient estimate, much more than the true parameter. This is perhaps because x2 is acting as a surrogate for x1 in the simple regression of y onto x2.\nThe situation in questions (c) to (e) present a text book example of collinearity. When two predictors (x1 and x2) are highly correlated, the parameter estimates in multiple linear regression tend to have large standard errors of estimation. This leads to imprecise estimates with wide confidence intervals. Thus, they null hypothesis cannot be rejected in the multiple linear regression. However, simple regression performed after dropping one of the correlated predictors can solve the problem of imprecise estimates. Here, the simple linear regression of either predictor shows them to be highly significant predictors of y. But, we must be wary that simple regression involving such predictors can lead to misleading estimates of the true underlying relationship.\n\n\n\n(g)\nNow, we add a new observation as provided in the question and fit the three models again. The new models are checked for leverage and standardized residual (outlier status) of the new observation. The results are compactly produced in the table produced at the end. We compare the models and find the following results: - In the multiple linear regression model, adding the new observation totally throws off the estimated coefficients for x1 and x2. Adding this observation has a major impact on the re-fitted model. The new observation is an outlier, having an absolute standardized residual &gt; 2 (though it is &lt; 3). The new observation has high leverage, and thus a high influence on the re-fitted model. This is shown in the diagnostic plots as well. Lastly, this means that a fitted model with high collinearity is not robust, it is very vulnerable to misread observations. - In the simple linear regression model with x1 as predictor, adding the new observation has very low impact on the coefficients. The new observation is an extreme outlier, with an absolute standardized residual greater than 3. However, the new observation has a very low leverage, and thus, it is not influential on the re-fitted model at all. - In the simple linear regression model with x2 as predictor, adding the new observation has a moderate to low impact on the coefficients. The new observation is not an outlier, and thus even though it has somewhat high leverage, it has no major influence on the fitted model.\n\nx1 &lt;- c(x1, 0.1)\nx2 &lt;- c(x2, 0.8)\ny &lt;- c(y, 6)\nfit23new &lt;- lm(y ~ x1 + x2)\nfit24new &lt;- lm(y ~ x1)\nfit25new &lt;- lm(y ~ x2)\nResult &lt;- data.frame(\n  Model = c(\n    \"y~x1+x2\", \"y~x1+x2 new\", \"y~x1\",\n    \"y~x1 new\", \"y~x2\", \"y~x2 new\"\n  ),\n  Intercept = rep(NA, 6), Beta_x1 = rep(NA, 6),\n  Beta_x2 = rep(NA, 6), Leverage = rep(NA, 6),\n  Standardized_Residual = rep(NA, 6)\n)\nResult[1, 2:4] &lt;- fit23$coefficients\nResult[2, 2:4] &lt;- fit23new$coefficients\nResult[3, 2:3] &lt;- fit24$coefficients\nResult[4, 2:3] &lt;- fit24new$coefficients\nResult[5, c(2, 4)] &lt;- fit25$coefficients\nResult[6, c(2, 4)] &lt;- fit25new$coefficients\n\n# Code to find Leverage of the new observation\nsum(hatvalues(fit23new) &gt; 2 * mean(hatvalues(fit23new)))\n\n[1] 2\n\nwhich.max(hatvalues(fit23new))\n\n101 \n101 \n\nResult[2, 5] &lt;- hatvalues(fit23new)[101]\nwhich(abs(rstudent(fit23new)) &gt; 2)\n\n 16  21  55  82 101 \n 16  21  55  82 101 \n\nResult[2, 6] &lt;- rstudent(fit23new)[101]\nsum(hatvalues(fit24new) &gt; 2 * mean(hatvalues(fit24new)))\n\n[1] 3\n\nwhich.max(hatvalues(fit24new))\n\n27 \n27 \n\nResult[4, 5] &lt;- hatvalues(fit24new)[101]\nwhich(abs(rstudent(fit24new)) &gt; 2)\n\n 21  55  56  82 101 \n 21  55  56  82 101 \n\nResult[4, 6] &lt;- rstudent(fit24new)[101]\nsum(hatvalues(fit25new) &gt; 2 * mean(hatvalues(fit25new)))\n\n[1] 8\n\nwhich.max(hatvalues(fit25new))\n\n101 \n101 \n\nResult[6, 5] &lt;- hatvalues(fit25new)[101]\nwhich(abs(rstudent(fit25new)) &gt; 2)\n\n 5 21 55 82 \n 5 21 55 82 \n\nResult[6, 6] &lt;- rstudent(fit25new)[101]\n\n\nlibrary(kableExtra)\nknitr::kable(Result, digits = 3) %&gt;% kable_classic_2()\n\n\n\n\nModel\nIntercept\nBeta_x1\nBeta_x2\nLeverage\nStandardized_Residual\n\n\n\n\ny~x1+x2\n2.130\n1.440\n1.010\nNA\nNA\n\n\ny~x1+x2 new\n2.227\n0.539\n2.515\n0.415\n2.113\n\n\ny~x1\n2.112\n1.976\nNA\nNA\nNA\n\n\ny~x1 new\n2.257\n1.766\nNA\n0.033\n3.438\n\n\ny~x2\n2.390\nNA\n2.900\nNA\nNA\n\n\ny~x2 new\n2.345\nNA\n3.119\n0.101\n1.141\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(fit23new)\n\n\n\npar(mfrow = c(2, 2))\nplot(fit24new)\n\n\n\npar(mfrow = c(2, 2))\nplot(fit25new)"
  },
  {
    "objectID": "Chapter3l.html",
    "href": "Chapter3l.html",
    "title": "Chapter 3 (R Lab)",
    "section": "",
    "text": "library(MASS)\nlibrary(ISLR)\nlibrary(car)"
  },
  {
    "objectID": "Chapter3l.html#libraries",
    "href": "Chapter3l.html#libraries",
    "title": "Chapter 3 (R Lab)",
    "section": "",
    "text": "library(MASS)\nlibrary(ISLR)\nlibrary(car)"
  },
  {
    "objectID": "Chapter3l.html#simple-linear-regression",
    "href": "Chapter3l.html#simple-linear-regression",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.2 Simple Linear Regression",
    "text": "3.6.2 Simple Linear Regression\n\ndata(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\nlm.fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)\n\nDisplaying the results of simple linear regression of medv on lstat.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nThe contents of lm.fit can be displayed as :—\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nUsing the predict function for predicting the values of medv for a given value(s) of lstat :—\n\npredict(lm.fit, data.frame(lstat=c(5,10,15)), interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat=c(5,10,15)), interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nThe plot of medv with lstat along with least squares regression line is as follows:-\n\npar(mfrow=c(2,2))\nplot(x=lstat, y=medv)\nabline(lm.fit, col=\"red\")\nplot(x=lstat, y=medv, pch=20)\nplot(x=lstat, y=medv, pch=\"+\")\nabline(lm.fit, col=\"red\", lwd=3)\nplot(1:20, 1:20, pch=1:20)\n\n\n\n\nPlotting the 4 diagnostic plots of lm.fit:—\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\nAlternatively, we can plot the residuals vs. fitted values; and studentized residuals vs. fitted values as follows: —\n\npar(mfrow=c(1,2))\nplot(x=predict(lm.fit), y=residuals(lm.fit))\nplot(x=predict(lm.fit), y=rstudent(lm.fit))\n\n\n\n\n\nNow, we calculate the leverage statistics using hatvalues function. The largest leverage is for the observation number 375 (which.max(hatvalues(lm.fit))). Also, I plot the studentized residuals vs. leverage statistic, just like Fig.3.13(right) in the book :–\n\n\npar(mfrow=c(1,2))\nplot(hatvalues(lm.fit))\nplot(x=hatvalues(lm.fit), y=rstudent(lm.fit),\n     xlab = \"Leverage\", ylab=\"Studentized Residuals\")\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Chapter3l.html#multiple-linear-regression",
    "href": "Chapter3l.html#multiple-linear-regression",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.3 Multiple Linear Regression",
    "text": "3.6.3 Multiple Linear Regression\nFitting a multiple linear regression as follows:-\n\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nPerforming regression of medv on all other variables :—\n\nlm.fit &lt;- lm(medv ~ ., data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nThe components of summary(lm.fit) are :–\n\nnames(summary(lm.fit))\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\nsummary(lm.fit)$r.squared ; summary(lm.fit)$sigma\n\n[1] 0.7406427\n\n\n[1] 4.745298\n\n\nCalculating V.I.F from car::vif() from the car package :—\n\ncar::vif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\n\nExcluding one variable (age, which has high p-value) from multiple regression :—\n\nlm.fit1 &lt;- lm(medv ~ . - age, data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter3l.html#interaction-terms",
    "href": "Chapter3l.html#interaction-terms",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.4 Interaction Terms",
    "text": "3.6.4 Interaction Terms\nIncluding interaction terms as follows :-\n\nsummary(lm(medv ~ lstat*age, data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter3l.html#non-linear-transformations-of-predictors",
    "href": "Chapter3l.html#non-linear-transformations-of-predictors",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.5 Non-Linear Transformations of Predictors",
    "text": "3.6.5 Non-Linear Transformations of Predictors\n\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2), data=Boston)\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nWe use anova() function to quantify how much the quadratic fit is better than linear fit. This is shown as below:\n\nlm.fit &lt;- lm(medv~lstat, data=Boston)\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2), data=Boston)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHence, the model containing \\(lstat^2\\) is far superior to the simple linear regression model. This is also shown in the the diagnostic plots as below:\n\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\nNow, we use the poly() function with the lm() call to include polynomials of a variable up to any degree.\n\n\nlm.fit5 &lt;- lm(medv ~ poly(lstat,5), data=Boston)\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nLastly, a log transformation of the predictor variable.\n\n\nsummary(lm(medv ~ log(lstat), data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(lstat), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.4599  -3.5006  -0.6686   2.1688  26.0129 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  52.1248     0.9652   54.00   &lt;2e-16 ***\nlog(lstat)  -12.4810     0.3946  -31.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.329 on 504 degrees of freedom\nMultiple R-squared:  0.6649,    Adjusted R-squared:  0.6643 \nF-statistic:  1000 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter3l.html#qualitative-predictor",
    "href": "Chapter3l.html#qualitative-predictor",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.6 Qualitative Predictor",
    "text": "3.6.6 Qualitative Predictor\nLoading the Carseats data set.\n\ndata(\"Carseats\")\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\nattach(Carseats)\n\nNow, we create a multiple linear regression with some interaction terms:-\n\n\nlm.fit &lt;- lm(Sales ~ . + Income*Advertising + Price*Age, data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income * Advertising + Price * Age, \n    data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nWe use the contrasts() function to display the dummy coding that R uses for qualitative variables such as ShelveLoc. We can use contrasts() to change the dummy values for different factor levels.\n\n\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Chapter3l.html#writing-functions",
    "href": "Chapter3l.html#writing-functions",
    "title": "Chapter 3 (R Lab)",
    "section": "3.6.7 Writing Functions",
    "text": "3.6.7 Writing Functions\nWe now write the function to load both libraries ISLR and MASS.\n\nLoadLibraries &lt;- function(){\n  library(MASS)\n  library(ISLR)\n  print(\"Libraries MASS and ISLR have been loaded!\")\n}\nLoadLibraries()\n\n[1] \"Libraries MASS and ISLR have been loaded!\""
  },
  {
    "objectID": "Chapter4e.html",
    "href": "Chapter4e.html",
    "title": "Chapter 4 (Exercises)",
    "section": "",
    "text": "library(ISLR)\nlibrary(MASS)\nlibrary(kableExtra)\nlibrary(tidyverse)\nlibrary(corrplot)\n\n\nConceptual\n\nQuestion 2\nThe proof is as follows: \\[ p_k(x) = \\frac {\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma} \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}}{\\sum_{l = 1}^K\\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma} \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}} \\qquad \\qquad (4.12)\\] \\[ p_k(x) = \\frac {\\pi_k  \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}}{\\sum_{l = 1}^K\\pi_l  \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}} . \\frac{\\frac{1}{\\sqrt{2\\pi}\\sigma}}{\\frac{1}{\\sqrt{2\\pi}\\sigma}} \\qquad \\qquad equation \\ 2\\] \\[ p_k(x) = \\frac {\\pi_k  \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}}{\\sum_{l = 1}^K\\pi_l  \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}} \\]\nThe denominator is does not depend on the value of \\(k\\), thus the value of \\(k\\) for which \\(p_k(x)\\) will be maximized will be the same as the value for which numerator of \\(equation 2\\) is maximized, i.e. \\[ \\pi_k  \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})} \\qquad \\qquad expression \\ 1\\] Now, \\(log\\) is a monotonous function, this means the value of \\(expression \\ 1\\) will be maximized its \\(log\\) is maximized. Thus, taking its log and calling it \\(\\delta_k(x)\\), \\[ \\delta_k(x) = log(\\pi_k) -\\frac{(x-\\mu_k)^2}{2\\sigma^2}\\] \\[ \\delta_k(x) = log(\\pi_k) -\\frac{x^2 + \\mu_k^2 -2x\\mu_k}{2\\sigma^2}\\] \\[ \\delta_k(x) = log(\\pi_k) -\\frac{x^2}{2\\sigma^2} - \\frac{\\mu_k^2}{{2\\sigma^2}} +\\frac{x\\mu_k}{\\sigma^2}\\] Further, the expression \\(\\frac{x^2}{2\\sigma^2}\\) does not vary with the class \\(k\\), thus can be ignored, giving us the final expression: \\[ \\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{{2\\sigma^2}} \\ + \\ log(\\pi_k)  \\qquad \\qquad (4.13)\\]\n\n\n\nQuestion 3\nFor this question, since we are assuming that in \\(1 \\ to \\ k\\) classes, \\(\\sigma_1^2 \\not= \\sigma_2^2 \\not= \\ ...\\ \\not= \\sigma_k^2\\), thus we replace \\(\\sigma^2\\) in equations above with \\(\\sigma_k^2\\) and repeat the same steps to get — \\[ \\delta_k(x) = log(\\pi_k) -\\frac{x^2}{2\\sigma_k^2} - \\frac{\\mu_k^2}{{2\\sigma_k^2}} +\\frac{x\\mu_k}{\\sigma_k^2}\\] None of these terms can now be ignored, as all will vary with \\(k\\), thus the final expression becomes quadratic in \\(x\\) as follows: \\[ \\delta_k(x) = -x^2 . \\frac{1}{2\\sigma_k^2} +x.\\frac{\\mu_k}{\\sigma_k^2} - \\frac{\\mu_k^2}{{2\\sigma_k^2}} + log(\\pi_k)\\]\n\n\n\n\nQuestion 4\nThe question is aimed at demonstration of the Curse of Dimensionality, which refers to the fact that then \\(p\\) is large, non-parametric approaches like \\(KNN\\), which depend of finding the training values nearest to a given test value of the predictors, will perform poorly.\n\n(a)\nThe question refers to a situation in which \\(p=1\\) and predictor X has a uniform distribution between 0 and 1, i.e. \\(x = U(0,1)\\). Thus, for any interval the proportion of observations within the interval $ [x - 0.05, x + 0.05]$ will be equal to ratio of length of interval to range of distribution. Now, the question tells us that the interval is 10% of the range of distribution. Thus, on average 0.1 (10%) of the observations will lie in the interval and will be used to make the prediction.\n\\[ Fraction \\ of\\ observations \\ in \\ KNN = \\frac{range\\ of\\ interval}{range \\ of\\ distribution} = \\frac {0.1}{1} = 0.1\\] If we are willing to consider a more nuanced approach, we realize that within the interval \\([0.05, 0.95]\\) the same principal holds, but for the intervals \\([0, 0.05] \\ , [0.95, 1]\\), slight difference will occur. + For \\(x &lt; 0.05\\), the observations considered will be \\((100x + 5)\\)% of the total observations. + For \\(x &gt; 0.95\\), the observations considered will be \\(105 - 100x\\)% of the total observations. Hence, we can compute the percentage of observations in the interval on average as: \\[\\int_{0.05}^{0.95}10 dx + \\int_0^{0.05}(100x + 5)dx + \\int_{0.95}^1(105 - 100x)dx = 9 + 0.375 + 0.375 = 9.75 %\\] However, we stick to the figure of 0.1 fo simplicity in further sections.\n\n\n\n(b)\nNow, the two variables are both \\(U(0,1)\\) distributed, and thus the distribution can be considered as evenly spread over square with both sides equal to 1. The probability distribution function is uniform, and thus evenly spread over the entire area. Hence, the fraction of observations in any given interval area will be \\(area \\ of \\ interval / area \\ of \\ the \\ range\\ of\\ distribution\\). For the interval of 10% of range of each variable, on average \\(\\frac{0.1 \\times 0.1}{1} = \\frac{0.1^2}{1} = 0.01\\) observations will be used to make the prediction.\n\n\n\n(c)\nContinuing on with the previous trend, if \\(p=100\\), and the interval used for prediction is 10% of the range of each variable, a total of \\(\\frac{0.1^{100}}{1} = 10^{-100} \\approx 0\\) fraction of the observations will be used to make a prediction.\n\n\n\n(d)\nThe answers to (a) to (c) above clearly show that a negligible fraction of observations will remain within the interval to be considered for prediction as the value of \\(p\\) increases.\n\n\n\n(e)\nLet us suppose \\(a\\) is the side of the hyper-cube. We need 10% (or, 0.1) of the observations. + For \\(p=1\\), to use 10% of the observations, we need \\(a^1 = 0.1\\). Thus, \\(a = 0.1\\). + For \\(p=2\\), to use 10% of the observations, we need \\(a^2 = 0.1\\). Thus, \\(a = \\sqrt{0.1} = 0.316\\). + For \\(p=10\\), to use 10% of the observations, we need \\(a^{10} = 0.1\\). Thus, \\(a = \\sqrt[10]{0.1} = 0.79\\). + For \\(p=100\\), to use 10% of the observations, we need \\(a^{100} = 0.1\\). Thus, \\(a = \\sqrt[100]{0.1} = 0.977\\). The answers show that with increasing \\(k\\), we end up using the entire range of each variable to make a prediction interval large enough to contain 10% of observations.\n\n\n\nQuestion 5\n\n(a)\nIf the Bayes Decision boundary is linear, we expect: + On the training set, QDA will nearly always perform better because it can better fit the variations in individual points. + On the test set, LDA is expected to perform better, because QDA will tend to over-fit to the noise. The Bayes Boundary will be better approximated by the LDA, and thus LDA will have lower test error rate.\n\n\n\n(b)\nIf the Bayes Decision boundary is linear, we expect: + On the training set, QDA will nearly always perform better because it can better fit the variations in individual points. + On the test set, QDA is expected to perform better, because LDA will suffer from higher bias. The non-linear Bayes Boundary will be better approximated by the QDA, and thus QDA will have lower test error rate.\n\n\n\n(c)\nIn general, as the sample size \\(n\\) increases, we are less concerned with the variance of the fitted model since we have a large number of observations to train the model on. Rather, we will be more concerned with the bias of the model which does not go away even after \\(n\\) is increased. Thus, we expect the test prediction accuracy of QDA relative to LDA to improve as \\(n\\) increases.\n\n\n\n(d)\nThe statement is FALSE. If the Bayes Decision Boundary for a given problem is linear, then on the test set, a more flexible model will end up overfitting and fit to the noise in the training data, over and above the true trends. This will reduce its accuracy when it is used on test data. Thus, QDA is not always guaranteed to achieve loer test error rates, especially if the Bayes decision boundary is linear.\n\n\n\nQuestion 6\nThe estimated logistic regression model can be written in equation form as: \\[ log(odds \\ of \\ receiving \\ an \\ A) = -6 \\ + \\ 0.05 \\times(hours \\ studied) \\ + \\ 1 \\times(undergrad \\ GPA)  \\]\n\n\n(a)\nThe estimated log(odds) for this student are -0.5 (-6+(0.05*40)+3.5) and 0.38 ( exp(-0.5)/(1+exp(-0.5)) ). \\[ log(odds \\ of \\ receiving \\ an \\ A) = -6 \\ + \\ 0.05 \\times(40) \\ + \\ 1 \\times(3.5) = -0.5 \\] \\[ odds \\ of \\ receiving \\ an \\ A = e^{-0.5} = 0.607 \\] \\[ Probability \\ of \\ receiving \\ A = \\frac{odds}{1+odds} = \\frac{0.607}{1.607} = 0.38\\]\n\n\n\n\n(b)\nFor getting 50% probability, the odds will have to be $ = = 1$. Thus, log(odds) will be \\(log(1) = 0\\). So, suppose the student studies for \\(x\\) hours, the equation may be re-written as: \\[ 0 = -6 \\ + \\ 0.05 \\times(x) \\ + \\ 1 \\times(3.5) \\] \\[ 0.05 \\times(x) = 6 - 3.5 = 2.5 \\] \\[ x = \\frac {2.5}{0.05} = 50  \\]\nThus, the student needs to study for 50 hours.\n\n\n\nQuestion 7\nThe question assumes normal distribution for \\(X\\). The given values by the question are: \\[ \\mu_{yes} = 10 \\ \\ ; \\ \\mu_{no} = 0 \\ \\ ; \\ \\ \\hat{\\sigma}^2 = 36 \\ \\ ; \\ \\ \\pi_{yes} = 0.8 \\ \\ ; \\ \\ \\pi_{no} = 0.2  \\] We now plug in these values into the formula obtained in \\((4.12)\\) after applying Bayes’ Theorem. \\[ p_k(x) = \\frac {\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma} \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}}{\\sum_{l = 1}^K\\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma} \\ exp{(-\\frac{1}{2\\sigma^2}{(x-\\mu_k)^2})}} \\qquad \\qquad (4.12)\\] \\[ p_{yes}(4) = \\frac {\\pi_{yes} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\ exp{(-\\frac{1}{2\\sigma^2}{(4 -\\mu_{yes})^2})}}{\\pi_{no} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\ exp{(-\\frac{1}{2\\sigma^2}{(4 -\\mu_{no})^2})} + \\pi_{yes} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\ exp{(-\\frac{1}{2\\sigma^2}{(4 -\\mu_{yes})^2}}} \\] \\[ p_{yes}(4) = \\frac{0.8 \\times exp(-\\frac {(4-10)^2}{2 \\times 36})}\n{0.2 \\times exp(-\\frac {(4-0)^2}{2 \\times 36}) + 0.8 \\times exp(-\\frac {(4-10)^2}{2 \\times 36}) }  .\n\\frac{\\frac{1}{\\sqrt{2\\pi}\\sigma}}{\\frac{1}{\\sqrt{2\\pi}\\sigma}} \\]\n\\[ p_{yes}(4) = \\frac{0.8 \\times exp({-1/2})}\n{0.2 \\times exp(-16/72) + 0.8 \\times exp(-1/2) } \\] \\[ p_{yes}(4) = 0. 752\\]\n\n\n\nQuestion 9\n\n(a)\nLet the fraction of people who default = \\(p\\). Fraction of people who do not default is \\(1-p\\). Then, odds of defaulting is \\(\\frac{p}{1-p}\\). So, we calculate p = 0.27 as shown below. \\[ \\frac{p}{1-p} = 0.37\\] \\[ \\frac{1-p}{p} = \\frac{1}{0.37}\\] \\[ \\frac{1}{p} -1 = \\frac{1}{0.37}\\] \\[ \\frac{1}{p} = \\frac{1}{0.37} + 1 = \\frac{1 + 0.37}{0.37} \\] \\[ p = \\frac{0.37}{1.37} = 0.27 \\]\n\n\n\n(b)\nThe Odds of an event is the ratio of probability of event happening to the probability of event not happening. The probability \\(p\\) is 16% or 0.16 in this case. Thus the odds are 0.1904762\n\\[ Odds = \\frac{p}{1-p} = \\frac{0.16}{1-0.16} = \\frac{0.16}{0.84} = 0.19\\]\n\n\n\n\nApplied\n\nQuestion 12\n\n(a)\nWe now write the function Power as below:\n\nPower &lt;- function(){print(2^3)}\nPower()\n\n[1] 8\n\n\n\n\n(b)\nThe new function Power2 is written as follows:\n\nPower2 &lt;- function(x,a){print(x^a)}\nPower2(3,8)\n\n[1] 6561\n\n\n\n\n(c)\nThe code to generate the results is given below:\n\nPower2(10,3)\n## [1] 1000\nPower2(8,17)\n## [1] 2.2518e+15\nPower2(131,3)\n## [1] 2248091\n\n\n\n(d)\nThe amended code to use return() within our newly defined function Power3 is shown below:\n\nPower3 &lt;- function(x,a){\n  result &lt;- x^a\n  return(result)\n}\n\n\n\n(e)\nThe plot is created using the code shown below:\n\n\npar(mfrow = c(2, 2))\nplot(\n  y = Power3(1:10, 2), x = 1:10, main = \"Plot of f(x)=x^2 for first 10 integers\",\n  xlab = \"x\", ylab = \"x^2\"\n)\nplot(\n  y = log(Power3(1:10, 2)), x = log(1:10), main = \"Plot of f(x)=x^2 on log scale\",\n  xlab = \"x\", ylab = \"x^2\"\n)\nplot(\n  y = log(Power3(1:10, 2)), x = 1:10, main = \"Plot of f(x)=x^2 on log scale for Y-Axis\",\n  xlab = \"x\", ylab = \"log(x^2)\"\n)\nplot(\n  y = Power3(1:10, 2), x = log(1:10), main = \"Plot of f(x)=x^2 on log scale for X-Axis\",\n  xlab = \"log(x)\", ylab = \"x^2\"\n)\n\n\n\n\n\n\n(f)\nThe function PlotPower() is created as shown below. Further, sample outputs are also displayed.\n\n\nPlotPower &lt;- function(x, a){\n  y &lt;- x^a\n  return(plot(x, y))\n}\npar(mfrow = c(2,2))\nPlotPower(1:10,3)\nPlotPower(1:100, 4)\nPlotPower(1:10, 0.33)\nPlotPower(1:100, 0.25)\n\n\n\n\n\n\n\nQuestion 13\nWe now use the Boston data set to fit classification models to predict whether a given suburb has a crime rate above or below the median, using Logistic Regression, LDA, QDA, KNN (with varying values of K). The steps are also listed within the code.\n\n\n# Load data set & libraries, set seed for reproducability\nlibrary(MASS)\nlibrary(class)\nset.seed(3)\ndata(\"Boston\")\nattach(Boston)\n# Create new binary variable for crime rate above median (1) or below median (0)\nBoston$crim01 &lt;- ifelse(Boston$crim &gt;= median(Boston$crim), yes = 1, no = 0)\n\n# Finding which variables are correlated with crim01 response\ncrim01_cor &lt;- data.frame(\n  Predictor = names(Boston)[1:14],\n  Beta.Coeff.Estimate = rep(NA, 14),\n  p.Value = rep(NA, 14)\n)\nfor (i in 1:14) {\n  fit &lt;- glm(crim01 ~ Boston[, i], data = Boston, family = binomial)\n  crim01_cor[i, 1] &lt;- names(Boston)[i]\n  crim01_cor[i, 2] &lt;- summary(fit)$coef[2, \"Estimate\"]\n  crim01_cor[i, 3] &lt;- summary(fit)$coef[2, \"Pr(&gt;|z|)\"]\n}\nkable(crim01_cor, digits = 2) %&gt;% kable_classic_2()\n\n\n\n\nPredictor\nBeta.Coeff.Estimate\np.Value\n\n\n\n\ncrim\n3501.14\n0.82\n\n\nzn\n-0.10\n0.00\n\n\nindus\n0.23\n0.00\n\n\nchas\n0.56\n0.12\n\n\nnox\n29.37\n0.00\n\n\nrm\n-0.46\n0.00\n\n\nage\n0.06\n0.00\n\n\ndis\n-1.05\n0.00\n\n\nrad\n0.38\n0.00\n\n\ntax\n0.01\n0.00\n\n\nptratio\n0.25\n0.00\n\n\nblack\n-0.03\n0.00\n\n\nlstat\n0.17\n0.00\n\n\nmedv\n-0.06\n0.00\n\n\n\n\n\n\n\n\n# Comparative Box-Plots to show distribution of crim01 against \n# standardized other variables can be plotted as well.\nStd.Boston &lt;- scale(Boston[,1:14])\nBostonLong &lt;- bind_cols(Std.Boston, Boston$crim01) %&gt;%\n  rename(crim01 = `...15`)\nBostonLong &lt;- BostonLong %&gt;%\n  mutate(crim01 = as_factor(crim01)) %&gt;%\n  gather(key = \"key\", value = \"value\", -crim01)\nggplot(data = BostonLong) +\n  geom_boxplot(mapping = aes(x = key, y = value, col = crim01)) + \n  theme_classic()\n\n\n\n\n\n# Create a training and test subset: Every fifth observation taken as test subset\ntest &lt;- as.numeric(rownames(Boston)) %% 5 == 0\n\n# Creating a table of error rates for various models using different predictors\nFinal.Result &lt;- data.frame(Predictors = rep(NA, 3),\n                           Logit = rep(NA, 3),\n                           LDA = rep(NA, 3),\n                           QDA = rep(NA, 3),\n                           KNN.1 = rep(NA, 3),\n                           KNN.5 = rep(NA, 3),\n                           KNN.10 = rep(NA,3))\n\n# Predictor Set 1 : all statistically significant predictors in Simple Regression\n\n# Create a new data.frame Boston1 having only response and predictors.\npredictors &lt;- which(crim01_cor$p.Value &lt; 0.05)\nBoston1 &lt;- Boston[, c(15, predictors)]\nTest.Y &lt;- Boston1$crim01[test]\nTrain.Y &lt;- Boston1$crim01[!test]\nTest.Set &lt;- Boston1[test,]\n# Naming the set of predictors to be used\nFinal.Result[1,1] &lt;- paste(colnames(Boston1)[-1], collapse = \"+\")\n# Logistic Regression\nfit.glm &lt;- glm(crim01~., data=Boston1, family=binomial, subset=!test)\nprob.glm &lt;- predict(fit.glm, newdata = Test.Set, type=\"response\")\npred.glm &lt;- ifelse(prob.glm &gt; 0.5, yes = 1, no = 0)\nFinal.Result[1,2] &lt;- mean(pred.glm != Test.Y)\n# LDA and QDA\nfit.lda &lt;- lda(crim01~., data=Boston1, subset=!test)\npred.lda &lt;- predict(fit.lda, newdata = Test.Set)$class\nFinal.Result[1,3] &lt;- mean(pred.lda != Test.Y)\nfit.qda &lt;- qda(crim01~., data=Boston1, subset=!test)\npred.qda &lt;- predict(fit.qda, newdata = Test.Set)$class\nFinal.Result[1,4] &lt;- mean(pred.qda != Test.Y)\n# KNN with K=1, K=5, K=10\nTrain.X &lt;- Boston1[!test, -1]\nTest.X &lt;- Boston1[test, -1]\npred.knn.1 &lt;- knn(Train.X, Test.X, Train.Y, k=1)\nFinal.Result[1,5] &lt;- mean(pred.knn.1 != Test.Y)\npred.knn.5 &lt;- knn(Train.X, Test.X, Train.Y, k=5)\nFinal.Result[1,6] &lt;- mean(pred.knn.5 != Test.Y)\npred.knn.10 &lt;- knn(Train.X, Test.X, Train.Y, k=10)\nFinal.Result[1,7] &lt;- mean(pred.knn.10 != Test.Y)\n\n# Predictor Set 2 : all statistically significant predictors in Multiple Regression\n\nfitp2 &lt;- glm(crim01 ~ . - crim, data=Boston, family=binomial)\npredictors2 &lt;- which(summary(fitp2)$coef[, \"Pr(&gt;|z|)\"] &lt; 0.05)[-1]\n# Creating dataset Boston2 with all new predictors and crim01\nBoston2 &lt;- Boston[, c(15, predictors2)]\nTest.Y &lt;- Boston2$crim01[test]\nTrain.Y &lt;- Boston2$crim01[!test]\nTest.Set &lt;- Boston2[test,]\n# Naming the set of predictors to be used\nFinal.Result[2,1] &lt;- paste(colnames(Boston2)[-1], collapse = \"+\")\n# Logistic Regression\nfit.glm &lt;- glm(crim01~., data=Boston2, family=binomial, subset=!test)\nprob.glm &lt;- predict(fit.glm, newdata = Test.Set, type=\"response\")\npred.glm &lt;- ifelse(prob.glm &gt; 0.5, yes = 1, no = 0)\nFinal.Result[2,2] &lt;- mean(pred.glm != Test.Y)\n# LDA and QDA\nfit.lda &lt;- lda(crim01~., data=Boston2, subset=!test)\npred.lda &lt;- predict(fit.lda, newdata = Test.Set)$class\nFinal.Result[2,3] &lt;- mean(pred.lda != Test.Y)\nfit.qda &lt;- qda(crim01~., data=Boston2, subset=!test)\npred.qda &lt;- predict(fit.qda, newdata = Test.Set)$class\nFinal.Result[2,4] &lt;- mean(pred.qda != Test.Y)\n# KNN with K=1, K=5, K=10\nTrain.X &lt;- Boston2[!test, -1]\nTest.X &lt;- Boston2[test, -1]\npred.knn.1 &lt;- knn(Train.X, Test.X, Train.Y, k=1)\nFinal.Result[2,5] &lt;- mean(pred.knn.1 != Test.Y)\npred.knn.5 &lt;- knn(Train.X, Test.X, Train.Y, k=5)\nFinal.Result[2,6] &lt;- mean(pred.knn.5 != Test.Y)\npred.knn.10 &lt;- knn(Train.X, Test.X, Train.Y, k=10)\nFinal.Result[2,7] &lt;- mean(pred.knn.10 != Test.Y)\n\n# Predictor Set 3 : high statistically significant predictors in Multiple Regression\n\npredictors3 &lt;- which(summary(fitp2)$coef[, \"Pr(&gt;|z|)\"] &lt; 0.01)[-1]\n# Creating dataset Boston3 with all new predictors and crim01\nBoston3 &lt;- Boston[, c(15, predictors3)]\nTest.Y &lt;- Boston3$crim01[test]\nTrain.Y &lt;- Boston3$crim01[!test]\nTest.Set &lt;- Boston3[test,]\n# Naming the set of predictors to be used\nFinal.Result[3,1] &lt;- paste(colnames(Boston3)[-1], collapse = \"+\")\n# Logistic Regression\nfit.glm &lt;- glm(crim01~., data=Boston3, family=binomial, subset=!test)\nprob.glm &lt;- predict(fit.glm, newdata = Test.Set, type=\"response\")\npred.glm &lt;- ifelse(prob.glm &gt; 0.5, yes = 1, no = 0)\nFinal.Result[3,2] &lt;- mean(pred.glm != Test.Y)\n# LDA and QDA\nfit.lda &lt;- lda(crim01~., data=Boston3, subset=!test)\npred.lda &lt;- predict(fit.lda, newdata = Test.Set)$class\nFinal.Result[3,3] &lt;- mean(pred.lda != Test.Y)\nfit.qda &lt;- qda(crim01~., data=Boston3, subset=!test)\npred.qda &lt;- predict(fit.qda, newdata = Test.Set)$class\nFinal.Result[3,4] &lt;- mean(pred.qda != Test.Y)\n# KNN with K=1, K=5, K=10\nTrain.X &lt;- Boston3[!test, -1]\nTest.X &lt;- Boston3[test, -1]\npred.knn.1 &lt;- knn(Train.X, Test.X, Train.Y, k=1)\nFinal.Result[3,5] &lt;- mean(pred.knn.1 != Test.Y)\npred.knn.5 &lt;- knn(Train.X, Test.X, Train.Y, k=5)\nFinal.Result[3,6] &lt;- mean(pred.knn.5 != Test.Y)\npred.knn.10 &lt;- knn(Train.X, Test.X, Train.Y, k=10)\nFinal.Result[3,7] &lt;- mean(pred.knn.10 != Test.Y)\n\nkable(Final.Result, digits = 2) %&gt;% kable_paper()\n\n\n\n\nPredictors\nLogit\nLDA\nQDA\nKNN.1\nKNN.5\nKNN.10\n\n\n\n\nzn+indus+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv\n0.11\n0.17\n0.15\n0.07\n0.07\n0.12\n\n\nzn+nox+dis+rad+tax+ptratio+black+medv\n0.13\n0.14\n0.15\n0.08\n0.08\n0.07\n\n\nnox+dis+rad+ptratio\n0.16\n0.17\n0.16\n0.05\n0.06\n0.09"
  },
  {
    "objectID": "Chapter4l.html",
    "href": "Chapter4l.html",
    "title": "Chapter 4 (Lab)",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE)\nlibrary(ISLR)\nlibrary(corrplot)\nlibrary(tidyverse)"
  },
  {
    "objectID": "Chapter4l.html#the-stock-market-data",
    "href": "Chapter4l.html#the-stock-market-data",
    "title": "Chapter 4 (Lab)",
    "section": "4.6.1 The Stock Market Data",
    "text": "4.6.1 The Stock Market Data\n\ndata(\"Smarket\")\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)\n\n[1] 1250    9\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\npairs(Smarket)\n\n\n\nround(cor(Smarket[,-9]),3)\n\n        Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today\nYear   1.000  0.030  0.031  0.033  0.036  0.030  0.539  0.030\nLag1   0.030  1.000 -0.026 -0.011 -0.003 -0.006  0.041 -0.026\nLag2   0.031 -0.026  1.000 -0.026 -0.011 -0.004 -0.043 -0.010\nLag3   0.033 -0.011 -0.026  1.000 -0.024 -0.019 -0.042 -0.002\nLag4   0.036 -0.003 -0.011 -0.024  1.000 -0.027 -0.048 -0.007\nLag5   0.030 -0.006 -0.004 -0.019 -0.027  1.000 -0.022 -0.035\nVolume 0.539  0.041 -0.043 -0.042 -0.048 -0.022  1.000  0.015\nToday  0.030 -0.026 -0.010 -0.002 -0.007 -0.035  0.015  1.000\n\ncorrplot(cor(Smarket[,-9]), method = \"pie\", tl.col = \"black\")\n\n\n\nattach(Smarket)\nplot(Volume)"
  },
  {
    "objectID": "Chapter4l.html#logidtic-regression",
    "href": "Chapter4l.html#logidtic-regression",
    "title": "Chapter 4 (Lab)",
    "section": "4.6.2 Logidtic Regression",
    "text": "4.6.2 Logidtic Regression\nFitting a logistic regression model using glm with family=binomial argument.\n\n\nfit.glm &lt;- glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family = binomial, data=Smarket)\nsummary(fit.glm)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe coefficients for each predictor and the respective p-values are displayed as follows:\n\n\ncoef(fit.glm)\n\n (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 \n-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 \n      Volume \n 0.135440659 \n\nsummary(fit.glm)$coef\n\n                Estimate Std. Error    z value  Pr(&gt;|z|)\n(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\nLag1        -0.073073746 0.05016739 -1.4565986 0.1452272\nLag2        -0.042301344 0.05008605 -0.8445733 0.3983491\nLag3         0.011085108 0.04993854  0.2219750 0.8243333\nLag4         0.009358938 0.04997413  0.1872757 0.8514445\nLag5         0.010313068 0.04951146  0.2082966 0.8349974\nVolume       0.135440659 0.15835970  0.8552723 0.3924004\n\ndata.frame(Coefficient = coef(fit.glm), pValue = summary(fit.glm)$coef[,\"Pr(&gt;|z|)\"])\n\n             Coefficient    pValue\n(Intercept) -0.126000257 0.6006983\nLag1        -0.073073746 0.1452272\nLag2        -0.042301344 0.3983491\nLag3         0.011085108 0.8243333\nLag4         0.009358938 0.8514445\nLag5         0.010313068 0.8349974\nVolume       0.135440659 0.3924004\n\n\nNow we use predict() function to create predicted values for Direction using the training data itself.\n\n\nprob.glm &lt;- predict(fit.glm, type=\"response\")\nprob.glm[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n\nNow, we convert the probabilities of Up movement in the vector of predicted response probsglm into categorical class labels Up or Down.\n\n\npred.glm &lt;- rep(\"Down\", nrow(Smarket))\npred.glm[prob.glm &gt; 0.5] &lt;- \"Up\"\ntable(pred.glm, Direction)\n\n        Direction\npred.glm Down  Up\n    Down  145 141\n    Up    457 507\n\nmean(pred.glm == Direction)\n\n[1] 0.5216\n\nmean(pred.glm != Direction)\n\n[1] 0.4784\n\n\nThus, the training error rate is 47.84% ( mean(pred.glm != Direction)*100 ).\nNow, we create a training subset and a testing subset of the Smarket data by using a Boolean Vector called train which is TRUE for values of Year&lt;2005. Thus, our training data set becomes Smarket[train,], while the testing data set is Smarket[!train]. Further, we create a training data set Smarket2005 and a new vector Direction2005 so that we can use them later to compute test error rate.\n\n\ntrain &lt;- Year &lt; 2005\ntable(train)\n\ntrain\nFALSE  TRUE \n  252   998 \n\nSmarket2005 &lt;- Smarket[!train,]\nDirection2005 &lt;- Direction[!train]\n\nNow we fit the logistic regression model on training data set, create a vector of predicted probabilities, then create a prediction vector of class labels and finally use the test data set to calculate test error rate.\n\n\nfit.glm &lt;- glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket,\n               family = binomial, subset = train)\nprob.glm &lt;- predict(fit.glm, newdata = Smarket2005, type=\"response\")\npred.glm &lt;- rep(\"Down\", nrow(Smarket2005))\npred.glm[prob.glm &gt; 0.5] &lt;- \"Up\"\ntable(pred.glm, Direction2005)\n\n        Direction2005\npred.glm Down Up\n    Down   77 97\n    Up     34 44\n\nmean(pred.glm != Direction2005)\n\n[1] 0.5198413\n\n\nThus, the test error rate is nearly 52%, ( round(mean(pred.glm != Direction2005),2)*100 ), which is even worse than random guessing.\n\nNow, we redo all the steps using only some predictors (Lag1 and Lag2) which seem somewhat related to the response Direction and hope to have a model with lower test error rate.\n\n\nfit.glm &lt;- glm(Direction ~ Lag1 + Lag2, data=Smarket, \n               family = binomial, subset=train)\nprob.glm &lt;- predict(fit.glm, newdata = Smarket2005, type=\"response\")\npred.glm &lt;- rep(\"Down\", nrow(Smarket2005))\npred.glm[prob.glm &gt; 0.5] &lt;- \"Up\"\ntable(pred.glm, Direction2005)\n\n        Direction2005\npred.glm Down  Up\n    Down   35  35\n    Up     76 106\n\n# The test accuracy of the fitted model is\nround(mean(pred.glm == Direction2005),2)\n\n[1] 0.56\n\n# Test error rate is\nmean(pred.glm != Direction2005)\n\n[1] 0.4404762\n\n\nThus, with fewer but relevant predictors, the test error rate is reduced to 44.04% ( mean(pred.glm != Direction2005)*100 % ). This is because the irrelevant predictors lead to increased uncertainty in the estimates of coefficients and lead to overall poor prediction.\nLastly, we predict the probability of market Direction Up when the values of Lag1 and Lag2 are specified:—\n\n\npredict(fit.glm, type=\"response\",\n        newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)))\n\n        1         2 \n0.4791462 0.4960939"
  },
  {
    "objectID": "Chapter4l.html#linear-discriminant-analysis",
    "href": "Chapter4l.html#linear-discriminant-analysis",
    "title": "Chapter 4 (Lab)",
    "section": "4.6.3 Linear Discriminant Analysis",
    "text": "4.6.3 Linear Discriminant Analysis\nFor linear discriminant analysis, we use lda function from the MASS library. We fit the model for observations from 2001 to 2004, and use 2005 as testing data set.\n\n\nlibrary(MASS)\n\n\nfit.lda &lt;- lda(Direction ~ Lag1+Lag2, data=Smarket, subset=train)\nfit.lda\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(fit.lda)\n\n\n\npred.lda &lt;- predict(fit.lda, newdata = Smarket2005)\nnames(pred.lda)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nThus, the predict function creates a list of objects:\n- class : this contains the predicted category of the response by the LDA. - posterior : a matrix containing posterior probabilities of response being in each category (columns) for all observations (rows). The colnames(pred.lda$posterior) shows us the categories for the response i.e. Down and Up. - x : a matrix containing the linear discriminants.\nFurther, the following commands shows what the model predicts for the year 2005:\n\n\ntable(pred.lda$class)\n\n\nDown   Up \n  70  182 \n\n\nWe can compare the predicted results with the actual market movement in 2005 as follows:\n\n\npred.class &lt;- pred.lda$class\ntable(pred.class, Direction2005)\n\n          Direction2005\npred.class Down  Up\n      Down   35  35\n      Up     76 106\n\n\nThe success rate and error rate on the test data set is calculated as follows:\n\n\n# Test data set success rate\nround(mean(pred.class == Direction2005),2)\n\n[1] 0.56\n\n# Failure rate\nround(mean(pred.class != Direction2005),2)\n\n[1] 0.44\n\n\nNow, we compare the results using thresholds of posterior probability as 50% and 90%.\n\n\n# Since the column 1 i.e. [,1] of `pred.lda$posterior` matrix represents category `Down`\nsum(pred.lda$posterior[,1] &gt;= 0.5)\n\n[1] 70\n\nsum(pred.lda$posterior[,1] &lt;= 0.5)\n\n[1] 182\n\n# This is same as \ntable(pred.lda$class)\n\n\nDown   Up \n  70  182 \n\n# Now using 90% proability of `Down` as cut-off\nsum(pred.lda$posterior[,1] &gt;= 0.9)\n\n[1] 0\n\n\nThus, there is no day in 2005 when the probability of market falling is 90% or more. Lastly, some random code to re-verify that column 1 of pred.lda$posterior matrix is for category Down:\n\n\npred.lda$posterior[1:20,1]\npred.class[1:20]"
  },
  {
    "objectID": "Chapter4l.html#quadratic-discriminant-analysis",
    "href": "Chapter4l.html#quadratic-discriminant-analysis",
    "title": "Chapter 4 (Lab)",
    "section": "4.6.4 Quadratic Discriminant Analysis",
    "text": "4.6.4 Quadratic Discriminant Analysis\nHere, we use the qda function in the MASS library to fit a Quadratic Discriminant Analysis model to the Smarket data in an attempt to predict Direction from Lag1 and Lag2. Once again, we split the data set into a training data and a testing data. Then we calculate the error rate and success rate of the QDA model.\n\n\nfit.qda &lt;- qda(Direction~Lag1+Lag2, data=Smarket, subset=train)\nfit.qda\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\npred.class &lt;- predict(fit.qda, newdata = Smarket2005)$class\ntable(pred.class, Direction2005)\n\n          Direction2005\npred.class Down  Up\n      Down   30  20\n      Up     81 121\n\n# Success Rate of QDA model\nround(mean(pred.class == Direction2005),3)\n\n[1] 0.599\n\n# Test Error rate in QDA\nround(mean(pred.class != Direction2005),3)\n\n[1] 0.401\n\n\nThus, it is evident that the QDA model captures the true relationship more accurately than the linear models, namely LDA and Logistic Regression."
  },
  {
    "objectID": "Chapter4l.html#k-nearest-neighbors",
    "href": "Chapter4l.html#k-nearest-neighbors",
    "title": "Chapter 4 (Lab)",
    "section": "4.6.5 K-Nearest Neighbors",
    "text": "4.6.5 K-Nearest Neighbors\nWe now use the K-Nearest Neighbors approach with \\(K=1\\) and \\(K=3\\) to predict market movement Direction in 2005 test data set after training the knn classifier on 2001-2004 training data set. Additionally we need to create training and test matrices of predictors, and training response vector. The function used is knn from the class library.\n\n\nlibrary(class)\nset.seed(1)\ntrain.X &lt;- cbind(Smarket$Lag1, Smarket$Lag2)[train,]\ntest.X &lt;- cbind(Smarket$Lag1, Smarket$Lag2)[!train,]\ntrain.Direction &lt;- Direction[train]\npred.knn &lt;- knn(train = train.X, test = test.X, cl = train.Direction, k = 1)\ntable(pred.knn, Direction2005)\n\n        Direction2005\npred.knn Down Up\n    Down   43 58\n    Up     68 83\n\n# Success rate in KNN wit k = 1\nmean(pred.knn == Direction2005)\n\n[1] 0.5\n\n# Using k=3 in KNN\npred.knn &lt;- knn(train = train.X, test = test.X, cl = train.Direction, k = 3)\ntable(pred.knn, Direction2005)\n\n        Direction2005\npred.knn Down Up\n    Down   48 54\n    Up     63 87\n\n# Success rate in KNN wit k = 1\nmean(pred.knn == Direction2005)\n\n[1] 0.5357143\n\n\nLastly, as a fun exercise, we create a graph depicting success rate as a function of \\(k\\), when \\(k\\) varies from 1 to 10.\n\n\nresult &lt;- data.frame(k = 1:10, SuccessRate = rep(NA, 10)) \nfor (i in 1:10){\n  pred.knn &lt;- knn(train = train.X, test = test.X, cl = train.Direction, k = i)\n  result$SuccessRate[i] &lt;- mean(pred.knn == Direction2005)\n}\nggplot(result)+\n  geom_line(aes(x=k, y=SuccessRate)) + \n  geom_point(aes(x=k, y=SuccessRate), col=\"black\", size=4)+\n  theme_classic() +\n  scale_x_continuous(breaks = 1:10)"
  },
  {
    "objectID": "Chapter4l.html#an-application-to-caravan-insurance-data",
    "href": "Chapter4l.html#an-application-to-caravan-insurance-data",
    "title": "Chapter 4 (Lab)",
    "section": "4.6.6 An application to Caravan Insurance Data",
    "text": "4.6.6 An application to Caravan Insurance Data\nNow, we use the Caravan data set to predict the Purchase variable, i.e. whether a person purchases an insurance for his/her Caravan based on 85 other demographic predictors. We are not interested in overall test error rate, but rather in success rate of purchasing insurance amongst predicted persons. That is, what percentage of persons predicted by the model to purchase Insurance actually did purchase insurance. This will allow a company to cut costs in selling policies. We use five different approaches : 1) KNN with \\(k=1\\), 2) KNN with \\(k=3\\), 3) KNN with \\(k=5\\), 4) Logistic regression with cut off probability of 0.5 for predicting Purchase and 5) Logistic regression with cut off probability of 0.25 for predicting Purchase.\n\n\ndata(\"Caravan\")\nattach(Caravan)\ntest &lt;- 1:1000\n# Calculating the fraction of Insurance Purchases in general\nround(summary(Purchase)[2]/(length(Purchase)),4)\n\n   Yes \n0.0598 \n\n# Standardizing the data set to implement KNN properly\nstandardized.X &lt;- scale(Caravan[,-86])\ntrain.X &lt;- standardized.X[-test,]\ntest.X &lt;- standardized.X[test,]\ntrain.y &lt;- Caravan$Purchase[-test]\ntest.y &lt;- Caravan$Purchase[test]\nset.seed(1)\npred.knn.1 &lt;- knn(train.X, test.X, train.y, k = 1) \nConMatrix &lt;- table(pred.knn.1, test.y)\ns1 &lt;- ConMatrix[2,2] / (ConMatrix[2,1] + ConMatrix[2,2])\npred.knn.3 &lt;- knn(train.X, test.X, train.y, k = 3) \nConMatrix &lt;- table(pred.knn.3, test.y)\ns2 &lt;- ConMatrix[2,2] / (ConMatrix[2,1] + ConMatrix[2,2])\npred.knn.5 &lt;- knn(train.X, test.X, train.y, k = 5) \nConMatrix &lt;- table(pred.knn.5, test.y)\ns3 &lt;- ConMatrix[2,2] / (ConMatrix[2,1] + ConMatrix[2,2])\nfit.glm.1 &lt;- glm(Purchase ~ ., data=Caravan, family = binomial, subset = -test)\nprob.glm.1 &lt;- predict(fit.glm.1, newdata = Caravan[test,], type=\"response\")\npred.glm.1 &lt;- rep(\"No\", length(test))\npred.glm.1[prob.glm.1 &gt; 0.5] &lt;- \"Yes\"\nConMatrix &lt;- table(pred.glm.1, test.y)\ns4 &lt;- ConMatrix[2,2] / (ConMatrix[2,1] + ConMatrix[2,2])\npred.glm.2 &lt;- rep(\"No\", length(test))\npred.glm.2[prob.glm.1 &gt; 0.25] &lt;- \"Yes\"\nConMatrix &lt;- table(pred.glm.2, test.y)\ns5 &lt;- ConMatrix[2,2] / (ConMatrix[2,1] + ConMatrix[2,2])\ndata.frame(Model = c(\"KNN (k=1)\", \"KNN (k=3)\", \"KNN (k=5)\", \"Logit.Reg.,p=0.5\", \"Logit.Reg.,p=0.25\"),\n           SuccessRate = round(c(s1, s2, s3, s4, s5)*100,1)) |&gt;\n  kableExtra::kbl() |&gt; kableExtra::kable_paper()\n\n\n\n\nModel\nSuccessRate\n\n\n\n\nKNN (k=1)\n11.7\n\n\nKNN (k=3)\n19.2\n\n\nKNN (k=5)\n26.7\n\n\nLogit.Reg.,p=0.5\n0.0\n\n\nLogit.Reg.,p=0.25\n33.3"
  },
  {
    "objectID": "Chapter5e.html#question-1",
    "href": "Chapter5e.html#question-1",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 1",
    "text": "Question 1\nFor this question, we need to find the value of \\(\\alpha\\) which minimizes the expression:\n\\[\n\\begin{aligned}\nTotal \\ Variance &= Var(\\alpha X + (1-\\alpha) Y) \\\\\n&= Var(\\alpha X) + Var((1-\\alpha)Y) + 2.Cov(\\alpha X, (1-\\alpha)Y) \\\\\n&= \\alpha^2Var(X) + (1-\\alpha)^2Var(Y) + 2Cov(X,Y)(\\alpha(1-\\alpha)) \\\\\n&= \\alpha^2 \\sigma_X^2 + (1-\\alpha)^2 \\sigma_Y^2 + 2 \\sigma_{XY}(\\alpha(1-\\alpha)) \\\\\n\\end{aligned}\n\\]\nNow, to find the value of \\(\\alpha\\) which minimizes this total variance, we take it’s first partial derivative with respect to \\(\\alpha\\) and set it equal to zero:–\n\\[\n\\begin{aligned}0 &= \\frac{\\partial}{\\partial \\alpha} \\Big( \\alpha^2 \\sigma_X^2 + (1-\\alpha)^2 \\sigma_Y^2 + 2 \\sigma_{XY}(\\alpha(1-\\alpha)) \\Big) \\\\0 &= 2\\alpha \\sigma_X^2 + 2(1-\\alpha) \\frac{\\partial (1-\\alpha)}{\\partial \\alpha} \\sigma_Y^2 + 2 \\sigma_{XY} \\Big( \\frac{\\partial (1-\\alpha)}{\\partial \\alpha}\\alpha + \\frac{\\partial (\\alpha)}{\\partial \\alpha}(1-\\alpha) \\Big)\\\\0 &= 2\\alpha \\sigma_X^2 - 2(1-\\alpha) \\sigma_Y^2 + 2 \\sigma_{XY}  (-\\alpha + 1 - \\alpha) \\\\0 &= 2\\alpha \\sigma_X^2 - 2\\sigma_Y^2 + 2\\alpha \\sigma_Y^2 + 2\\sigma_{XY}  - 4\\alpha \\sigma_{XY} \\\\2 \\Big(\\sigma_Y^2 - \\sigma_{XY} \\Big) &= 2 \\Big(\\sigma_X^2 + \\sigma_Y^2  -2 \\sigma_{XY}\\Big) \\alpha \\\\\\alpha &= \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2  -2 \\sigma_{XY}} \\qquad (5.6)\\end{aligned}\n\\]\nFurther, to verify that the first partial derivative is indeed the minimum value of Variance, we find the second partial derivative with respect to \\(\\alpha\\) and see whether it is zero or not.\n\\[\n\\begin{aligned}\n&= \\frac{\\partial^2}{\\partial^2 \\alpha} \\Big( \\alpha^2 \\sigma_X^2 + (1-\\alpha)^2 \\sigma_Y^2 + 2 \\sigma_{XY}(\\alpha(1-\\alpha)) \\Big) \\\\\n&= \\frac{\\partial}{\\partial \\alpha} \\Big(2\\alpha \\sigma_X^2 - 2\\sigma_Y^2 + 2\\alpha \\sigma_Y^2 + 2\\sigma_{XY}  - 4\\alpha \\sigma_{XY} \\Big) \\\\\n&= 2 \\sigma_X^2 -0 + 2 \\sigma_Y^2 + 0  - 4 \\sigma_{XY} \\\\\n&= 2 \\Big( \\sigma_X^2 + 2 \\sigma_Y^2 - 2 \\sigma_{XY} \\Big) \\\\\n&= 2 \\Big( Var(X-Y) \\Big)\n\\end{aligned}\n\\]\nNow, the variance of any variable is always positive. Hence, the expression \\(2Var(X-Y)\\) will always be positive. Thus, we can confirm that the minimum value is at the first partial derivative. Equation 5.6 is thus proved."
  },
  {
    "objectID": "Chapter5e.html#question-2",
    "href": "Chapter5e.html#question-2",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 2",
    "text": "Question 2\nWe will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.\n\n(a)\nWhat is the probability that the first bootstrap observation is not the jth observation from the original sample? Justify your answer.\nThe bootstrap method samples each bootstrap observation from the original sample randomly with replacement. Thus, for each bootstrap observation, every value in the original sample has an equal probability of being selected. This probability is simply \\(^1/_n\\). So, the probability that first bootstrap observation is the \\(j^{th}\\) observation from the original sample is \\(^1/_n\\). Finally, the probability that first bootstrap observation is not the \\(j^{th}\\) observation from original sample is \\(1 - (^1/_n)\\).\n\n\n(b)\nWhat is the probability that the second bootstrap observation is not the jth observation from the original sample?\nAs answered earlier, the bootstrap method samples each bootstrap observation from the original sample randomly with replacement. Thus, for each and every bootstrap observation, every value in the original sample has an equal probability of being selected. This probability is simply \\(^1/_n\\).\nSo, the probability that second (or, in fact any \\(k^{th}\\)) bootstrap observation is the \\(j^{th}\\) observation from the original sample is \\(^1/_n\\). Finally, the probability that the second bootstrap observation is not the \\(j^{th}\\) observation from original sample is \\(1 - (^1/_n)\\).\n\n\n(c)\nArgue that the probability that the jth observation is not in the bootstrap sample is \\((1 − 1/n)^n\\).\nSince the bootstrap method samples each observation as a random sample (with replacement) from the original sample, it can be said that each bootstrap sample is an independent random variable. Now, the probability of joint occurrence of independent events (in this case, not selecting the \\(j^{th}\\) observation of original sample) \\(n\\) different times is: \\[\n\\begin{aligned}\nProbability &= Pr_1 \\times Pr_2 \\times Pr_3 \\times \\ ... \\times \\ Pr_n \\\\\n&= (1 - \\ ^1/_n) \\ \\times \\ (1 - \\ ^1/_n) \\ \\times \\  (1 - \\ ^1/_n) \\ \\times \\ ... \\ (1 - \\ ^1/_n) \\\\\n&= (1 - \\ ^1/_n)^n\n\\end{aligned}\n\\]\n\n\n(d)\nWhen n = 5, what is the probability that the jth observation is in the bootstrap sample?\nThe probability of the \\(j^{th}\\) observation being in the bootstrap sample is (1 - probability of \\(j^{th}\\) observation being not in the boostrap sample). For \\(n=5\\), this is 0.672. \\[\n\\begin{aligned}\nProbability &= 1- (1 - \\ ^1/_n)^n \\\\\n&= 1 - (1-0.2)^5 \\\\\n&= 1 - 0.8^5 \\\\\n&= 0.672\n\\end{aligned}\n\\]\n\n\n(e)\nWhen n = 100, what is the probability that the jth observation is in the bootstrap sample?\nThe probability of the \\(j^{th}\\) observation being in the bootstrap sample is (1 - probability of \\(j^{th}\\) observation being not in the bootstrap sample). For \\(n=100\\), this is 0.634. \\[\n\\begin{aligned}\nProbability &= 1- (1 - \\ ^1/_n)^n \\\\\n&= 1 - (1-0.01)^{100} \\\\\n&= 1 - 0.99^{100} \\\\\n&= 0.634\n\\end{aligned}\n\\]\n\n\n(f)\nWhen n = 10, 000, what is the probability that the jth observation is in the bootstrap sample?\nAs calculated above, the probability of the \\(j^{th}\\) observation being in the bootstrap sample is (1 - probability of \\(j^{th}\\) observation being not in the bootstrap sample). For \\(n=10000\\), this is 0.632. \\[\n\\begin{aligned}\nProbability &= 1- (1 - \\ ^1/_n)^n \\\\\n&= 1 - (1-0.0001)^{10000} \\\\\n&= 1 - 0.9999^{10000} \\\\\n&= 0.632\n\\end{aligned}\n\\]\n\n\n(g)\nCreate a plot that displays, for each integer value of n from 1 to 100, 000, the probability that the jth observation is in the bootstrap sample. Comment on what you observe.\nWe can use the formula derived above to create such a plot in R. The code and output is shown below. We observe that as \\(n \\to \\infty\\), the value of this probability approaches an asymptote of approximately 0.632.\n\nn &lt;- 1:100000\nPr &lt;- 1 - (1 - 1 / n)^n\nplot(n, Pr)\nlibrary(ggplot2)\n\n\n\nggplot() +\n  geom_point(aes(n, Pr), shape = 20) +\n  labs(\n    x = \"Value of n\",\n    y = \"Probability (jth observation in bootstrap sample)\"\n  ) +\n  theme_classic() +\n  scale_y_continuous(\n    breaks = c(0.6, 0.632, 0.7, 0.8, 0.9, 1),\n    limits = c(0.6, 1)\n  )\n\n\n\n\n\n\n(h)\nWe will now investigate numerically the probability that a bootstrap sample of size n = 100 contains the jth observation. Here j = 4. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample. Comment on the results obtained.\nThe R-code and results are displayed below. The Code contains an implementation of a bootstrap method, where we take 10,000 samples (each containing 100 numbers) from the list of first 100 positive integers 1,2,3,…,100 with replacement. We then check whether the sample contains the integer 4. We then find whether each sample contains digit 4, and store the result (TRUE or FALSE) in the \\(i^{th}\\) value of store vector. The mean of store then gives the probability that a bootstrap of size n=100 contains the 4th observation.\n\nstore &lt;- rep(NA, 10000)\nfor (i in 1:10000) {\n  store[i] &lt;- sum(sample(1:100, replace = TRUE) == 4) &gt; 0\n}\nmean(store)\n## [1] 0.6311\n\nThe results reconcile with out calculated probability. It is very close to 0.632. Further, a known formula from Calculus states that: \\[\\lim_{n \\rightarrow \\infty}(1 + \\ ^x/_n \\ )^n = e^x\\] Here, \\(x=-1\\), and thus the expression is equivalent to \\(e^{-1}\\), which is 0.368. So, \\(1 - e^{-1}\\) is 0.632 which is the calculated answer for previous two questions."
  },
  {
    "objectID": "Chapter5e.html#question-3",
    "href": "Chapter5e.html#question-3",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 3",
    "text": "Question 3\nWe now review k-fold cross-validation.\n\n(a)\nExplain how k-fold cross-validation is implemented.\nIn k-fold cross validation, we first split up the data into \\(k\\) folds,\\(X_1, X_2, ... , X_k\\), each of length approximately \\(n/k\\).\n\nWe then use the first fold \\(X_1\\) of length \\(n/k\\) as the test set, and the remaining \\(k-1\\) folds combined, i.e. \\(X_2 + X_3 +... + X_k\\) of length $ n - n/k$, are used as the training set. The error rate is calculated, say \\(e_1\\).\nThen, the second fold fold \\(X_2\\) is used as as the test set, and the remaining \\(k-1\\) folds combined, i.e. \\(X_1 + X_3 +... + X_k\\), are used as the training set. The error rate is calculated, say \\(e_2\\)\nThis step is repeated for all \\(k\\) folds.\nThe average error rate is then used as the k-fold cross validation error rate: \\[ e = \\frac{1}{k} \\sum_{i=1}^{k} e_i \\]\n\n\n\n(b)\nWhat are the advantages and disadvantages of k-fold cross validation relative to:\n\ni. compared to the validation set approach?\nAdvantages of k-fold cross validation\n\nThe error rate is less variable. In validation set approach, the error rate is highly variable and depends on the specific validation and testing set chosen.\nWhen \\(k&gt;2\\), the k-fold cross validation yields more accurate estimates of the test error rate. This is because the validation set approach breaks up the data set into a validation set and a testing set. Due to lower number of observations in the validation set, the model gets to be trained on fewer observations. So, it overestimates the true error rates. In contrast, k-fold cross validation uses a larger training set, and somewhat ameliorates this situation.\n\n\nDisadvantages of k-fold cross validation\nIt is computationally more intensive, as the model needs to be fitted \\(k\\) times. But this is not a major concern in today’s computers.\n\n\nii. compared to LOOCV?\nAdvantages of k-fold cross validation\n\nAs compared to LOOCV, k-fold cross validation is computationally less expensive (when \\(k &lt; n\\)) because the model needs to be fitted a fewer number of times. A sole exception is linear regression, where a shortcut formula exists for calculating the LOOCV error rate using leverage value \\(h_i\\) of each observation.\nLOOCV averages the error rates over \\(n\\) different models, each run on a training set consisting of \\(n-1\\) observations. Since the training set of each LOOCV model is nearly identical, the error rates tend to be highly correlated. The average of highly correlated terms exhibits high variance. Thus, the LOOCV estimate suffers from high variance.\n\nDisadvantages of k-fold cross validation\n\nEach LOOCV model uses \\(n-1\\) observations to fit the model, and thus uses a training set which is nearly identical to the original data set. Thus, LOOCV suffers from a very low bias. Hence, in terms of accuracy of the estimated error rate, LOOCV performs better when bias of the estimate is a major concern.\nOverall, empirical evidence suggests that k-fold cross validation with \\(k=5\\) or \\(k=10\\) performs best considering this bias-variance trade off."
  },
  {
    "objectID": "Chapter5e.html#question-4",
    "href": "Chapter5e.html#question-4",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 4",
    "text": "Question 4\nSuppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\nTo estimate the standard deviation of response \\(Y\\) for a particular value of predictor (say \\(X = x\\)), we can use the bootstrap method as follows:\nFirst, for some large number of repetitions \\(B\\), we repeatedly take a random sample with replacement from the observations of \\(X\\). Lets call these samples \\(X^{*1}, X^{*2} ... X^{*B}\\). - We use the random samples \\(X^{*1}, X^{*2} ... X^{*B}\\) and their corresponding response values (\\(Y\\)) to train the statistical learning method one after the other. - For each repetition we estimate the predicted value of response i.e. \\(\\hat{y^{*i}}\\) for the particular value of predictor \\(X=x\\). We term these predictions as \\(\\hat{Y} = \\hat{y^{*1}}, \\hat{y^{*2}} ... \\hat{y^{*B}}\\).\nThen we find the standard deviation of this estimated \\(\\hat{Y}\\). This standard deviation is the bootstrap estimate for the true standard deviation of response \\(Y\\). \\[ SE_B(Y) = \\sqrt {\\frac{1}{B-1} \\sum_{i=1}^{B} \\big( \\hat{y}^{*i} - \\overline{\\hat{y}}  \\big)^2   }\\] where, \\[ \\overline{\\hat{y}} = \\frac{1}{B} \\sum_{i'=1}^{B} y^{*i'}\\].\nThis formula can be re-written as equation 5.8 from the text book: \\[ SE_B(Y) = \\sqrt {\\frac{1}{B-1} \\sum_{i=1}^{B} \\Big( \\hat{y}^{*i} - \\frac{1}{B} \\sum_{i'=1}^{B} y^{*i'}  \\Big)^2   }\\]"
  },
  {
    "objectID": "Chapter5e.html#question-6",
    "href": "Chapter5e.html#question-6",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 6",
    "text": "Question 6\nIn Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n\n(a)\nFit a logistic regression model that uses income and balance to predict default.\nWe fit the logistic regression model as shown below:\n\nlibrary(ISLR)\nlibrary(kableExtra)\ndata(\"Default\")\nfit.glm &lt;- glm(default ~ income + balance, data = Default, family = binomial)\n\n# To print table\nsummary(fit.glm)$coef |&gt;\n  kbl(digits = 5) |&gt;\n  kableExtra::kable_material()\n\n\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n-11.54047\n0.43476\n-26.54468\n0e+00\n\n\nincome\n0.00002\n0.00000\n4.17418\n3e-05\n\n\nbalance\n0.00565\n0.00023\n24.83628\n0e+00\n\n\n\n\n\n\n\n\n\n(b)\nUsing the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\nWe use a random vector train to divide equal number of observations into the training set and validation set. We also set seed for replicability. The results show that the validation set error rate is 2.64 %.\n\nset.seed(3)\n# i. Split the sample set into a training set and a validation set.\ntrain &lt;- sample(x = 1:nrow(Default), size = nrow(Default) / 2)\n\n# ii. Fit a multiple logistic regression model using only the training observations\nfit.glm &lt;- glm(default ~ income + balance,\n  data = Default,\n  family = binomial, subset = train\n)\n\n# Check whether 1 stands for Yes or No in default\ncontrasts(Default$default)\n\n    Yes\nNo    0\nYes   1\n\n# iii. Obtain a prediction of default status for each individual\nprob.glm &lt;- predict(fit.glm,\n  newdata = Default[-train, ],\n  type = \"response\"\n)\n# Compute predicted response and create Confusion Matrix\npred.glm &lt;- ifelse(prob.glm &gt; 0.5, yes = \"Yes\", no = \"No\")\ntable(pred.glm, Default$default[-train])\n\n        \npred.glm   No  Yes\n     No  4822  109\n     Yes   23   46\n\n# iv. Compute the validation set error rate\nmean(pred.glm != Default$default[-train])\n\n[1] 0.0264\n\n\n\n\n(c)\nRepeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\nThe 3 replications are done in the code below. The error rates obtained for the validation set are 2.8 %, 2.74 % and 2.4 % which shows a fair amount of variability in the results depending on which specific set of observations are used for training and validation set.\n\nset.seed(3)\nError.Rates &lt;- rep(NA, 3)\nfor (i in 1:3) {\n  train &lt;- sample(x = 1:nrow(Default), size = nrow(Default) / 2)\n  fit.glm &lt;- glm(default ~ income + balance,\n    data = Default,\n    family = binomial, subset = train\n  )\n  prob.glm &lt;- predict(fit.glm,\n    newdata = Default[-train, ],\n    type = \"response\"\n  )\n  pred.glm &lt;- ifelse(prob.glm &gt; 0.5, yes = \"Yes\", no = \"No\")\n  Error.Rates[i] &lt;- mean(pred.glm != Default$default[-train])\n}\nError.Rates\n\n[1] 0.0264 0.0280 0.0274\n\n\n\n\n(d)\nNow consider a logistic regression model that predicts the probability of “default” using “income”, “balance”, and a dummy variable for “student”. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for “student” leads to a reduction in the test error rate.\nThe new logistic regression model is fit as shown below, and the test error using validation set approach is computed. The validation set error rate is 2.5 %. This error rate is nearly same as before. Thus, adding student variable in the logistic regression model does not lead to any significant reduction in test error rate.\n\n# Checking which values represent student status in `student`\ncontrasts(Default$student)\n\n    Yes\nNo    0\nYes   1\n\n# fitting multiple logistic regression model and computing predicted responses\nfit.glm &lt;- glm(default ~ income + balance + student,\n  data = Default,\n  family = binomial, subset = train\n)\nprob.glm &lt;- predict(fit.glm,\n  newdata = Default[-train, ],\n  type = \"response\"\n)\npred.glm &lt;- ifelse(prob.glm &gt; 0.5, yes = \"Yes\", no = \"No\")\n\n# Display Confusion Matrix\ntable(pred.glm, Default$default[-train])\n\n        \npred.glm   No  Yes\n     No  4816  123\n     Yes   18   43\n\n# Computing validation set error rate\nmean(pred.glm != Default$default[-train])\n\n[1] 0.0282"
  },
  {
    "objectID": "Chapter5e.html#question-6-1",
    "href": "Chapter5e.html#question-6-1",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 6",
    "text": "Question 6\nWe continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n\n(a)\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\nThe coefficients and their associated standard errors for income and balance are shown below.\n\nfit.glm &lt;- glm(default ~ income + balance, \n               data = Default, \n               family = binomial)\n\n# Displaying coefficients and standard errors\nsummary(fit.glm)$coef[2:3,1:2]\n##             Estimate   Std. Error\n## income  2.080898e-05 4.985167e-06\n## balance 5.647103e-03 2.273731e-04\n\n\n\n(b)\nWrite a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nWe write the function in R code as below:—\n\nboot.fn &lt;- function(data, index){\nfit &lt;- glm(default ~ income + balance, data = data, \n           family = binomial, subset = index)\nreturn((fit)$coef)\n}\n# boot.fn(Default, 1:nrow(Default)) \n# Trial code to test the correctness of function\n\n\n\n(c)\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\nWe use the boot function from boot library in Rto compute the standard errors as follows:\n\nlibrary(boot)\nb1 = boot(data = Default, statistic = boot.fn, R = 1000)\nb1\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Default, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n         original        bias     std. error\nt1* -1.154047e+01 -4.168442e-02 4.382152e-01\nt2*  2.080898e-05  2.107314e-07 4.865876e-06\nt3*  5.647103e-03  1.981887e-05 2.311037e-04\n\n\n\n\n(d)\nComment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\nThe coefficient estimates from the bootstrap and glm() function are nearly identical. Similarly, the standard error for income coefficient is 4.98e-6 and 5.02e-6, which are nearly the same. Lastly, the standard error for balance coefficient is 2.27e-4 and 2.31e-4, which are nearly same. Thus, it can be concluded that glm() and bootstrap are in agreement. The logistic regression model fits the data quite well."
  },
  {
    "objectID": "Chapter5e.html#question-7",
    "href": "Chapter5e.html#question-7",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 7",
    "text": "Question 7\nIn sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the “Weekly” data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).\n\n(a)\nFit a logistic regression model that predicts “Direction” using “Lag1” and “Lag2”.\n\nlibrary(ISLR)\ndata(Weekly)\nfit.glm &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial)\n\n\n\n(b)\nFit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation.\n\ntest &lt;- 1\nfit.glm &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial,\n               subset = -test)\n\n\n\n(c)\nUse the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if \\(P(Direction=Up|Lag1, Lag2) &gt; 0.5\\). Was this observation correctly classified?\nThe code given below predicts the Direction for the first observation. It also shows that the first observation was incorrectly classified.\n\nprob.glm &lt;- predict(fit.glm, newdata = Weekly[test,], type = \"response\")\n\n# Check the value of Up in dataset\ncontrasts(Weekly$Direction)\n##      Up\n## Down  0\n## Up    1\n# Predict the response for observation no. 1\npred.glm &lt;- ifelse(test = prob.glm &gt; 0.5, yes = \"Up\", no = \"Down\")\npred.glm\n##    1 \n## \"Up\"\n\n# This observation was correctly classified: TRUE or FALSE\npred.glm == Weekly$Direction[test]\n## [1] FALSE\n\n\n\n(d)\nWrite a for loop from i = 1 to i = n, where n is the number of observations in the data set, that performs each of the following steps\nThe code is written below using for loops. The result is stored in the vector Errors. The final result indicates that nearly 45% of the looped models resulted in an erroneous prediction.\n\n# Create a vector Errors to store results of loops\nErrors &lt;- rep(NA, nrow(Weekly))\n# Creating for loop from 1 to n\nfor (i in 1:nrow(Weekly)) {\n  test &lt;- i\n  \n  # i. Fit a logistic regression model using all but the ith observation \n  fit.glm &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial,\n               subset = -test)\n  # ii. Compute the posterior probability of the market moving \"Up\" for \n  #     the ith observation.\n  prob.glm &lt;- predict(fit.glm, newdata = Weekly[test,], type = \"response\")\n  \n  # iii. Use the posterior probability for the ith observation in order \n  #      to predict whether or not the market moves up.\n  pred.glm &lt;- ifelse(test = prob.glm &gt; 0.5, yes = \"Up\", no = \"Down\")\n  \n  # iv. Determine whether or not an error was made in predicting\n  #     the direction for the ith observation. If an error was made,\n  #     then indicate this as a 1, and otherwise indicate it as a 0.\n  Errors[i] &lt;- ifelse(pred.glm != Weekly$Direction[test], yes = 1, no = 0)\n}\nmean(Errors)\n\n[1] 0.4499541\n\n\n\n\n(e)\nTake the average of the n numbers obtained in (d) in order to obtain the LOOCV estimate for the test error. Comment on the results.\nAs shown above. the mean(Errors) shows us LOOCV estimate for test error rate as 44.995 %. This is an exceptionally high error rate, and only slightly better than random guessing error rate of 50 %."
  },
  {
    "objectID": "Chapter5e.html#question-8",
    "href": "Chapter5e.html#question-8",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 8",
    "text": "Question 8\nWe will now perform cross-validation on a simulated data set.\n\n(a)\nIn this data set, what is n and what is p? Write out the model used to generate the data in equation form.\nWe generate a simulated data set as follows, based on the code given in the question. In this data set, \\(n = 100\\) and \\(p=2\\). The model used to generate the data in equation form is: \\[ y = x \\ - \\ 2x^2 \\ + \\ \\epsilon\\]\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- x - 2 * x^2 + rnorm(100)\n\n\n\n(b)\nCreate a scatter-plot of X against Y . Comment on what you find.\nThe scatter-plot of \\(y\\) vs. \\(x\\) is shown below. It shows a prominent non-linear pattern between \\(x\\) and \\(y\\).\n\nplot(x,y)\n\n\n\n\n\n\n(c)\nSet a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\nThe code below uses for loops to calculate LOOCV error rate for the fitted models involving polynomials of \\(x\\). The results are displayed in the Result data-frame at the end. It is clear the the LOOCV error rate of model (i) is very high, and thereafter the other models have similar error rates. There seems no benefit of adding \\(x^3\\) and \\(x^4\\) to the model.\n\nlibrary(boot)\n# Setting random seed\nset.seed(3)\n\n# Creating Data Frames for x and y, and for storing final result\nDataset &lt;- data.frame(x, y)\nResult &lt;- data.frame(\n  Model = c(\"i\", \"ii\", \"iii\", \"iv\"),\n  ErrorRate = rep(NA, 4)\n)\n\n# Using loops to calculate LOOCV Error Rate\nfor (i in 1:4) {\n  fit &lt;- glm(y ~ poly(x, i), data = Dataset)\n  Result[i, 2] &lt;- cv.glm(data = Dataset, fit)$delta[1]\n}\n\n# Displaying the error rates for all four models\nResult\n\n  Model ErrorRate\n1     i 7.2881616\n2    ii 0.9374236\n3   iii 0.9566218\n4    iv 0.9539049\n\n\n\n\n(d)\nRepeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?\nWe now repeat the results using another seed, lets say set.seed(9). The results are computed below. The resulting error rates are exactly the same irrespective of the random seed. This is expected because LOOCV eliminates the aspect of randomness by using every single observation as a validation set. Thus, LOOCV estimate will never vary based on random seed. Further, in case of linear regression, LOOCV estimates can be computed with a short cut formula which does not depend on random sampling at all.\n\nlibrary(boot)\nset.seed(9)\nDataset &lt;- data.frame(x, y)\nResult1 &lt;- data.frame(\n  Model = c(\"i\", \"ii\", \"iii\", \"iv\"),\n  ErrorRate = rep(NA, 4)\n)\nfor (i in 1:4) {\n  fit &lt;- glm(y ~ poly(x, i), data = Dataset)\n  Result1[i, 2] &lt;- cv.glm(data = Dataset, fit)$delta[1]\n}\nResult1 |&gt; kbl()\n\n\n\n\nModel\nErrorRate\n\n\n\n\ni\n7.2881616\n\n\nii\n0.9374236\n\n\niii\n0.9566218\n\n\niv\n0.9539049\n\n\n\n\n\n\n\n\n\n(e)\nWhich of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.\nIn the models computed in (c) above, the model with \\(x\\) and \\(x^2\\), i.e. model (ii) has the lowest LOOCV error. This is expected, as the true model is same as the estimate model (ii). We had created the true model in part (a).\n\n\n(f)\nComment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\nWe can compute the coefficient estimates from fitting the models again as follows, and print the summary(fit) to examine the statistical significance of each predictor. The predictors \\(x\\) and \\(x^2\\) are statistically significant predictors in all models. However, the predictors \\(x^3\\) and \\(x^4\\) are not statistically significant. These results agree with the conclusions drawn based on cross-validation results which showed that the error rates for models using \\(x^3\\) and \\(x^4\\) are not any better than previous models. Adding the terms \\(x^3\\) and \\(x^4\\) did not lower the error rate.\n\nfor (i in 1:4) {\n  fit &lt;- lm(y ~ poly(x, i), data = Dataset)\n  print(summary(fit))\n}\n\n\nCall:\nlm(formula = y ~ poly(x, i), data = Dataset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5161 -0.6800  0.6812  1.5491  3.8183 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -1.550      0.260  -5.961 3.95e-08 ***\npoly(x, i)     6.189      2.600   2.380   0.0192 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.6 on 98 degrees of freedom\nMultiple R-squared:  0.05465,   Adjusted R-squared:  0.045 \nF-statistic: 5.665 on 1 and 98 DF,  p-value: 0.01924\n\n\nCall:\nlm(formula = y ~ poly(x, i), data = Dataset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9650 -0.6254 -0.1288  0.5803  2.2700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -1.5500     0.0958  -16.18  &lt; 2e-16 ***\npoly(x, i)1   6.1888     0.9580    6.46 4.18e-09 ***\npoly(x, i)2 -23.9483     0.9580  -25.00  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.958 on 97 degrees of freedom\nMultiple R-squared:  0.873, Adjusted R-squared:  0.8704 \nF-statistic: 333.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nCall:\nlm(formula = y ~ poly(x, i), data = Dataset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9765 -0.6302 -0.1227  0.5545  2.2843 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -1.55002    0.09626 -16.102  &lt; 2e-16 ***\npoly(x, i)1   6.18883    0.96263   6.429 4.97e-09 ***\npoly(x, i)2 -23.94830    0.96263 -24.878  &lt; 2e-16 ***\npoly(x, i)3   0.26411    0.96263   0.274    0.784    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9626 on 96 degrees of freedom\nMultiple R-squared:  0.8731,    Adjusted R-squared:  0.8691 \nF-statistic: 220.1 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\nCall:\nlm(formula = y ~ poly(x, i), data = Dataset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0550 -0.6212 -0.1567  0.5952  2.2267 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -1.55002    0.09591 -16.162  &lt; 2e-16 ***\npoly(x, i)1   6.18883    0.95905   6.453 4.59e-09 ***\npoly(x, i)2 -23.94830    0.95905 -24.971  &lt; 2e-16 ***\npoly(x, i)3   0.26411    0.95905   0.275    0.784    \npoly(x, i)4   1.25710    0.95905   1.311    0.193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9591 on 95 degrees of freedom\nMultiple R-squared:  0.8753,    Adjusted R-squared:  0.8701 \nF-statistic: 166.7 on 4 and 95 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Chapter5e.html#question-9",
    "href": "Chapter5e.html#question-9",
    "title": "Chapter 5 (Exercises)",
    "section": "Question 9",
    "text": "Question 9\n\n(a)\nThe estimate for population mean of medv is 22.53. This is the sample mean \\(\\hat{\\mu}\\) which is an unbiased estimate for \\(\\mu\\), the true population mean. The R code to compute it is given below.\n\nlibrary(MASS)\ndata(Boston)\nattach(Boston)\nmean(medv)\n\n[1] 22.53281\n\n\n\n\n(b)\nThe estimated standard error for \\(\\hat{\\mu}\\) can be given by \\(sd/\\sqrt{n}\\), where \\(sd\\) stands for standard deviation of the random variable whose true mean is \\(\\mu\\) and \\(n\\) represents number of observations in the sample. The R code below calculates this estimated standard error as 0.409.\n\nsd(medv)/(sqrt(nrow(Boston)))\n\n[1] 0.4088611\n\n\n\n\n(c)\nWe now compute the standard error of estimated mean of medv using bootstrap. The estimated standard error using bootstrap method is 0.408, which is reasonably close to the previously calculated standard error of 0.409.\n\nlibrary(boot)\n\n# Creating a function which returns mean of medv as output\nboot.fn &lt;- function(data, index) {\n  return(mean(data$medv[index]))\n}\n\n# Testing the boot.fn on whole Boston data set to ensure it works\nboot.fn(Boston, 1:nrow(Boston))\n## [1] 22.53281\n\n# Using bootstrap to calculate standard error\nset.seed(3)\nboot(data = Boston, statistic = boot.fn, R = 10000)\n## \n## ORDINARY NONPARAMETRIC BOOTSTRAP\n## \n## \n## Call:\n## boot(data = Boston, statistic = boot.fn, R = 10000)\n## \n## \n## Bootstrap Statistics :\n##     original       bias    std. error\n## t1* 22.53281 0.0002812451    0.407684\n\n\n\n(d)\nWe compute the 95% confidence interval for mean of medv using bootstrap standard error estimates. The resulting 95% Confidence Interval is [21.72 , 23.35]. The 95 % confidence interval computed using the t.test function in R is [21.73 , 23.34]. Thus, we conclude the the two confidence intervals are nearly identical.\n\nmean(medv) - 2 *0.408\n\n[1] 21.71681\n\nmean(medv) + 2 *0.408\n\n[1] 23.34881\n\n# Using t.test to calculate the 95 % Confidence Interval\nt.test(Boston$medv)\n\n\n    One Sample t-test\n\ndata:  Boston$medv\nt = 55.111, df = 505, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.72953 23.33608\nsample estimates:\nmean of x \n 22.53281 \n\n\n\n\n(e)\nThe estimated median value of medv in the population is the median of the Boston sample data set. This \\(\\hat{\\mu}\\) value is 21.2 as calculated below.\n\nmedian(Boston$medv)\n\n[1] 21.2\n\n\n\n\n(f)\nAs stated in the question, there is no easy formula to compute the standard error associated with a estimated sample median. Instead, we can use bootstrap method to compute the estimated standard error associated with the sample median. The estimated standard error is 0.37.\n\nlibrary(boot)\n\n# Creating a function which returns median of medv as output\nboot.fn1 &lt;- function(data, index){\n  return(median(data$medv[index]))\n}\n\n# Testing the boot.fn on whole Boston data set to ensure it works\nboot.fn1(Boston, 1:nrow(Boston))\n\n[1] 21.2\n\n# Using bootstrap to calculate standard error\nset.seed(3)\nboot(data = Boston, statistic = boot.fn1, R = 10000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston, statistic = boot.fn1, R = 10000)\n\n\nBootstrap Statistics :\n    original    bias    std. error\nt1*     21.2 -0.013245   0.3779208\n\n\n\n\n(g)\nWe can compute the estimate for 10^{th} percentile of true population medv using the quantile() function of the medv in Boston data set. The estimated \\(\\hat{\\mu_{0.1}}\\) is 12.75 as shown below.\n\nquantile(Boston$medv, 0.1)\n\n  10% \n12.75 \n\n\n\n\n(h)\nAgain, there is no easy mathematical formula to calculate estimated standard error associated with 10th percentile of a sample. We can use bootstrap to calculate the estimated standard error as shown below. The estimated standard error for \\(\\hat{\\mu_{0.1}}\\) is 0.5.\n\nlibrary(boot)\n\n# Creating a function which returns 10th percentile of medv as output\nboot.fn2 &lt;- function(data, index) {\n  return(quantile(data$medv[index], 0.1))\n}\n\n# Testing the boot.fn on whole Boston data set to ensure it works\nboot.fn2(Boston, 1:nrow(Boston))\n\n  10% \n12.75 \n\n# Using bootstrap to calculate standard error\nset.seed(3)\nboot(data = Boston, statistic = boot.fn2, R = 10000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston, statistic = boot.fn2, R = 10000)\n\n\nBootstrap Statistics :\n    original   bias    std. error\nt1*    12.75 0.011105   0.5001274"
  },
  {
    "objectID": "Chapter5l.html#cross-validation-and-the-bootstrap",
    "href": "Chapter5l.html#cross-validation-and-the-bootstrap",
    "title": "Chapter 5 (Lab)",
    "section": "Cross-Validation and the Bootstrap",
    "text": "Cross-Validation and the Bootstrap\n\n5.3.1 The Validation Set Approach\nWe try out the Validation Set approach using Auto data set, and predict mpg using horsepower.\n\nlibrary(ISLR)\nset.seed(1)\ndata(Auto)\nattach(Auto)\nlibrary(tidyverse)\nlibrary(kableExtra)\n\n# Create a train vector, to create a random sample as \"Training Set\".\ntrain &lt;- sample(1:392, size = 196, replace = FALSE)\n\n# Create a linear regression.\nfit.lm &lt;- lm(mpg ~ horsepower, data = Auto, subset = train)\npred.lm &lt;- predict(fit.lm, newdata = Auto[-train, ])\nMSE.lin &lt;- mean((pred.lm - Auto$mpg[-train])^2)\nMSE.lin\n\n[1] 23.26601\n\n# Create polynomial regression 2 and 3\nfit.lm2 &lt;- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\npred.lm2 &lt;- predict(fit.lm2, newdata = Auto[-train, ])\nMSE.lin2 &lt;- mean((pred.lm2 - Auto$mpg[-train])^2)\nMSE.lin2\n\n[1] 18.71646\n\nfit.lm3 &lt;- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\npred.lm3 &lt;- predict(fit.lm3, newdata = Auto[-train, ])\nMSE.lin3 &lt;- mean((pred.lm3 - Auto$mpg[-train])^2)\nMSE.lin3\n\n[1] 18.79401\n\n# Create 10 different validation sets and plot the MSE from polynomials upto 10\nResult &lt;- data.frame(\n  Poly1 = rep(NA, 10),\n  Poly2 = rep(NA, 10),\n  Poly3 = rep(NA, 10),\n  Poly4 = rep(NA, 10),\n  Poly5 = rep(NA, 10),\n  Poly6 = rep(NA, 10),\n  Poly7 = rep(NA, 10),\n  Poly8 = rep(NA, 10),\n  Poly9 = rep(NA, 10),\n  Poly10 = rep(NA, 10),\n  Repetition = c(1:10)\n)\nfor (p in 1:10) {\n  for (i in 1:10) {\n    set.seed(i)\n    train &lt;- sample(1:392, size = 196, replace = FALSE)\n    fit.lm &lt;- lm(mpg ~ poly(horsepower, p), data = Auto, subset = train)\n    pred.lm &lt;- predict(fit.lm, newdata = Auto[-train, ])\n    Result[i, p] &lt;- mean((pred.lm - Auto$mpg[-train])^2)\n  }\n}\nkable(Result, digits = 1) |&gt; kable_classic_2()\n\n\n\n\nPoly1\nPoly2\nPoly3\nPoly4\nPoly5\nPoly6\nPoly7\nPoly8\nPoly9\nPoly10\nRepetition\n\n\n\n\n23.3\n18.7\n18.8\n19.2\n19.4\n19.6\n19.0\n19.1\n19.1\n22.9\n1\n\n\n25.7\n20.4\n20.4\n20.3\n19.8\n19.6\n19.5\n19.9\n20.8\n22.1\n2\n\n\n22.0\n17.7\n17.7\n17.6\n17.1\n17.1\n17.2\n17.2\n17.2\n18.2\n3\n\n\n24.3\n19.0\n19.0\n19.3\n19.1\n19.0\n18.9\n19.3\n19.5\n19.5\n4\n\n\n20.9\n16.8\n17.0\n17.3\n16.8\n16.6\n16.6\n16.5\n16.6\n18.2\n5\n\n\n23.6\n17.7\n17.7\n18.0\n17.6\n17.3\n17.1\n17.2\n17.2\n17.5\n6\n\n\n22.3\n19.7\n20.0\n19.9\n19.2\n18.9\n18.7\n19.7\n22.4\n26.5\n7\n\n\n24.9\n18.9\n18.9\n19.2\n19.2\n20.4\n21.7\n19.8\n25.0\n22.4\n8\n\n\n27.5\n22.1\n22.5\n22.4\n21.6\n21.3\n21.1\n21.2\n21.2\n21.2\n9\n\n\n26.4\n19.9\n20.3\n20.2\n19.3\n19.0\n18.7\n18.7\n18.7\n21.1\n10\n\n\n\n\n\n\n\nThese results show that a quadratic fit is much better than linear fit, but the fits of higher order are not significantly better as MSE stays the same.\n\n# Attempt to reproduce graph in Figure 5.2 (right hand side panel)\nResultLong &lt;- gather(Result, key = \"Poly\", value = \"MSE\", -Repetition)\nResultLong &lt;- ResultLong %&gt;%\n  mutate(Poly = parse_number(Poly)) %&gt;%\n  mutate(Repetition = paste0(\"Rep\", Repetition, sep = \"\"))\nggplot(data = ResultLong) +\n  geom_line(aes(x = Poly, y = MSE, color = Repetition), lwd = 1) +\n  theme(legend.position = \"none\") +\n  theme_bw()\n\n\n\n\n\n\n5.3.2 Leave-One-Out Cross Validation (LOOCV)\nPerform LOOCV using cv.glm function of the boot library.\n\nlibrary(boot)\nfit.glm &lt;- glm(mpg ~ horsepower, data = Auto)\ncv.err &lt;- cv.glm(data = Auto, glmfit = fit.glm)\nnames(cv.err)\n\n[1] \"call\"  \"K\"     \"delta\" \"seed\" \n\ncv.err$delta\n\n[1] 24.23151 24.23114\n\n\nNow, we create a loop using for function to calculate LOOCV error rate for polynomials 1 to 5.\n\nset.seed(17)\ncv.error = rep(NA, 10)\nfor (i in 1:10) {\n  fit.glm &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] &lt;- cv.glm(data = Auto, glmfit = fit.glm)$delta[1]\n}\nround(cv.error, 2)\n\n [1] 24.23 19.25 19.33 19.42 19.03 18.98 18.83 18.96 19.07 19.49\n\n\n\n\n5.3.3 k-Fold Cross Validation\nWe now use the cv.glm function to do k-fold CV (using k=10).\n\nset.seed(17)\ncv.error.10 &lt;- rep(NA, 10)\nfor (i in 1:10) {\n  fit.glm &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] &lt;- cv.glm(data = Auto, glmfit = fit.glm)$delta[1]\n}\nround(cv.error.10, 2)\n\n [1] 24.23 19.25 19.33 19.42 19.03 18.98 18.83 18.96 19.07 19.49\n\n\n\n\n5.3.4 The Bootstrap\n\nlibrary(ISLR)\ndata(\"Portfolio\")\nsummary(Portfolio)\n\n       X                  Y           \n Min.   :-2.43276   Min.   :-2.72528  \n 1st Qu.:-0.88847   1st Qu.:-0.88572  \n Median :-0.26889   Median :-0.22871  \n Mean   :-0.07713   Mean   :-0.09694  \n 3rd Qu.: 0.55809   3rd Qu.: 0.80671  \n Max.   : 2.46034   Max.   : 2.56599  \n\n# Creating a function with an index which can then be use for the boot function\n# alpha.fn to return alpha with lowest variance\nalpha.fn &lt;- function(data, index){\n  X &lt;- data$X[index]\n  Y &lt;- data$Y[index]\n  return((var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2*cov(X,Y) ) )\n}\n\n# Using alpha.fn to find alpha using the full data set\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.5758321\n\n# Using alpha.fn to find alpha using a random sample of 100 with replacement\nset.seed(2)\nalpha.fn(Portfolio, sample(100, 100, replace = TRUE))\n\n[1] 0.5539577\n\n# We can repeat this many many times using different seed. Or,\n\n# We can use boot() function to automate this\nlibrary(boot)\nboot(Portfolio, alpha.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 0.0009480333  0.09303824\n\n\nEstimating the accuracy of a linear regression model\n\n# Comparing coefficients from a linear regression of mpg onto horsepower,\n# from standard least squares regression and bootstrap methods.\n\n# Creating a function to be used in bootstrap which computes output i.e. coefficients\n# of interest, and takes in an index vector as input.\nboot.fn &lt;- function(data, index) {\n  return(coef(lm(mpg ~ horsepower, data = data, subset = index)))\n}\n\n# Testing the boot.fn on the entire Auto data set\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n# Getting coefficient estimates using a random sample of 392 with replacement\nboot.fn(Auto, sample(392, 392, replace = TRUE))\n\n(Intercept)  horsepower \n 39.5910901  -0.1524194 \n\nboot.fn(Auto, sample(392, 392, replace = TRUE))\n\n(Intercept)  horsepower \n 41.1261106  -0.1690852 \n\n# We can repeat this command a thousand times, or\n\n# Use bootstrap to automate this process\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0907930895  0.82249078\nt2* -0.1578447 -0.0007940743  0.00708581\n\n# Compare these with standard errors from least squares estimates\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\nNow, we perform bootstrap to compare standard errors from least squares estimates of linear regression of mpg on quadratic form of horsepower, and standard errors computed using bootstrap.\n\n# creating a boot.fn to be used in boot() later\nboot.fn &lt;- function(data, index){\n  return(coefficients(lm(mpg ~ horsepower + I(horsepower^2), data = data, subset = index)))\n}\nboot.fn(Auto, 1:392)\n\n    (Intercept)      horsepower I(horsepower^2) \n   56.900099702    -0.466189630     0.001230536 \n\nset.seed(1)\n\n# Using boot() to estimate standard errors of coefficients\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\n# Compare with standard errors of least square estimates\nsummary(lm(mpg~horsepower+I(horsepower^2), data = Auto))$coef\n\n                    Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\nhorsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\nI(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21"
  },
  {
    "objectID": "Chapter6e.html",
    "href": "Chapter6e.html",
    "title": "Chapter 6 (Exercises)",
    "section": "",
    "text": "library(ISLR)\nlibrary(kableExtra)"
  },
  {
    "objectID": "Chapter6e.html#question-1",
    "href": "Chapter6e.html#question-1",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 1",
    "text": "Question 1\n\n(a)\nWhich of the three models with \\(k\\) predictors has the smallest training RSS?\nThe Best Subset Selection model will have the smallest training RSS. The reasoning is that for the model with \\(k\\) predictors:\n\nThe Best Subset Selection model with \\(k\\) predictors is the best one out of \\(p\\choose{k}\\) models with \\(k\\) predictors. It has lowest training Residual Sum of Squares (RSS) amongst these models.\nThe Forward Step-wise Selection model with \\(k\\) predictors is the best out of only \\((p-k)\\) models which augment the predictors in \\(\\mathcal{M}_{k-1}\\) model.\nThe Backward Step-wise Selection model with \\(k\\) predictors is the best one amongst \\(k\\) models that contain all but one less predictor than the \\(\\mathcal{M}_{k+1}\\) model.\nThus, amongst the three, Best Subset Selection chooses this model from the widest possible option list of models with \\(k\\) predictors. So, it will always have the smallest training RSS.\n\n\n\n(b)\nWhich of the three models with k predictors has the smallest test RSS?\nIt is difficult to predict, as any of the three models could have the lowest test RSS. The Test RSS is totally different than Training RSS. A model with \\(k\\) parameters which has the lowest training RSS (in this case, the Best Subset Selection model) could easily overfit the training data leading to a misleadingly low Training RSS, but a very high Test RSS.\nThus, at large values of \\(k\\) (\\(k \\approx n\\)), overfitting will lead to Best Subset Selection models with lowest training RSS to have a high Test RSS. Hence, any of the three models could have smallest Test RSS depending on the specific situation and value of \\(k\\).\n\n\n(c)\nTrue or False:\ni. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.\nTRUE. The \\(\\mathcal{M}_{k+1}\\) model in Forward stepwise selection is chosen by adding a predictor to the existing \\(\\mathcal{M}_{k}\\) model, after considering the best amongst \\(p-k\\) models that can augment the existing \\(\\mathcal{M}_{k}\\) model. So, the predictors in \\(\\mathcal{M}_{k}\\) model identified by forward stepwise selection will always be a subset of the predictors in \\(\\mathcal{M}_{k+1}\\) model.\nii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.\nTRUE. The \\(\\mathcal{M}_{k}\\) model in Backward stepwise selection is chosen by dropping one predictor from the existing \\(\\mathcal{M}_{k+1}\\) model, after considering the best amongst \\(k\\) model options (each of which drops a different predictor). So, the predictors in \\(\\mathcal{M}_{k}\\) model identified by Backward stepwise selection will always be a subset of the predictors in \\(\\mathcal{M}_{k+1}\\) model.\niii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by forward stepwise selection.\nFALSE. The \\(\\mathcal{M}_{k}\\) model in Backward stepwise selection is reached after consecutively dropping \\(p-k\\) predictors, one at each step, starting from the full model \\(\\mathcal{M}_{p}\\). In contrast, the \\(\\mathcal{M}_{k+1}\\) model in Forward stepwise selection is arrived at by consecutively adding a predictor at each step to the Null Model \\(\\mathcal{M}_{0}\\). There is no guarantee that the iterations in these two different algorithms will arrive at the same set of predictors for each value of \\(k\\). Hence, the \\(\\mathcal{M}_{k}\\) model in Backward stepwise selection could contain some predictors which have not yet been added into the \\(\\mathcal{M}_{k+1}\\) model of Forward stepwise selection.\niv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.\nFALSE. The \\(\\mathcal{M}_{k}\\) model in Forward stepwise selection is reached after adding a predictor at each of the \\(k\\) steps to the initial Null Model \\(\\mathcal{M}_{0}\\). On the other hand, the \\(\\mathcal{M}_{k+1}\\) model in Backward stepwise selection is reached after consecutively dropping \\(p-(k+1)\\) predictors, one at each step, starting from the full model \\(\\mathcal{M}_{p}\\). There is no guarantee that the iterations in these two different algorithms will arrive at the same set of predictors for each value of \\(k\\). Hence, the \\(\\mathcal{M}_{k+1}\\) model in Forward stepwise selection could contain some predictors which have already been dropped from the \\(\\mathcal{M}_{k+1}\\) model of Backward stepwise selection.\nv. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.\nFALSE. The \\(k\\)-variable Best subset selection model (\\(\\mathcal{M}_{k}\\)) is selected after comparing the full set of \\(p\\choose{k}\\) models containing \\(k\\) predictors. This selection process is not based on the models \\(\\mathcal{M}_{k+1}\\) , \\(\\mathcal{M}_{k-1}\\) or any other model. Thus, there is no guarantee that all of the predictors contained in \\(\\mathcal{M}_{k}\\) will be present in \\(\\mathcal{M}_{k+1}\\) model as well."
  },
  {
    "objectID": "Chapter6e.html#question-2",
    "href": "Chapter6e.html#question-2",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 2",
    "text": "Question 2\n\n(a)\nFor lasso, answer is (iii). Lasso is less flexible than least squares regression. It produces a sparse model and lowers the variance of the coefficients. The reduced flexibility does lead to some increase in bias. Thus, Lasso will give improved prediction accuracy when its increase in bias (more accurately, Bias Squared) is less than its decrease in variance.\n\n\n(b)\nFor ridge regression, answer is again (iii). Ridge Regression is shrinkage method like Lasso. It will constrain or regularize all the coefficients towards zero, thereby decreasing their variance. Thus, it is less flexible than least squares regression. The reduced flexibility does lead to some increase in bias. Thus, Ridge regression will give improved prediction accuracy when its increase in bias (more accurately, Bias Squared) is less than its decrease in variance.\n\n\n(c)\nFor non-linear methods, the answer is (ii). Non-linear methods are more flexible than linear least squares regression, since they allow the fit to vary over different portions of the predictor set. This leads to a lower bias, but higher variance in the estimates produced by non-linear methods. Hence, non-linear methods will give improved prediction accuracy when the increase in variance is less than its decrease in bias (or, more accurately, Bias-squared)."
  },
  {
    "objectID": "Chapter6e.html#question-3",
    "href": "Chapter6e.html#question-3",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 3",
    "text": "Question 3\nThis question asks us to fit the model of lasso, which places a budget on the \\(l_1\\) norm \\(\\sum{|\\beta_j|}\\). \\[ \\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr) \\ \\ \\text{ subject to } \\ \\ \\sum_{j=1}^p|\\beta_j|\\le s \\]\n\n(a)\nAnswer is (iv). Training RSS will steadily decrease. As we increase \\(s\\) from 0, the \\(l_1\\) norm of the estimated coefficients will increase. Thus, the coefficients will increase in magnitude away from the Null model (at \\(s=0\\)). As a result, the training RSS will decrease steadily.\n\n\n(b)\nAnswer is (ii). As we increase \\(s\\) from 0, and move towards higher values of \\(s\\), we are increasingly moving from a Null model towards the least squares regression model. The intermediate models will be of the lasso form, with regularized coefficients to reduce variance. The test MSE will initially decrease (null model will have high Test MSE) but will eventually increase when \\(s \\to \\infty\\) (least squares model) due to higher variance in coefficient estimates. Thus, the test RSS will initially decrease, but eventually start increasing leading to a U-shape.\n\n\n(c)\nAnswer is (iii). Variance will steadily increase. As we increase \\(s\\) from 0, and move towards higher values of \\(s\\), we are increasing moving from a Null model (which has Zero Variance) towards the least squares regression model (Highest Variance). The intermediate lasso models will have lower variance than the least squares model because they shrink / regularize the coefficients, reducing their variance.\n\n\n(d)\nAnswer is (iv). (Squared) Bias will steadily decrease. As we increase \\(s\\) from 0, and move towards higher values of \\(s\\), we are increasing moving from a Null model (which has maximum bias, as it doesn’t use any predictor rather always predicts \\(\\overline y\\) for all \\(y_i\\)) towards the least squares regression model (Least Squares regression estiamtes of coefficients are unbiased estimators). The intermediate lasso models will have higher bias than the least squares model because they shrink / regularize the coefficients (and make some of them zero).\n\n\n(e)\nAnswer is (v). Irreducible error (\\(\\epsilon\\)) will stay constant. \\(\\epsilon\\) is a feature of the data set, and not of the fitted model. Thus, no matter which model we may fit, \\(\\epsilon\\) will stay constant all across the values of \\(s\\)."
  },
  {
    "objectID": "Chapter6e.html#question-4",
    "href": "Chapter6e.html#question-4",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 4",
    "text": "Question 4\nThis question estimates regression coefficients in a linear regression model by minimizing the following term. This is a ridge regression model with a tuning parameter \\(\\lambda\\). \\[ \\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr) - \\lambda\\sum_{j=1}^p\\beta_j^2 \\]\n\n(a)\nAnswer is (iii). The training RSS will steadily increase. The model with \\(\\lambda = 0\\) corresponds to the least squares model (which minimizes Residual Sum of Squares on the training data set), while the model with \\(\\lambda \\to \\infty\\) corresponds to the Null Model (which has the highest training RSS, equivalent to Total Sum of Squares TSS). The intermediate ridge regression models will shrink the \\(\\beta\\) coefficients towards zero, leading to some increase in bias (and, thus training RSS). As a result, the training RSS will steadily increase with rising \\(\\lambda\\).\n\n\n(b)\nAnswer is (ii). As we increase \\(\\lambda\\) from 0, and move towards higher values of \\(\\lambda\\), we are if effect, moving from a Least Squares Regression model towards the NULL model. The intermediate models will be of the ridge regression form, with regularized coefficients to reduce variance. The test MSE will initially decrease due to lower variance of ridge regression coefficients, but will eventually increase when \\(\\lambda \\to \\infty\\) (Null model). Thus, the test RSS will initially decrease, but eventually start increasing leading to a U-shape.\n\n\n(c)\nAnswer is (iv). Variance will steadily decrease. At \\(\\lambda = 0\\), the least squares model has the highest variance amongst all values of \\(\\lambda\\). At \\(\\lambda \\to \\infty\\), the Null model will have zero variance as it only predicts a single value \\(\\overline{y}\\) for all observations. The intermediate ridge regression models shrink the coefficients, and thus have lower variance as compared to the least squares regression model. Thus, variance will steadily decrease with rising \\(\\lambda\\).\n\n\n(d)\nAnswer is (iii). (Squared) Bias will steadily increase. At \\(\\lambda = 0\\), the least squares model is unbiased. Thus squared bias is zero. At \\(\\lambda \\to \\infty\\), the Null model will have the highest bias of all models, as it ignores all predictors, and predicts \\(\\overline{y}\\) for all observations. The intermediate ridge regression models shrink the coefficients, and thus have some added bias as compared to the least squares regression model. Thus, (squared) bias will steadily increase with rising \\(\\lambda\\).\n\n\n(e)\nAnswer is (v). Irreducible error (\\(\\epsilon\\)) will remain constant. \\(\\epsilon\\) is a feature of the data set, and not of the fitted model. Thus, no matter which model we may fit, \\(\\epsilon\\) will stay constant all across the values of \\(\\lambda\\)."
  },
  {
    "objectID": "Chapter6e.html#question-6",
    "href": "Chapter6e.html#question-6",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 6",
    "text": "Question 6\n\n(a)\nThis question asks us to consider the equation (6.12) with predictor \\(p=1\\). The equation is thus simplified as: \\[\n\\min_{\\beta} \\Big( \\ \\sum_{j=1}^{p} (y_j - \\beta_j)^2 \\ + \\ \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\ \\Big) \\qquad \\qquad (6.12)\n\\] \\[\n\\min_{\\beta} \\Big( \\ (y_j - \\beta_j)^2 \\ + \\ \\lambda \\beta_j^2 \\ \\Big)\n\\]\nThe equation 6.14 tells us that this expression should be minimized when: \\[\n\\hat{\\beta_j^R} = \\frac{y_j}{1 + \\lambda} \\qquad \\qquad (6.14)\n\\] The following R code demonstrates this for randomly chosen values of y = 10 and lambda = 3. The plot at the end of the code shows that the value of beta, computed using (6.14) corresponds to the lowest of the curve drawn using (6.12). Thus, this plot confirms that (6.12) is solved by (6.14).\n\n# Setting arbitrary values for y and lambda\ny &lt;- 10\nlambda &lt;- 3\n\n# Creating a vector of beta values to test and plot the expression\nbeta &lt;- seq(from = 0.01, to = 10, length = 100)\n\n# Calculating expression of equation 6.12\nexpression &lt;- (y - beta)^2 + lambda * (beta)^2\n\n# Computing expected beta value as per (6.14) to minimize expression\nbeta_exp &lt;- y / (1 + lambda)\n\n# Plotting the expression versus beta values\nplot(beta, expression,\n  xlab = \"Value of Beta\",\n  ylab = \"Value of expression in (6.12)\",\n  main = \"Ridge Regression\", type = \"l\"\n)\n# Adding a red point at expected beta value to see if it coincides with minimum of curve\npoints(beta_exp, (y - beta_exp)^2 + lambda * (beta_exp)^2,\n  col = \"red\", pch = 20, cex = 1.5\n)\n\n\n\n\n\n\n(b)\nThis question asks us to consider the equation (6.13) with predictor \\(p=1\\). The equation is simplified as: \\[\n\\min_{\\beta} \\Big( \\ \\sum_{j=1}^{p} (y_j - \\beta_j)^2 \\ + \\ \\lambda \\sum_{j=1}^{p} |\\beta_j| \\ \\Big) \\qquad \\qquad (6.13)\n\\] \\[\n\\min_{\\beta} \\Big( \\ (y_j - \\beta_j)^2 \\ + \\ \\lambda |\\beta_j| \\ \\Big)\n\\]\nThe equation 6.14 tells us three scenarios whereby this expression should be minimized: \\[\n\\hat{\\beta_j^L} = y_j - \\lambda/2 \\qquad \\text{if} \\ y_j &gt; \\lambda/2 \\qquad \\qquad (6.15)(i)\n\\] \\[\n\\hat{\\beta_j^L} = y_j + \\lambda/2 \\qquad \\text{if} \\ y_j &lt; -\\lambda/2 \\qquad \\qquad (6.15)(ii)\n\\] \\[\n\\hat{\\beta_j^L} = 0 \\qquad \\text{if} \\ |y_j| \\leq \\lambda/2 \\qquad \\qquad (6.15)(iii)\n\\] The following R code demonstrates three different scenarios. Their corresponding graphs are labeled as 6.15(i), 6.15(ii) and 6.15(iii) for randomly chosen values of y in each scenario, and lambda = 3. The plot at the end of the code shows that the values of beta, computed using (6.15) correspond to the lowest of the curves drawn using (6.13) no matter what the value of y and \\(\\lambda\\) are. Thus, the plots confirm that (6.13) is solved by (6.15).\n\n# Setting fixed lambda value, and a sequence of beta values\nlambda &lt;- 3\nbeta &lt;- seq(from = -10, to = 10, length = 100)\npar(mfrow = c(1, 3))\n\n# 6.15(i) [when y &gt; lambda/2]\ny &lt;- 8\nexpression &lt;- (y - beta)^2 + lambda * (abs(beta))\nbeta_exp &lt;- y - lambda / 2\nplot(beta, expression,\n  xlab = \"Value of Beta\",\n  ylab = \"Value of expression in (6.13)\",\n  main = \"6.15(i)\", type = \"l\"\n)\npoints(beta_exp, (y - beta_exp)^2 + lambda * (abs(beta_exp)),\n  col = \"red\", pch = 20, cex = 1.5\n)\n\n# 6.15(ii) [when y &lt; -lambda/2]\ny &lt;- -8\nexpression &lt;- (y - beta)^2 + lambda * (abs(beta))\nbeta_exp &lt;- y + lambda / 2\nplot(beta, expression,\n  xlab = \"Value of Beta\",\n  ylab = \"Value of expression in (6.13)\",\n  main = \"6.15(ii)\", type = \"l\"\n)\npoints(beta_exp, (y - beta_exp)^2 + lambda * (abs(beta_exp)),\n  col = \"red\", pch = 20, cex = 1.5\n)\n\n# 6.15(ii) [when y &lt; -lambda/2]\ny &lt;- -1\nexpression &lt;- (y - beta)^2 + lambda * (abs(beta))\nbeta_exp &lt;- 0\nplot(beta, expression,\n  xlab = \"Value of Beta\",\n  ylab = \"Value of expression in (6.13)\",\n  main = \"6.15(iii)\", type = \"l\"\n)\npoints(beta_exp, (y - beta_exp)^2 + lambda * (abs(beta_exp)),\n  col = \"red\", pch = 20, cex = 1.5\n)"
  },
  {
    "objectID": "Chapter6e.html#question-8",
    "href": "Chapter6e.html#question-8",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 8",
    "text": "Question 8\n\n(a)\nTo generate a simulated data set, we use the rnorm() function to create x and error term eps (\\(\\epsilon\\)) of length \\(n = 100\\).\n\nset.seed(3)\nx = rnorm(100)\neps = rnorm(100) \n\n\n\n(b)\nWe generate a a vector y based on the model described below, where \\(\\beta_0 = 6\\), \\(\\beta_1 = 5\\), \\(\\beta_2 = 4\\) and \\(\\beta_3 = 3\\). \\[ Y = \\beta_0 \\ + \\ \\beta_1X \\ + \\ \\beta_2X^2 \\ + \\ \\beta_3X^3 \\ + \\epsilon\\] \\[ Y = 6 \\ + \\ 5X \\ + \\ 4X^2 \\ + \\ 3X^3 \\ + \\epsilon\\]\n\ny = 6 + 5*(x) + 4*(x^2) + 3*(x^3) + eps\n\n\n\n(c)\nWe first create a data.frame sim to compile this simulated x and y. Then, we use regsubsets() function of the leaps library to perform best subset selection in order to choose the best model containing the predictors \\(X\\), \\(X^2\\), \\(X^3\\),…,\\(X^{10}\\). According to the \\(C_p\\), BIC statistic, the model with 3 predictors is picked up as the best model (This is actually the correct model). The adjusted-\\(R^2\\) erroneously picks up model \\(\\mathcal{M}_{10}\\) as the best model, but nearly all the models \\(\\mathcal{M}_{3}\\) to \\(\\mathcal{M}_{10}\\) are similar in adjusted-\\(R^2\\) value.\nThe coefficients of the best model picked are \\(\\hat{\\beta_0} = 5.96\\), \\(\\hat{\\beta_1} = 4.89\\), \\(\\hat{\\beta_2} = 4.09\\) and \\(\\hat{\\beta_3} = 3.02\\). These are very close to the actual values.\n\n# Creating the data.frame sim\nsim &lt;- data.frame(y = y, x = x)\n\n# Using regsubsets() to perform Best Subset Selection\nlibrary(leaps)\nregfit.best &lt;- regsubsets(\n  y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) +\n    I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10),\n  data = sim, nvmax = 10\n)\nregfit.best.sum &lt;- summary(regfit.best)\n\n# Plotting the Cp, BIC and Adjusted R-Squared of best models\n# with different number of predictors\npar(mfrow = c(1, 3), mar = c(3, 2, 2, 3))\nplot(regfit.best.sum$cp,\n  type = \"b\", xlab = \"Number of Predictors\",\n  main = \"Cp\"\n)\npoints(\n  x = which.min(regfit.best.sum$cp), y = min(regfit.best.sum$cp),\n  col = \"red\", cex = 2, pch = 20\n)\nplot(regfit.best.sum$bic,\n  type = \"b\", xlab = \"Number of Predictors\",\n  main = \"B.I.C\"\n)\npoints(\n  x = which.min(regfit.best.sum$bic), y = min(regfit.best.sum$bic),\n  col = \"red\", cex = 2, pch = 20\n)\nplot(regfit.best.sum$adjr2,\n  type = \"b\", xlab = \"Number of Predictors\",\n  main = \"Adjusted R-Squared\"\n)\npoints(\n  x = which.max(regfit.best.sum$adjr2), y = max(regfit.best.sum$adjr2),\n  col = \"red\", cex = 2, pch = 20\n)\n\n\n\n# Displaying the coefficients of the best model obtained\ncoef(regfit.best, 3)\n\n(Intercept)           x      I(x^2)      I(x^3) \n   5.957565    4.888312    4.087611    3.017670 \n\n\n\n\n(d)\nThe steps done in (c) above are repeated here with Forward step-wise selection and Backward step-wise selection. The function regsubsets() of leaps library does this job, when the argument method = is added. The results from forward step-wise selection are the same as that of best subset selection (it picks \\(\\mathcal{M}_{3}\\) as the best model containing \\(X\\), \\(X^2\\) and \\(X^3\\)). However, the backward step-wise selection picks up \\(\\mathcal{M}_{5}\\) as the best model (containing \\(X\\), \\(X^2\\), \\(X^5\\), \\(X^7\\) and \\(X^9\\)). This shows that the results from forward and backward selection are generally, but not always, in agreement with the Best Subset Selection.\n\n# Fitting the Forward Selection Model\nregfit.fwd &lt;- regsubsets(\n  y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) +\n    I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10),\n  data = sim, method = \"forward\", nvmax = 10\n)\nregfit.fwd.sum &lt;- summary(regfit.fwd)\n\n# Picking the best model and displaying its coefficients\nwhich.min(regfit.fwd.sum$bic)\n\n[1] 3\n\ncoef(regfit.fwd, which.min(regfit.fwd.sum$bic))\n\n(Intercept)           x      I(x^2)      I(x^3) \n   5.957565    4.888312    4.087611    3.017670 \n\n# Forward Stepwise Selection\n# Plotting the Cp, BIC and Adjusted R-Squared versus number of predictors\npar(mfrow = c(1, 3), mar = c(3, 2, 2, 3))\nplot(regfit.fwd.sum$cp, type = \"b\", xlab = \"Number of Predictors\", main = \"Cp\")\npoints(x = which.min(regfit.fwd.sum$cp), y = min(regfit.fwd.sum$cp), col = \"red\", cex = 2, pch = 20)\nplot(regfit.fwd.sum$bic, type = \"b\", xlab = \"Number of Predictors\", main = \"B.I.C\")\npoints(x = which.min(regfit.fwd.sum$bic), y = min(regfit.fwd.sum$bic), col = \"red\", cex = 2, pch = 20)\nplot(regfit.fwd.sum$adjr2, type = \"b\", xlab = \"Number of Predictors\", main = \"Adjusted R-Squared\")\npoints(x = which.max(regfit.fwd.sum$adjr2), y = max(regfit.fwd.sum$adjr2), col = \"red\", cex = 2, pch = 20)\n\n\n\n# Fitting the Backward Selection Model\nregfit.bwd &lt;- regsubsets(\n  y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) +\n    I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10),\n  data = sim, method = \"backward\", nvmax = 10\n)\nregfit.bwd.sum &lt;- summary(regfit.bwd)\n\n# Picking the best model and displaying its coefficients\nwhich.min(regfit.bwd.sum$bic)\n\n[1] 5\n\ncoef(regfit.bwd, which.min(regfit.bwd.sum$bic))\n\n(Intercept)           x      I(x^2)      I(x^5)      I(x^7)      I(x^9) \n  5.9461466   5.5757855   4.0891739   3.2378562  -1.1414537   0.1208527 \n\n# Backward Stepwise Selection\n# Plotting the Cp, BIC and Adjusted R-Squared versus number of predictors\npar(mfrow = c(1, 3), mar = c(3, 2, 2, 3))\nplot(regfit.bwd.sum$cp, type = \"b\", xlab = \"Number of Predictors\", main = \"Cp\")\npoints(x = which.min(regfit.bwd.sum$cp), y = min(regfit.bwd.sum$cp), col = \"red\", cex = 2, pch = 20)\nplot(regfit.bwd.sum$bic, type = \"b\", xlab = \"Number of Predictors\", main = \"B.I.C\")\npoints(x = which.min(regfit.bwd.sum$bic), y = min(regfit.bwd.sum$bic), col = \"red\", cex = 2, pch = 20)\nplot(regfit.bwd.sum$adjr2, type = \"b\", xlab = \"Number of Predictors\", main = \"Adjusted R-Squared\")\npoints(x = which.max(regfit.bwd.sum$adjr2), y = max(regfit.bwd.sum$adjr2), col = \"red\", cex = 2, pch = 20)\n\n\n\n\n\n\n(e)\nNow, we fit a lasso model to the sim data set using \\(X\\), \\(X^2\\), \\(X^3\\),…,\\(X^{10}\\) as predictors. For this, we use the glmnet() function with argument alpha = 1 from the glmnet library. We also use cross-validation to select the optimal value of \\(\\lambda\\) using cv.glmnet() function. Further, we plot the cross-validation error as a function of \\(\\lambda\\). The resulting coefficients at the optimal \\(\\lambda\\) value are found using the predict() function. The results show that the lasso correctly finds out the relevant predictors \\(X\\), \\(X^2\\) and \\(X^3\\). The coefficients found using lasso at the optimal \\(\\lambda\\) are quite close to the actual values i.e. \\(\\hat{\\beta_0} = 6.01\\), \\(\\hat{\\beta_1} = 4.9\\), \\(\\hat{\\beta_2} = 4.01\\) and \\(\\hat{\\beta_3} = 2.98\\).\n\n# Loading library and creating X matrix and Y vector\nlibrary(glmnet)\nX = model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + \n                     I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), \n                     data = sim)[,-1]\nY = sim$y\n\n# Fitting the Lasso model on training set\nlasso.mod = glmnet(X, Y, alpha = 1)\n\n# Using Cross-Validation Lasso, displaying optimal value of lambda\ncv.lasso = cv.glmnet(X, Y, alpha = 1)\nbestlam = cv.lasso$lambda.min\nbestlam\n\n[1] 0.03346741\n\n# Plotting Cross-Validation Error as a function of lambda\npar(mfrow = c(1, 1))\nplot(cv.lasso)\n\n\n\n# Displaying resulting coefficients at optimal lambda value\ncoefs = predict(lasso.mod, type = \"coefficients\", s = bestlam)[1:10,]\ncoefs\n\n(Intercept)           x      I(x^2)      I(x^3)      I(x^4)      I(x^5) \n   6.000538    4.896203    4.025918    2.986646    0.000000    0.000000 \n     I(x^6)      I(x^7)      I(x^8)      I(x^9) \n   0.000000    0.000000    0.000000    0.000000 \n\n# Displaying only non-zero coefficients\nround(coefs[coefs != 0], 2)\n\n(Intercept)           x      I(x^2)      I(x^3) \n       6.00        4.90        4.03        2.99 \n\n\n\n\n(f)\nWe now prepare the new model, where \\(\\beta_0 = 2\\) and \\(\\beta_7 = 3\\), as follows:\n\\[ Y = \\beta_0 \\ + \\ \\beta_7X^7 \\ + \\epsilon\\] \\[ Y = 2 \\ + \\ 3X^7 \\ + \\epsilon\\]\nThen, we perform the best subset selection and lasso.\nFirst, on performing the best subset selection, we find that the BIC and \\(C_p\\) determine the best model to be one with 3 predictors: \\(X^4\\), \\(X^6\\) and \\(X^7\\). This predicts \\(\\hat{\\beta_0} = 1.95\\) and \\(\\hat{\\beta_7} = 2.99\\), which are quite close to true values. But, the best subset selection ends up including two irrelevant predictors in the best model.\n\n# Generate data set according to the new model, using previous x and eps\ny1 &lt;- 2 + 3 * (x^7) + eps\n\n# Create new data.frame sim1\nsim1 &lt;- data.frame(y1 = y1, x = x)\n\n# Perform Best Subset Selection on new data\nlibrary(leaps)\nregfit.best1 &lt;- regsubsets(\n  y1 ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) +\n    I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10),\n  data = sim1, nvmax = 10\n)\nregfit.best.sum1 &lt;- summary(regfit.best)\n\n# Displaying the coefficients for best model picked by Cp and BIC\nround(coef(regfit.best1, which.min(regfit.best.sum1$bic)), 2)\n\n(Intercept)      I(x^4)      I(x^6)      I(x^7) \n       1.95        0.15       -0.04        2.99 \n\n# Plotting the Cp, BIC and Adjusted R-Squared for various models in Best Subset Selection\npar(mfrow = c(1, 3), \n    mar = c(3, 2, 2, 3))\nplot(regfit.best.sum1$cp, type = \"b\", xlab = \"Number of Predictors\", main = \"Cp\")\npoints(x = which.min(regfit.best.sum1$cp), y = min(regfit.best.sum1$cp), col = \"red\", cex = 2, pch = 20)\nplot(regfit.best.sum1$bic, type = \"b\", xlab = \"Number of Predictors\", main = \"B.I.C\")\npoints(x = which.min(regfit.best.sum1$bic), y = min(regfit.best.sum1$bic), col = \"red\", cex = 2, pch = 20)\nplot(regfit.best.sum1$adjr2, type = \"b\", xlab = \"Number of Predictors\", main = \"Adjusted R-Squared\")\npoints(x = which.max(regfit.best.sum1$adjr2), y = max(regfit.best.sum1$adjr2), col = \"red\", cex = 2, pch = 20)\n\n\n\n\nNow, we perform lasso on the new data set. The code is shown below. The results indicate that the best model selected by cv.glmnet() function correctly identifies \\(\\beta_7\\) as the only significant predictor. The coefficient values of the selected lasso model are \\(\\hat{\\beta_0} = 1.76\\) and \\(\\hat{\\beta_7} = 2.91\\), which are slightly inaccurate with respect to the intercept. But, there are no irrelevant predictors included in the model, which provides the advantage of ease of interpretation.\nThese results indicate that lasso performs better than best subset selection when only a few of the predictor variables are actually related to the response. Lasso does a much better job in producing a sparse model which aids interpretability. However, if prediction accuracy is the sole purpose, or if many predictors are associated with the response, then Best Subset Selection can often outperform lasso.\n\nlibrary(glmnet)\nX1 = model.matrix(y1 ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + \n                     I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), \n                     data = sim1)[,-1]\nY1 = sim1$y1\n\n# Fitting the Lasso model on training set\nlasso.mod1 = glmnet(X1, Y1, alpha = 1)\n\n# Using Cross-Validation Lasso, displaying optimal value of lambda\ncv.lasso1 = cv.glmnet(X1, Y1, alpha = 1)\nbestlam1 = cv.lasso$lambda.min\nbestlam1\n\n[1] 0.03346741\n\n# Displaying resulting coefficients at optimal lambda value\ncoefs1 = predict(lasso.mod1, type = \"coefficients\", s = bestlam1)[1:10,]\ncoefs1\n\n(Intercept)           x      I(x^2)      I(x^3)      I(x^4)      I(x^5) \n   1.762545    0.000000    0.000000    0.000000    0.000000    0.000000 \n     I(x^6)      I(x^7)      I(x^8)      I(x^9) \n   0.000000    2.910907    0.000000    0.000000 \n\n# Displaying only non-zero coefficients\nround(coefs1[coefs1 != 0], 2)\n\n(Intercept)      I(x^7) \n       1.76        2.91"
  },
  {
    "objectID": "Chapter6e.html#question-9",
    "href": "Chapter6e.html#question-9",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 9",
    "text": "Question 9\n\n(a)\nWe first load the College data set and split it into training and test data sets. To split the data set, we create a boolean vector train using random sampling which creates nearly two equal subsets. We then use train to create the two subsets: College.Train and College.Test.\n\nlibrary(ISLR)\ndata(\"College\")\n\n# Check for and remove missing values\nCollege = na.omit(College)\n\n# Creating a boolean vector train to split the data set\nset.seed(3)\ntrain = sample(c(TRUE, FALSE), size = nrow(College), replace = TRUE)\n\n# Creating two split data sets\nCollege.Train = College[train, ]\nCollege.Test = College[!train, ]\n\n\n\n(b)\nWe now fit a linear model using least squares on the training set. The Test Mean Squared Error for this model is 1,450,888.\n\n# Fitting linear model\nlin.fit = lm(Apps ~ ., data = College.Train)\n\n# Getting predicted values in the Test Set\npred.lin.fit = predict(lin.fit, newdata = College.Test)\n\n# Calculating Test MSE\nmse.lm = mean((College.Test$Apps - pred.lin.fit)^2)\nmse.lm\n\n[1] 1450888\n\n\n\n\n(c)\nNow, we fit a ridge regression model on the training set, with a \\(\\lambda\\) chosen by cross-validation. The Test Mean Squared Error is calculated to be 1,488,034. This is nearly equal to, or slightly higher than the test error obtained in least squares regression.\n\nlibrary(glmnet)\n\n# Creating Matrices for use in glmnet function, creating lambda values grid\nTrainX = model.matrix(Apps ~ ., data = College.Train)[,-1]\nTrainY = College.Train$Apps\nTestX = model.matrix(Apps ~ ., data = College.Test)[,-1]\ngrid = 10^seq(10, -2, length = 100)\n\n# Fitting Ridge Regression model and Cross Validation Model\nridge.mod = glmnet(TrainX, TrainY, alpha = 0, lambda = grid, thresh = 1e-12)\nridge.cv.mod = cv.glmnet(TrainX, TrainY, alpha = 0, lambda = grid, thresh = 1e-12)\n\n# Selecting the best lambda value\nlam.r = ridge.cv.mod$lambda.min\n\n# Predicting values for the Test Set\npred.ridge = predict(ridge.mod, newx = TestX, s = lam.r)\n\n# Calculating the Test MSE\nmse.rr = mean((College.Test$Apps - pred.ridge)^2)\nmse.rr\n\n[1] 1450919\n\n\n\n\n(d)\nNow, we fit a lasso model on the training set, with \\(\\lambda\\) chosen by cross validation. The Test MSE using lasso is 1,450,907. This is again nearly the same as least squares regression. There are 18 non-zero coefficients (1 intercept and 17 predictors). Thus, the lasso does not weed out any of the predictors. This is why the test MSE for lasso and least squares regression is nearly the same.\n\n# Fitting Lasso model and Cross Validation Lasso Model\nlasso.mod = glmnet(TrainX, TrainY, alpha = 1, lambda = grid, thresh = 1e-12)\nlasso.cv.mod = cv.glmnet(TrainX, TrainY, alpha = 1, lambda = grid, thresh = 1e-12)\n\n# Selecting the best lambda value\nlam.l = lasso.cv.mod$lambda.min\n\n# Predicting values for the Test Set\npred.lasso = predict(lasso.mod, newx = TestX, s = lam.l)\ncoefs.lasso = predict(lasso.mod, s = lam.l, type = \"coefficients\")[1:18,]\n\n# Calculating the Test MSE\nmse.l = mean((College.Test$Apps - pred.lasso)^2)\nmse.l\n\n[1] 1450907\n\n# Displaying the non-zero coefficient estimates selected by lasso at best lambda value\nlength(coefs.lasso[coefs.lasso != 0])\n\n[1] 18\n\n\n\n\n(e)\nNow, we fit a Principal Components Regression model on the training set, with \\(M\\) chosen by cross validation. The summary function on the pcr() object and the plot of cross-validation MSE tells us that the chosen \\(M=17\\). Thus, no dimension reduction occurs in the training data set using PCR. The test MSE is calculated to be 1,450,888. This is the same as least squares regression. This is expected because no reduction in dimensionality occurs by PCR in this data set.\n\n# Loading library and setting seed\nlibrary(pls)\nset.seed(3)\n\n# Fitting a pcr regression on training set\npcr.fit = pcr(Apps ~ ., data = College.Train, scale = TRUE, validation = \"CV\")\n\npar(mfrow = c(1, 1))\n# Plotting the Cross-Validation MSE and finding the M with minimum MSE\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\nwhich.min(pcr.fit$validation$adj)\n## [1] 17\n\n# Calculating predicted values and test MSE\npred.pcr = predict(pcr.fit, newdata = College.Test, ncomp = 17)\nmse.pcr = mean((College.Test$Apps - pred.pcr)^2)\nmse.pcr\n## [1] 1450888\n\n\n\n(f)\nNow, we fit a Partial Least Squares model on the training set, with \\(M\\) chosen by cross validation. The summary function on the plsr() object and the plot of cross-validation MSE tells us that the chosen \\(M=16\\). Thus, negligible dimension reduction occurs in the training data set using PLS. The test MSE is calculated to be 1,450,875. This is the same as least squares regression. This is expected because negligible reduction in dimensionality occurs by using Partial Least Squares in this data set.\n\n# Fitting a PLS regression on training set\npls.fit = plsr(Apps ~ ., data = College.Train, scale = TRUE, validation = \"CV\")\n\n# Plotting the Cross-Validation MSE and finding M with minimum MSE\nvalidationplot(pls.fit, val.type = \"MSEP\")\n\n\n\nwhich.min(pls.fit$validation$adj)\n\n[1] 16\n\n# Calculating predicted values and test MSE\npred.pls = predict(pls.fit, newdata = College.Test, ncomp = 16)\nmse.pls = mean((College.Test$Apps - pred.pls)^2)\nmse.pls\n\n[1] 1450875\n\n\n\n\n(g)\nThe results from all five methods: least squares regression, ridge regression, lasso, principal components regression and partial least squares regression yield similar test MSE. There is very little difference among test errors from these five approaches. Rather, all the models predict Apps using other predictors in the College data set with high accuracy. Using the code below, we depict a table with MSE of the “Null Model” and the MSE from these five methods. All these five methods explain around 92 % of the variability in Apps.\n\n# Calculating MSE of the Null Model\nmse.null &lt;- mean((College.Test$Apps - mean(College.Test$Apps))^2)\n\nTestMSE &lt;- c(mse.null, mse.lm, mse.rr, mse.l, mse.pcr, mse.pls)\nTestRSquared &lt;- round(1 - (TestMSE / mse.null), 4)\ncomparison &lt;- data.frame(\n  Models = c(\n    \"Null\", \"Least Squares\", \"Ridge Regression\",\n    \"Lasso\", \"Principal Components Regression\",\n    \"Partial Least Sqaures Regression\"\n  ),\n  TestMSE = TestMSE,\n  TestRSquared = TestRSquared\n)\ncomparison %&gt;%\n  kbl() %&gt;%\n  kable_classic_2()\n\n\n\n\nModels\nTestMSE\nTestRSquared\n\n\n\n\nNull\n17818435\n0.0000\n\n\nLeast Squares\n1450888\n0.9186\n\n\nRidge Regression\n1450919\n0.9186\n\n\nLasso\n1450907\n0.9186\n\n\nPrincipal Components Regression\n1450888\n0.9186\n\n\nPartial Least Sqaures Regression\n1450875\n0.9186"
  },
  {
    "objectID": "Chapter6e.html#question-10",
    "href": "Chapter6e.html#question-10",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 10",
    "text": "Question 10\n\n(a)\nFirst, we create a simulated data set with \\(p=20\\) features and \\(n=1000\\) observations. First, we create a matrix X with 20 random variables X1, X2, ... ,X20 over 1000 observations. We also create an \\(\\epsilon\\) (error term) called eps. Now, we make a simulated Y which is dependent on even numbered columns / variables of X. For ease, we make all coefficients \\(\\beta_j = 3\\). Finally, we combine X and Y into a simulated data set sim.\n\n# Creating predictors and error term\nX &lt;- rnorm(n = 20 * 1000)\nX &lt;- matrix(X,\n  nrow = 1000, ncol = 20,\n  dimnames = list(NULL, paste(\"X\", c(1:20), sep = \"\"))\n)\neps &lt;- rnorm(1000)\n\n# Creating a simulated response Y that is only related to even numbered predictors\n# Keeping all coefficients = 3 (for ease of remembering and comparing later)\n\npredictors &lt;- seq(from = 2, to = 20, by = 2) # vector of actual predictor columns\ncoeffs &lt;- rep(3, 10) # vector of coefficients, all equal to 3\n\nY &lt;- X[, predictors] %*% coeffs + eps\n\n# Creating the data.frame 'sim'\nSim &lt;- data.frame(cbind(Y, X))\ncolnames(Sim)[1] &lt;- \"Y\"\n\n\n\n(b)\nWe now split the data set sim into a training set of 100 observations Train.Sim, and a testing set of 900 observation Test.Sim.\n\n# Create boolean vector train to split into training data (100) and test data (900)\nset.seed(3)\ntrain = sample(x = 1:nrow(Sim), size = 100, replace = FALSE)\n\n# Create Training and Test Subsets of data.frame\nTrain.Sim = Sim[train, ]\nTest.Sim = Sim[-train, ]\n\n\n\n(c)\nWe now perform the Best Subset Selection on the training set. For this, initially we load the library leaps which contains the regsubsets() function to perform best subset selection. Then, we create training matrix TrainX (we could bypass this redundancy using original X variable, but proceeding this way using the split data frame keeps things simple). We also create a TrainY vector to use in regsubsets(). The rss component of the summary of regsubsets() created object contains Residual Sum of Squares. We divide these by n (number of training observations) to obtain Training MSE. The plot of Training MSE, as expected, shows that minimum training MSE is achieved with the model having maximum predictors (\\(p=20\\)).\n\n# Load libraries, and create matrices and vector for regsubsets()\nlibrary(leaps)\nTrainX &lt;- model.matrix(Y ~ ., data = Train.Sim)\nTrainY &lt;- Train.Sim$Y\n\n# Running a Best Subset Selection on Training Data Set\nregfit.best &lt;- regsubsets(TrainX, TrainY, nvmax = 20)\n\nReordering variables and trying again:\n\n# Calculating Training MSE\nTrain.MSE &lt;- rep(NA, 20)\nfor (i in 1:20) {\n  coefs &lt;- coef(regfit.best, id = i)\n  Pred.Y.Train &lt;- TrainX[, names(coefs)] %*% coefs\n  Train.MSE[i] &lt;- mean((TrainY - Pred.Y.Train)^2)\n}\nplot(Train.MSE,\n  type = \"b\", col = \"blue\",\n  xlab = \"Number of Predictors in model\",\n  ylab = \"Training MSE\"\n)\npoints(\n  x = which.min(Train.MSE),\n  y = min(Train.MSE),\n  col = \"blue\", cex = 2, pch = 20\n)\nlegend(x = 5, y = 50, \"Training MSE\", col = \"blue\", lty = 1)\n\n\n\n\n\n\n(d)\nNow, we compute the Test MSE using the best models (for each number of predictors selected by the Best Subset Selection Method). The plot of Test MSE (overlaid on Training MSE) shows us that the Test MSE is always higher than the Training MSE.\n\n# Creating Testing X matrix and Y vector to use in calculating Test MSE\nTestX &lt;- model.matrix(Y ~ ., data = Test.Sim)\nTestY &lt;- Test.Sim$Y\n\n# Calculating Test MSE for each of 20 models selected by best subset selection\nTest.MSE &lt;- rep(NA, 20)\nfor (i in 1:20) {\n  coefs &lt;- coef(regfit.best, id = i)\n  Pred.Y &lt;- TestX[, names(coefs)] %*% coefs\n  Test.MSE[i] &lt;- mean((Pred.Y - TestY)^2)\n}\n\n# Plotting the Test MSE\nplot(Test.MSE,\n  type = \"b\", xlab = \"Number of Predictors in model\",\n  ylab = \"Mean Squared Error\", col = \"red\"\n)\npoints(\n  x = which.min(Test.MSE), y = min(Test.MSE),\n  col = \"red\", cex = 2, pch = 20\n)\npoints(Train.MSE,\n  type = \"b\", col = \"blue\",\n  xlab = \"Number of Predictors in model\", ylab = \"Training MSE\"\n)\npoints(\n  x = which.min(Train.MSE), y = min(Train.MSE),\n  col = \"blue\", cex = 2, pch = 20\n)\nlegend(x = 14, y = 80, c(\"Training MSE\", \"Test MSE\"), col = c(\"blue\", \"red\"), lty = c(1, 1))\n\n\n\n\n\n\n(e)\nThe 10-variable model has the lowest Test MSE. Thus, best subset selection method has correctly picked the correct number of predictors in the best model. Further, it has even correctly identified the 10 relevant predictors as the even numbered predictors. The coefficient values are also very close to the actual value of 3. The code below displays the results.\n\n# Finding Model with lowest Test MSE\nwhich.min(Test.MSE)\n\n[1] 20\n\n# Displaying the predictors identified by the Best Test MSE model\nnames(coef(regfit.best, id = which.min(Test.MSE)))\n\n [1] \"(Intercept)\" \"X2\"          \"X3\"          \"X4\"          \"X5\"         \n [6] \"X6\"          \"X7\"          \"X8\"          \"X9\"          \"X10\"        \n[11] \"X11\"         \"X12\"         \"X13\"         \"X14\"         \"X15\"        \n[16] \"X16\"         \"X17\"         \"X18\"         \"X19\"         \"X20\"        \n[21] \"(Intercept)\"\n\n\n\n\n(f)\nThe model at which the test set MSE is minimized is very close to the true model used to generate the data. The coefficient values displayed below are very close to the actual coefficient value of 3 for each of the predictors (ranging from 2.84 to 3.17). Though, the model does add an unnecessary intercept of 0.17. Overall, the best subset selection does an excellent job.\n\nround(coef(regfit.best, id = which.min(Test.MSE)),2)\n\n(Intercept)          X2          X3          X4          X5          X6 \n      -0.15        3.00        0.03        2.99        0.07        3.00 \n         X7          X8          X9         X10         X11         X12 \n       0.04        3.03       -0.07        2.74       -0.15        2.97 \n        X13         X14         X15         X16         X17         X18 \n      -0.08        2.85       -0.03        3.03        0.02        2.85 \n        X19         X20 (Intercept) \n       0.13        3.09        0.00 \n\n\n\n\n(g)\nWe now create a plot of the following quantity \\(\\sqrt{\\sum_{j=1}^{p} (\\beta_j - \\hat{\\beta_j^r})^2}\\) versus the number of predictors in each of the models selected by Best Subset Selection. First, we create a vector of true coefficients created by us in the part (a) of the question (including the intercept). Then, we calculate \\(\\sqrt{\\sum_{j=1}^{p} (\\beta_j - \\hat{\\beta_j^r})^2}\\) for each of the models using a for loop. The resulting plot is shown below. It is clear that the plot reaches its minimum at exactly the same point as in (d), i.e. difference in coefficients is least when the test MSE is minimum.\n\n# Re-create a vector of true coefficients we set in (a)\nTrueCoefs &lt;- rep(0, 21)\nTrueCoefs[predictors] &lt;- 3\nnames(TrueCoefs) &lt;- c(paste(\"X\", c(1:20), sep = \"\"), \"(Intercept)\")\n\n# Create an empty data-frame to store the results\nDiffCoefs &lt;- rep(NA, 20)\n\n# Run the loop for calculating Square-Root of difference in coefficients for\n# best subset models selected at each number of predictors\nfor (i in 1:20) {\n  vals &lt;- coef(regfit.best, id = i)\n  preds &lt;- names(coef(regfit.best, id = i))\n  DiffCoefs[i] &lt;- sqrt(sum((TrueCoefs[preds] - vals)^2))\n}\nplot(DiffCoefs,\n  type = \"b\", col = \"orange\",\n  xlab = \"Number of predictors in the model\",\n  ylab = \"Sqrt(Sum of (betas - beta_hats)^2)\"\n)\npoints(\n  x = which.min(DiffCoefs), y = min(DiffCoefs),\n  col = \"orange\", cex = 2, pch = 20\n)"
  },
  {
    "objectID": "Chapter6e.html#question-11",
    "href": "Chapter6e.html#question-11",
    "title": "Chapter 6 (Exercises)",
    "section": "Question 11",
    "text": "Question 11\n\n(a)\nFor part (a) of this question, we fit the various models and approaches within linear methods to predict per capital crime rate crim in the Boston data set. Right from the part (a), we will split the data into equally sized training and test sets. However, in question (b) when propose the best model, we will fit the model we select at the end of this part (a) to the entire data set to get the best coefficient estimates.\n\n# Loading libraries and examining data set\nlibrary(MASS)\ndata(\"Boston\")\nBoston &lt;- na.omit(Boston)\ndim(Boston)\n\n[1] 506  14\n\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# Create a training and test set for `Boston` data set\nset.seed(3)\ntrain &lt;- sample(c(TRUE, FALSE), size = nrow(Boston), replace = TRUE)\ntable(train)\n\ntrain\nFALSE  TRUE \n  252   254 \n\n# Creating Test & Train X matrices and Y vectors to use in fitting models\nTrainX &lt;- model.matrix(crim ~ ., data = Boston[train, ])\nTrainY &lt;- Boston$crim[train]\nTestX &lt;- model.matrix(crim ~ ., data = Boston[!train, ])\nTestY &lt;- Boston$crim[!train]\n\nInitially, we start with Best Subset Selection. The best subset selection model statistics \\(C_p\\) and B.I.C. suggest using the model with 3 predictors rad, black and lstat. However, the validation set approach and the 10-fold Cross Validation approach pick up the best model as the one with 9 predictors: zn, indus, nox, dis, rad, ptratio, black, lstat and medv. The lowest Test MSE achieved is 39.33 with 10-fold CV in 9-variable model and 37.06 in Validation-Set approach.\n\nlibrary(leaps)\nfit.bss &lt;- regsubsets(crim ~ ., data = Boston[train, ], nvmax = 13)\n\n# Examining the best model based on fitted model statistics\npar(mfrow = c(1, 3), mar = c(5, 2, 3, 1))\nplot(summary(fit.bss)$bic,\n  type = \"b\", col = \"blue\", xlab = \"Model Predictors\",\n  ylab = \"\", main = \"B.I.C\"\n)\npoints(\n  x = which.min(summary(fit.bss)$bic), y = min(summary(fit.bss)$bic),\n  col = \"blue\", pch = 20, cex = 2\n)\nplot(summary(fit.bss)$cp,\n  type = \"b\", col = \"red\", xlab = \"Model Predictors\",\n  ylab = \"\", main = \"Cp\"\n)\npoints(\n  x = which.min(summary(fit.bss)$cp), y = min(summary(fit.bss)$cp),\n  col = \"red\", pch = 20, cex = 2\n)\nplot(summary(fit.bss)$adjr2,\n  type = \"b\", col = \"purple\", xlab = \"Model Predictors\",\n  ylab = \"\", main = \"Adjusted R-Squared\"\n)\npoints(\n  x = which.max(summary(fit.bss)$adjr2), y = max(summary(fit.bss)$adjr2),\n  col = \"purple\", pch = 20, cex = 2\n)\n\n\n\n# Displaying best model based on fitted model statistics\nround(coef(fit.bss, id = 3), 2)\n\n(Intercept)         rad       black       lstat \n       1.26        0.46       -0.02        0.27 \n\n# Computing Test MSE using a self-created function\npredict.regsubsets &lt;- function(object, newdata, id, ...) {\n  formula &lt;- as.formula(object$call[[2]]) # Extracting formula from regsubsets object\n  cofs &lt;- coef(object, id) # Extracting coefficients with their names\n  testmat &lt;- model.matrix(formula, data = newdata) # Create test X matrix\n  testmat[, names(cofs)] %*% cofs # Predicted values\n}\n\n# Finding best model (lowest Test MSE) as per Validation Set Approach\nModelsMSE &lt;- rep(NA, 13)\nfor (i in 1:13) {\n  pred.bss &lt;- predict.regsubsets(fit.bss, newdata = Boston[!train, ], id = i)\n  ModelsMSE[i] &lt;- mean((TestY - pred.bss)^2)\n}\n\n# Finding the best model (lowest Test MSE) by 10-fold Cross-Validation\nset.seed(3)\nk &lt;- 10\nfolds &lt;- sample(1:k, size = nrow(Boston), replace = TRUE)\nCVerrors &lt;- matrix(NA,\n  nrow = k, ncol = 13,\n  dimnames = list(NULL, paste(1:13))\n)\nfor (j in 1:k) {\n  fit &lt;- regsubsets(crim ~ ., data = Boston[folds != j, ], nvmax = 13)\n  for (i in 1:13) {\n    pred &lt;- predict.regsubsets(fit, newdata = Boston[folds == j, ], id = i)\n    CVerrors[j, i] &lt;- mean((Boston$crim[folds == j] - pred)^2)\n  }\n}\nMSEs.CV &lt;- apply(CVerrors, MARGIN = 2, FUN = mean)\n\n# Plotting the Test MSEs from Validation Set and 10-fold CV approaches\npar(mfrow = c(1, 2))\nplot(ModelsMSE,\n  type = \"b\", xlab = \"Number of Predictors in the Model\",\n  ylab = \"\", main = \"Validation Set Test MSE\", col = \"orange\"\n)\npoints(\n  x = which.min(ModelsMSE), y = min(ModelsMSE), col = \"orange\",\n  cex = 2, pch = 20\n)\nplot(MSEs.CV,\n  type = \"b\", xlab = \"Number of Predictors in the Model\",\n  ylab = \"\", main = \"10-fold CV Test MSE\", col = \"brown\"\n)\npoints(\n  x = which.min(MSEs.CV), y = min(MSEs.CV), col = \"brown\",\n  cex = 2, pch = 20\n)\n\n\n\n# Displaying the best model selected finally (Best Subset Selection)\nround(coef(regsubsets(crim ~ ., data = Boston, nvmax = 13), id = 9), 2)\n\n(Intercept)          zn       indus         nox         dis         rad \n      19.12        0.04       -0.10      -10.47       -1.00        0.54 \n    ptratio       black       lstat        medv \n      -0.27       -0.01        0.12       -0.18 \n\n\nNow, we use the lasso on the same data set using glmnet() function of the glmnet library. We also compute the ideal \\(\\lambda\\) value using cv.glmnet(). Then, using the ideal \\(\\lambda\\) tuning parameter, we compute the Test MSE. The results indicate that the best lasso model has a Test MSE of 37.34. It uses all 13 predictors.\n\nlibrary(glmnet)\n\n# Fitting lasso model \nlasso.mod = glmnet(TrainX, TrainY, alpha = 1)\n\n# Fitting lasso with cross-validation for lambda value\nlasso.cv = cv.glmnet(TrainX, TrainY, alpha = 1)\nbestlam = lasso.cv$lambda.min\n\n# Computing Test MSE for best lasso model\nlasso.pred = predict(lasso.mod, newx = TestX, s = bestlam)\nMSE.lasso = mean((TestY - lasso.pred)^2)\nMSE.lasso\n\n[1] 37.35164\n\nlasso.coefs = predict(lasso.mod, s = bestlam, type = \"coefficients\")[1:15,]\nnames(lasso.coefs[lasso.coefs != 0])\n\n [1] \"(Intercept)\" \"zn\"          \"indus\"       \"chas\"        \"nox\"        \n [6] \"rm\"          \"dis\"         \"rad\"         \"ptratio\"     \"black\"      \n[11] \"lstat\"       \"medv\"       \n\n\nNow, we use the ridge regression on the same data set using glmnet() function with alpha = 0 from the glmnet library. We also compute the ideal \\(\\lambda\\) value using cv.glmnet(). Then, using the ideal \\(\\lambda\\) tuning parameter, we compute the Test MSE. The results indicate that the best ridge regression model has a Test MSE of 37.87.\n\n# Fitting ridge regression model\nrr.mod &lt;- glmnet(TrainX, TrainY, alpha = 0)\n\n# Fitting ridge regression with cross-validation for lambda value\nrr.cv &lt;- cv.glmnet(TrainX, TrainY, alpha = 0)\nbestlam &lt;- rr.cv$lambda.min\n\n# Computing Test MSE for best ridge regression model\nrr.pred &lt;- predict(rr.mod, newx = TestX, s = bestlam)\nMSE.rr &lt;- mean((TestY - rr.pred)^2)\nMSE.rr\n\n[1] 37.90711\n\nrr.coefs &lt;- predict(rr.mod, s = bestlam, type = \"coefficients\")[1:15, ]\nround(rr.coefs, 3)\n\n(Intercept) (Intercept)          zn       indus        chas         nox \n     12.424       0.000       0.031      -0.096      -0.642      -3.226 \n         rm         age         dis         rad         tax     ptratio \n     -0.535       0.006      -0.567       0.349       0.005      -0.074 \n      black       lstat        medv \n     -0.014       0.180      -0.087 \n\n# Plotting Diagnostics of Lasso and Ridge Regression\npar(mfrow = c(2, 2), mar = c(2.5, 2, 2, 2))\nplot(lasso.cv)\ntext(x = -4, y = 100, \"Lasso CV\")\nplot(rr.cv)\ntext(x = 2, y = 100, \"Ridge Regression CV\")\nplot(lasso.mod)\ntext(x = 4, y = -8, \"Lasso Coefficients\")\nplot(rr.mod)\ntext(x = 4, y = -4, \"Lasso Coefficients\")\n\n\n\n\nNow, we use Principal Components Regression and Partial Least Squares on this data set using pcr() and plsr() functions from pls library. The cross validation Test MSEs are calculated automatically. The least Test MSE occur at \\(M=13\\). Thus, no dimension reduction occurs. The lowest Test MSEs are around 42 with both PCR and PLS. These are higher than other methods.\n\nlibrary(pls)\n\n# Fitting the PCR and PLS models\nset.seed(3)\nfit.pcr &lt;- pcr(crim ~ ., data = Boston, scale = TRUE, validation = \"CV\")\nfit.pls &lt;- plsr(crim ~ ., data = Boston, scale = TRUE, validation = \"CV\")\n\n# Plotting the Test MSE (Cross Validation) from PCR and PLS regression\npar(mfrow = c(1, 2))\nvalidationplot(fit.pcr, val.type = \"MSEP\", main = \"PCR\", ylab = \"MSE\")\nvalidationplot(fit.pls, val.type = \"MSEP\", main = \"PLS\", ylab = \"MSE\")\n\n\n\n\n\n\n(b)\nBased on these results in (a), the model with the lowest Test MSE happens to be Best Subset Selection model with 9 predictors. But, since validation set Test MSE is highly variable, we use the Cross-Validation Test MSE. The model with lowest Cross Validation Test MSE turns out to be the Lasso model using all 13 predictors.\n\n\n(c)\nYes, the model chosen in part (b) includes all the 13 predictors, i.e. all features of the data set are used for prediction. This is in agreement with the findings from ridge regression, principal components regression and partial least squares. None of these models favor dimension reduction."
  },
  {
    "objectID": "Chapter6l.html",
    "href": "Chapter6l.html",
    "title": "Chapter 6 (Lab)",
    "section": "",
    "text": "Load the libraries ISLR and leaps to perform Best Subset Selection on Hitters data.\n\nlibrary(ISLR)\nlibrary(leaps)\nlibrary(kableExtra)\ndata(Hitters)\n\n# Examine the data set\ndim(Hitters)\n\n[1] 322  20\n\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\n# Find and remove missing data\nsum(is.na(Hitters))\n\n[1] 59\n\nHitters &lt;- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\n# Create an Object with Best Subset Selection performed on Hitters Data set\nregfit.trial &lt;- regsubsets(Salary ~ ., data = Hitters)\n# Examine contents of summary of the regsubsets output\nnames(regfit.trial)\n\n [1] \"np\"        \"nrbar\"     \"d\"         \"rbar\"      \"thetab\"    \"first\"    \n [7] \"last\"      \"vorder\"    \"tol\"       \"rss\"       \"bound\"     \"nvmax\"    \n[13] \"ress\"      \"ir\"        \"nbest\"     \"lopt\"      \"il\"        \"ier\"      \n[19] \"xnames\"    \"method\"    \"force.in\"  \"force.out\" \"sserr\"     \"intercept\"\n[25] \"lindep\"    \"nullrss\"   \"nn\"        \"call\"     \n\nnames(summary(regfit.trial))\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n# Now use Best Subset Selection using all 19 variables\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: exhaustive\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n# Display R-Squared, Adjusted R-Squared, Mallow's Cp and B.I.C from the best models\nnames(summary(regfit.full))\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\nsummary(regfit.full)$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\nsummary(regfit.full)$adjr2\n\n [1] 0.3188503 0.4208024 0.4450753 0.4672734 0.4808971 0.4972001 0.5007849\n [8] 0.5137083 0.5180572 0.5222606 0.5225706 0.5217245 0.5206736 0.5195431\n[15] 0.5178661 0.5162219 0.5144464 0.5126097 0.5106270\n\nsummary(regfit.full)$cp\n\n [1] 104.281319  50.723090  38.693127  27.856220  21.613011  14.023870\n [7]  13.128474   7.400719   6.158685   5.009317   5.874113   7.330766\n[13]   8.888112  10.481576  12.346193  14.187546  16.087831  18.011425\n[19]  20.000000\n\nsummary(regfit.full)$bic\n\n [1]  -90.84637 -128.92622 -135.62693 -141.80892 -144.07143 -147.91690\n [7] -145.25594 -147.61525 -145.44316 -143.21651 -138.86077 -133.87283\n[13] -128.77759 -123.64420 -118.21832 -112.81768 -107.35339 -101.86391\n[19]  -96.30412\n\n\nNow, we plot the various statistics generated by the Best Subset Selection using the regsubsets() function in the object regfit.full.\n\npar(mfrow = c(2, 2))\n# Plotting Residual Sum of Squares\nplot(summary(regfit.full)$rss,\n  xlab = \"Number of Variables\",\n  ylab = \"Residual Sum of Squares\", type = \"l\"\n)\npoints(\n  x = which.min(summary(regfit.full)$rss),\n  y = min(summary(regfit.full)$rss),\n  col = \"red\", cex = 2, pch = 20\n)\n\n# Plotting Adjusted R-Squared\nplot(summary(regfit.full)$adjr2,\n  xlab = \"Number of variables\",\n  ylab = \"Adjusted R-Squared\", type = \"l\"\n)\npoints(\n  x = which.max(summary(regfit.full)$adjr2),\n  y = max(summary(regfit.full)$adjr2),\n  col = \"red\", cex = 2, pch = 20\n)\n\n# Plotting Mallow's Cp\nplot(summary(regfit.full)$cp,\n  xlab = \"Number of variables\",\n  ylab = \"Mallow's Cp\", type = \"l\"\n)\npoints(\n  x = which.min(summary(regfit.full)$cp),\n  y = min(summary(regfit.full)$cp),\n  col = \"red\", cex = 2, pch = 20\n)\n\n# Plotting Bayesian Information Criterion (B.I.C)\nplot(summary(regfit.full)$bic,\n  xlab = \"Number of variables\",\n  ylab = \"B.I.C\", type = \"l\"\n)\npoints(\n  x = which.min(summary(regfit.full)$bic),\n  y = min(summary(regfit.full)$bic),\n  col = \"red\", cex = 2, pch = 20\n)\n\n\n\n\nNow, we use the in-built plot function in the regsubsets() to examine the models.\n\nplot(regfit.full, scale = \"r2\")\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\ncoef(regfit.full, 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n\nWe can use the regsubsets() function to do forward and backward selection as well. We use the same data set Hitters and compare the selected models of 7 variables from the three approaches: (1) Best Subset Selection (2) Forward Selection and (3) Backward Selection.\n\n# Forward Selection; Listing the variables in 7 variable model\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters, method = \"forward\")\nnames(coef(regfit.fwd, 7))\n\n[1] \"(Intercept)\" \"AtBat\"       \"Hits\"        \"Walks\"       \"CRBI\"       \n[6] \"CWalks\"      \"DivisionW\"   \"PutOuts\"    \n\n# Backward Selection; Listing out the variables selected in 7 variable model\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters, method = \"backward\")\nnames(coef(regfit.bwd, 7))\n\n[1] \"(Intercept)\" \"AtBat\"       \"Hits\"        \"Walks\"       \"CRuns\"      \n[6] \"CWalks\"      \"DivisionW\"   \"PutOuts\"    \n\n# Create a nice table to display and compare the coefficients\nComCoef7 &lt;- data.frame(\n  BestSubsetSelection = names(coef(regfit.full, 7)),\n  ForwardSelection = names(coef(regfit.fwd, 7)),\n  BackwardSelection = names(coef(regfit.bwd, 7))\n)\nComCoef7\n\n  BestSubsetSelection ForwardSelection BackwardSelection\n1         (Intercept)      (Intercept)       (Intercept)\n2                Hits            AtBat             AtBat\n3               Walks             Hits              Hits\n4              CAtBat            Walks             Walks\n5               CHits             CRBI             CRuns\n6              CHmRun           CWalks            CWalks\n7           DivisionW        DivisionW         DivisionW\n8             PutOuts          PutOuts           PutOuts\n\n\n\n\n\nWe create test and train boolean vectors to be used to subset the Hitters data set into a training subset and a validation subset. The, we use regsubsets() to find the Best Subset Selection model for \\(k = 1 to p\\) variables in the model. Finally, we will calculate the Mean Squared Error (MSE) for each of the models. This will allow us to see which model has the lowest MSE, i.e. Test Error on the Validation Set.\n\n# Creating Training and Test boolean vectors\nset.seed(5)\ntrain &lt;- sample(c(TRUE, FALSE), size = nrow(Hitters), replace = TRUE)\ntest &lt;- !train\n\n# Run the Best Subset Selection on the training data\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)\n\n# Creating a test matrix (X) with which we can multiply the coefficients of the\n# best model to generate predicted values of Y (i.e. Salary). The model.matrix()\n# creates matrices based on formulas (eg: including dummy variables etc.)\ntest.matrix &lt;- model.matrix(Salary ~ ., data = Hitters[test, ])\n\n# Create empty vector to store Validation Set Error Rates for each best model\nValSetErrors &lt;- rep(NA, 19)\n\n# Calculate Validation Set error rate for each model using loops\nfor (i in 1:19) {\n  coeffs &lt;- coef(regfit.best, id = i) # Extract coefficients in ith model\n  pred &lt;- test.matrix[, names(coeffs)] %*% coeffs # Calculate predicted Y\n  ValSetErrors[i] &lt;- mean((Hitters$Salary[test] - pred)^2)\n}\n\n# Find which Model has minimum MSE\nwhich.min(ValSetErrors)\n\n[1] 10\n\n# Display the coefficients used in the model with minimum MSE\ncoef(regfit.best, id = which.min(ValSetErrors))\n\n(Intercept)       AtBat        Hits       Walks       CHits       CRuns \n-57.3166343  -1.1352411   5.6853121   4.3289280  -0.5825003   1.3342097 \n       CRBI      CWalks   DivisionW     PutOuts  NewLeagueN \n  0.6996060  -0.2490187 -58.2957815   0.1895886  33.3294093 \n\n# Finally, we obtain coefficient estimates from running the best subset selection\n# model on the complete data set. This will allow us to get better coefficient\n# estimates.\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\nround(coef(regfit.best, id = 10),\n  digits = 3\n)\n\n(Intercept)       AtBat        Hits       Walks      CAtBat       CRuns \n    162.535      -2.169       6.918       5.773      -0.130       1.408 \n       CRBI      CWalks   DivisionW     PutOuts     Assists \n      0.774      -0.831    -112.380       0.297       0.283 \n\n\nNow, we can create an automated function to calculate MSE for each model of a regsubsets() object, so that we can use it in loops later when we use Cross-Validation.\n\npredict.regsubsets &lt;- function(object, newdata, id, ...){\n  formula &lt;- as.formula(object$call[[2]]) # Extracting formula from regsubsets object \n  coeffs &lt;- coef(object, id) # Extracting coefficients with their names\n  testmat &lt;- model.matrix(formula, data = newdata) # Create test X matrix\n  testmat[, names(coeffs)] %*% coeffs # Predicted values\n}\n\nWe now use Cross Validation, instead of Validation Set approach to find out the Test MSE.\n\n# Create k folds in the data-set for k-fold Cross Validation\nk &lt;- 10\nset.seed(1)\nfolds &lt;- sample(1:k, size = nrow(Hitters), replace = TRUE)\n# table(folds) # To check : Each observation has been assigned one of the k folds\n\n# Create a matrix to store k Test MSEs of each of the 19 models\ncv.errors &lt;- matrix(\n  data = NA,\n  nrow = k, ncol = 19,\n  dimnames = list(NULL, 1:19)\n)\n\n# Create two loops : (1) Outer Loop to select the cross-validation k fold and\n# then run regsubsets (2) Inner loop to find Test MSE of each on 1 to 19 variable\n# models in the k-th test fold\nfor (j in 1:k) {\n  reg.fit &lt;- regsubsets(Salary ~ .,\n    nvmax = 19,\n    data = Hitters[folds != j, ]\n  )\n  for (i in 1:19) {\n    pred &lt;- predict.regsubsets(\n      object = reg.fit, id = i,\n      newdata = Hitters[folds == j, ]\n    )\n    cv.errors[j, i] &lt;- mean((Hitters$Salary[folds == j] - pred)^2) # Store MSE\n  }\n}\n\n# Calculate mean MSE for each of 1 to 19 variable models. Plot MSEs.\nmean.cv.errors &lt;- apply(cv.errors, MARGIN = 2, FUN = mean)\npar(mfrow = c(1, 1))\nplot(mean.cv.errors,\n  type = \"b\",\n  xlab = \"Number of variables in the Model\",\n  ylab = paste(k, \"-fold Cross-Validation MSE\", collapse = \"\")\n)\npoints(\n  x = which.min(mean.cv.errors),\n  y = min(mean.cv.errors),\n  col = \"red\", pch = 20, cex = 2\n)\n\n\n\n# Now, find coefficients for best model selected by CV, using regsubsets() on\n# full data set\nround(coef(regsubsets(Salary ~ ., Hitters, nvmax = 19),\n  id = which.min(mean.cv.errors)\n), 2)\n\n(Intercept)       AtBat        Hits       Walks      CAtBat       CRuns \n     162.54       -2.17        6.92        5.77       -0.13        1.41 \n       CRBI      CWalks   DivisionW     PutOuts     Assists \n       0.77       -0.83     -112.38        0.30        0.28"
  },
  {
    "objectID": "Chapter6l.html#best-subset-selection",
    "href": "Chapter6l.html#best-subset-selection",
    "title": "Chapter 6 (Lab)",
    "section": "",
    "text": "Load the libraries ISLR and leaps to perform Best Subset Selection on Hitters data.\n\nlibrary(ISLR)\nlibrary(leaps)\nlibrary(kableExtra)\ndata(Hitters)\n\n# Examine the data set\ndim(Hitters)\n\n[1] 322  20\n\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\n# Find and remove missing data\nsum(is.na(Hitters))\n\n[1] 59\n\nHitters &lt;- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\n# Create an Object with Best Subset Selection performed on Hitters Data set\nregfit.trial &lt;- regsubsets(Salary ~ ., data = Hitters)\n# Examine contents of summary of the regsubsets output\nnames(regfit.trial)\n\n [1] \"np\"        \"nrbar\"     \"d\"         \"rbar\"      \"thetab\"    \"first\"    \n [7] \"last\"      \"vorder\"    \"tol\"       \"rss\"       \"bound\"     \"nvmax\"    \n[13] \"ress\"      \"ir\"        \"nbest\"     \"lopt\"      \"il\"        \"ier\"      \n[19] \"xnames\"    \"method\"    \"force.in\"  \"force.out\" \"sserr\"     \"intercept\"\n[25] \"lindep\"    \"nullrss\"   \"nn\"        \"call\"     \n\nnames(summary(regfit.trial))\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n# Now use Best Subset Selection using all 19 variables\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: exhaustive\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n# Display R-Squared, Adjusted R-Squared, Mallow's Cp and B.I.C from the best models\nnames(summary(regfit.full))\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\nsummary(regfit.full)$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\nsummary(regfit.full)$adjr2\n\n [1] 0.3188503 0.4208024 0.4450753 0.4672734 0.4808971 0.4972001 0.5007849\n [8] 0.5137083 0.5180572 0.5222606 0.5225706 0.5217245 0.5206736 0.5195431\n[15] 0.5178661 0.5162219 0.5144464 0.5126097 0.5106270\n\nsummary(regfit.full)$cp\n\n [1] 104.281319  50.723090  38.693127  27.856220  21.613011  14.023870\n [7]  13.128474   7.400719   6.158685   5.009317   5.874113   7.330766\n[13]   8.888112  10.481576  12.346193  14.187546  16.087831  18.011425\n[19]  20.000000\n\nsummary(regfit.full)$bic\n\n [1]  -90.84637 -128.92622 -135.62693 -141.80892 -144.07143 -147.91690\n [7] -145.25594 -147.61525 -145.44316 -143.21651 -138.86077 -133.87283\n[13] -128.77759 -123.64420 -118.21832 -112.81768 -107.35339 -101.86391\n[19]  -96.30412\n\n\nNow, we plot the various statistics generated by the Best Subset Selection using the regsubsets() function in the object regfit.full.\n\npar(mfrow = c(2, 2))\n# Plotting Residual Sum of Squares\nplot(summary(regfit.full)$rss,\n  xlab = \"Number of Variables\",\n  ylab = \"Residual Sum of Squares\", type = \"l\"\n)\npoints(\n  x = which.min(summary(regfit.full)$rss),\n  y = min(summary(regfit.full)$rss),\n  col = \"red\", cex = 2, pch = 20\n)\n\n# Plotting Adjusted R-Squared\nplot(summary(regfit.full)$adjr2,\n  xlab = \"Number of variables\",\n  ylab = \"Adjusted R-Squared\", type = \"l\"\n)\npoints(\n  x = which.max(summary(regfit.full)$adjr2),\n  y = max(summary(regfit.full)$adjr2),\n  col = \"red\", cex = 2, pch = 20\n)\n\n# Plotting Mallow's Cp\nplot(summary(regfit.full)$cp,\n  xlab = \"Number of variables\",\n  ylab = \"Mallow's Cp\", type = \"l\"\n)\npoints(\n  x = which.min(summary(regfit.full)$cp),\n  y = min(summary(regfit.full)$cp),\n  col = \"red\", cex = 2, pch = 20\n)\n\n# Plotting Bayesian Information Criterion (B.I.C)\nplot(summary(regfit.full)$bic,\n  xlab = \"Number of variables\",\n  ylab = \"B.I.C\", type = \"l\"\n)\npoints(\n  x = which.min(summary(regfit.full)$bic),\n  y = min(summary(regfit.full)$bic),\n  col = \"red\", cex = 2, pch = 20\n)\n\n\n\n\nNow, we use the in-built plot function in the regsubsets() to examine the models.\n\nplot(regfit.full, scale = \"r2\")\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\ncoef(regfit.full, 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076"
  },
  {
    "objectID": "Chapter6l.html#forward-and-backward-step-wise-selection",
    "href": "Chapter6l.html#forward-and-backward-step-wise-selection",
    "title": "Chapter 6 (Lab)",
    "section": "",
    "text": "We can use the regsubsets() function to do forward and backward selection as well. We use the same data set Hitters and compare the selected models of 7 variables from the three approaches: (1) Best Subset Selection (2) Forward Selection and (3) Backward Selection.\n\n# Forward Selection; Listing the variables in 7 variable model\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters, method = \"forward\")\nnames(coef(regfit.fwd, 7))\n\n[1] \"(Intercept)\" \"AtBat\"       \"Hits\"        \"Walks\"       \"CRBI\"       \n[6] \"CWalks\"      \"DivisionW\"   \"PutOuts\"    \n\n# Backward Selection; Listing out the variables selected in 7 variable model\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters, method = \"backward\")\nnames(coef(regfit.bwd, 7))\n\n[1] \"(Intercept)\" \"AtBat\"       \"Hits\"        \"Walks\"       \"CRuns\"      \n[6] \"CWalks\"      \"DivisionW\"   \"PutOuts\"    \n\n# Create a nice table to display and compare the coefficients\nComCoef7 &lt;- data.frame(\n  BestSubsetSelection = names(coef(regfit.full, 7)),\n  ForwardSelection = names(coef(regfit.fwd, 7)),\n  BackwardSelection = names(coef(regfit.bwd, 7))\n)\nComCoef7\n\n  BestSubsetSelection ForwardSelection BackwardSelection\n1         (Intercept)      (Intercept)       (Intercept)\n2                Hits            AtBat             AtBat\n3               Walks             Hits              Hits\n4              CAtBat            Walks             Walks\n5               CHits             CRBI             CRuns\n6              CHmRun           CWalks            CWalks\n7           DivisionW        DivisionW         DivisionW\n8             PutOuts          PutOuts           PutOuts"
  },
  {
    "objectID": "Chapter6l.html#choosing-among-models-using-the-validation-set-approach-and-cross-validation",
    "href": "Chapter6l.html#choosing-among-models-using-the-validation-set-approach-and-cross-validation",
    "title": "Chapter 6 (Lab)",
    "section": "",
    "text": "We create test and train boolean vectors to be used to subset the Hitters data set into a training subset and a validation subset. The, we use regsubsets() to find the Best Subset Selection model for \\(k = 1 to p\\) variables in the model. Finally, we will calculate the Mean Squared Error (MSE) for each of the models. This will allow us to see which model has the lowest MSE, i.e. Test Error on the Validation Set.\n\n# Creating Training and Test boolean vectors\nset.seed(5)\ntrain &lt;- sample(c(TRUE, FALSE), size = nrow(Hitters), replace = TRUE)\ntest &lt;- !train\n\n# Run the Best Subset Selection on the training data\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)\n\n# Creating a test matrix (X) with which we can multiply the coefficients of the\n# best model to generate predicted values of Y (i.e. Salary). The model.matrix()\n# creates matrices based on formulas (eg: including dummy variables etc.)\ntest.matrix &lt;- model.matrix(Salary ~ ., data = Hitters[test, ])\n\n# Create empty vector to store Validation Set Error Rates for each best model\nValSetErrors &lt;- rep(NA, 19)\n\n# Calculate Validation Set error rate for each model using loops\nfor (i in 1:19) {\n  coeffs &lt;- coef(regfit.best, id = i) # Extract coefficients in ith model\n  pred &lt;- test.matrix[, names(coeffs)] %*% coeffs # Calculate predicted Y\n  ValSetErrors[i] &lt;- mean((Hitters$Salary[test] - pred)^2)\n}\n\n# Find which Model has minimum MSE\nwhich.min(ValSetErrors)\n\n[1] 10\n\n# Display the coefficients used in the model with minimum MSE\ncoef(regfit.best, id = which.min(ValSetErrors))\n\n(Intercept)       AtBat        Hits       Walks       CHits       CRuns \n-57.3166343  -1.1352411   5.6853121   4.3289280  -0.5825003   1.3342097 \n       CRBI      CWalks   DivisionW     PutOuts  NewLeagueN \n  0.6996060  -0.2490187 -58.2957815   0.1895886  33.3294093 \n\n# Finally, we obtain coefficient estimates from running the best subset selection\n# model on the complete data set. This will allow us to get better coefficient\n# estimates.\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)\nround(coef(regfit.best, id = 10),\n  digits = 3\n)\n\n(Intercept)       AtBat        Hits       Walks      CAtBat       CRuns \n    162.535      -2.169       6.918       5.773      -0.130       1.408 \n       CRBI      CWalks   DivisionW     PutOuts     Assists \n      0.774      -0.831    -112.380       0.297       0.283 \n\n\nNow, we can create an automated function to calculate MSE for each model of a regsubsets() object, so that we can use it in loops later when we use Cross-Validation.\n\npredict.regsubsets &lt;- function(object, newdata, id, ...){\n  formula &lt;- as.formula(object$call[[2]]) # Extracting formula from regsubsets object \n  coeffs &lt;- coef(object, id) # Extracting coefficients with their names\n  testmat &lt;- model.matrix(formula, data = newdata) # Create test X matrix\n  testmat[, names(coeffs)] %*% coeffs # Predicted values\n}\n\nWe now use Cross Validation, instead of Validation Set approach to find out the Test MSE.\n\n# Create k folds in the data-set for k-fold Cross Validation\nk &lt;- 10\nset.seed(1)\nfolds &lt;- sample(1:k, size = nrow(Hitters), replace = TRUE)\n# table(folds) # To check : Each observation has been assigned one of the k folds\n\n# Create a matrix to store k Test MSEs of each of the 19 models\ncv.errors &lt;- matrix(\n  data = NA,\n  nrow = k, ncol = 19,\n  dimnames = list(NULL, 1:19)\n)\n\n# Create two loops : (1) Outer Loop to select the cross-validation k fold and\n# then run regsubsets (2) Inner loop to find Test MSE of each on 1 to 19 variable\n# models in the k-th test fold\nfor (j in 1:k) {\n  reg.fit &lt;- regsubsets(Salary ~ .,\n    nvmax = 19,\n    data = Hitters[folds != j, ]\n  )\n  for (i in 1:19) {\n    pred &lt;- predict.regsubsets(\n      object = reg.fit, id = i,\n      newdata = Hitters[folds == j, ]\n    )\n    cv.errors[j, i] &lt;- mean((Hitters$Salary[folds == j] - pred)^2) # Store MSE\n  }\n}\n\n# Calculate mean MSE for each of 1 to 19 variable models. Plot MSEs.\nmean.cv.errors &lt;- apply(cv.errors, MARGIN = 2, FUN = mean)\npar(mfrow = c(1, 1))\nplot(mean.cv.errors,\n  type = \"b\",\n  xlab = \"Number of variables in the Model\",\n  ylab = paste(k, \"-fold Cross-Validation MSE\", collapse = \"\")\n)\npoints(\n  x = which.min(mean.cv.errors),\n  y = min(mean.cv.errors),\n  col = \"red\", pch = 20, cex = 2\n)\n\n\n\n# Now, find coefficients for best model selected by CV, using regsubsets() on\n# full data set\nround(coef(regsubsets(Salary ~ ., Hitters, nvmax = 19),\n  id = which.min(mean.cv.errors)\n), 2)\n\n(Intercept)       AtBat        Hits       Walks      CAtBat       CRuns \n     162.54       -2.17        6.92        5.77       -0.13        1.41 \n       CRBI      CWalks   DivisionW     PutOuts     Assists \n       0.77       -0.83     -112.38        0.30        0.28"
  },
  {
    "objectID": "Chapter6l.html#ridge-regression",
    "href": "Chapter6l.html#ridge-regression",
    "title": "Chapter 6 (Lab)",
    "section": "6.6.1 Ridge Regression",
    "text": "6.6.1 Ridge Regression\nWe can fit ridge regression using glmnet() with alpha = 0. First, we need to create a vector of possible values of lambda, lamvec to use in glmnet().\n\nlamvec = 10^seq(from = 10, to = -2, length = 100)\n\n# Fitting ridge regression model\nridge.mod = glmnet(x, y, alpha = 0, lambda = lamvec)\n\n# Examine some of the contents of a ridge regression object \nclass(ridge.mod)\n\n[1] \"elnet\"  \"glmnet\"\n\nnames(ridge.mod)\n\n [1] \"a0\"        \"beta\"      \"df\"        \"dim\"       \"lambda\"    \"dev.ratio\"\n [7] \"nulldev\"   \"npasses\"   \"jerr\"      \"offset\"    \"call\"      \"nobs\"     \n\n# Matrix containing the Coefficients for each value of lambda in a 20 X 100 matrix\ndim(coef(ridge.mod))\n\n[1]  20 100\n\n# Examining the ell-2 norm of the coefficients at some lambda values to check the\n# fact that at high lambda values, this ell-2 norm should be low (near zero)\n\n# Get the 25th Lambda value i.e. a large lambda value (we expect ell-2 norm to be low)\nridge.mod$lambda[25]\n\n[1] 12328467\n\nsqrt(sum(coef(ridge.mod)[-1, 25]^2))\n\n[1] 0.006553409\n\n# Get the 75th Lambda value i.e. a small lambda value (we expect ell-2 norm to be high)\nridge.mod$lambda[75]\n\n[1] 10.72267\n\nsqrt(sum(coef(ridge.mod)[-1, 75]^2))\n\n[1] 140.3536\n\n# Plot the coefficients simply using plot() on the glmnet object\nplot(ridge.mod)\n\n\n\n\nWe can now try to predict the coefficients for a fixed arbitrary value of lambda = 50, which we may not have used in the original ridge regression fitting using glmnet().\n\npredict(ridge.mod, s = 50, type = \"coefficients\")\n\n20 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  4.876610e+01\nAtBat       -3.580999e-01\nHits         1.969359e+00\nHmRun       -1.278248e+00\nRuns         1.145892e+00\nRBI          8.038292e-01\nWalks        2.716186e+00\nYears       -6.218319e+00\nCAtBat       5.447837e-03\nCHits        1.064895e-01\nCHmRun       6.244860e-01\nCRuns        2.214985e-01\nCRBI         2.186914e-01\nCWalks      -1.500245e-01\nLeagueN      4.592589e+01\nDivisionW   -1.182011e+02\nPutOuts      2.502322e-01\nAssists      1.215665e-01\nErrors      -3.278600e+00\nNewLeagueN  -9.496680e+00\n\n\nNow, which is the best value of lambda to use for predicting the coefficients? We can answer this question using cross validation. First, we split half of sample as training set, and the remaining half as test set.\n\n# Setting seed for replicability, and creating training and testing sets\nset.seed(1)\ntrain = sample(c(TRUE, FALSE), size = nrow(x), replace = TRUE)\ntest = !train\n\n# Running Ridge Regression on Training Set\nridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = lamvec, \n                   thresh = 1e-12)\n\n# Calculate predicted MSE for some arbitrary lambda value, say 4\nridge.pred = predict(ridge.mod, s = 4, newx = x[test,])\nmean((ridge.pred - y[test])^2)\n\n[1] 143937.4\n\n# Calculate MSE for a very large lambda (i.e. NULL model, all coefficients nearly zero)\nridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])\nmean((ridge.pred - y[test])^2)\n\n[1] 208338.9\n\n# Re-check that this MSE is same as using mean of y as predicted value for all of test set\nmean( (mean(y[train]) - y[test] )^2 )\n\n[1] 208338.9\n\n# Both values are nearly the same. Further, at lambda = 4, MSE is lower than NULL model\n\n# Calculate MSE for lambda = 0 (i.e. least squares regression)\n# ridge.pred = predict(ridge.mod, x = x[test,], y = y[test],\n#                      s = 0, newx = x[test,], \n#                      exact = T)\n#mean((ridge.pred - y[test])^2)\n\n# Verify that the MSE is same as least squares regression\n# lmfit &lt;- lm(Salary ~ ., Hitters, subset = train)\n# lm.pred &lt;- predict(lmfit, newdata = Hitters[test,])\n# mean((lm.pred - y[test])^2)\n\nNow, we use the cross-validation approach to select the lambda value with the lowest MSE. The function cv.glmnet() automatically performs this with a default value of folds = 10.\n\nset.seed(3)\ncv.out = cv.glmnet(x[train, ], y[train], alpha = 0)\n\n# Examine the output of cv.glmnet()\nclass(cv.out)\n\n[1] \"cv.glmnet\"\n\nnames(cv.out)\n\n [1] \"lambda\"     \"cvm\"        \"cvsd\"       \"cvup\"       \"cvlo\"      \n [6] \"nzero\"      \"call\"       \"name\"       \"glmnet.fit\" \"lambda.min\"\n[11] \"lambda.1se\" \"index\"     \n\n# Plotting the output of cv.glmnet()\nplot(cv.out)\n\n\n\n# Finding the test MSE associated with the best lambda value\nbestlam = cv.out$lambda.min\nridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])\nmean( (ridge.pred - y[test])^2 )\n\n[1] 143953.8\n\n# Finally, we find out the coefficients for all variables in Hitters for lambda \n# value of bestlam using the full dataset\nout = glmnet(x, y, alpha = 0)\nround(predict(out, type = \"coefficients\", s = bestlam),3)[1:20, ]\n\n(Intercept)       AtBat        Hits       HmRun        Runs         RBI \n     36.159      -0.246       1.696      -1.104       1.172       0.829 \n      Walks       Years      CAtBat       CHits      CHmRun       CRuns \n      2.481      -4.869       0.008       0.097       0.596       0.193 \n       CRBI      CWalks     LeagueN   DivisionW     PutOuts     Assists \n      0.198      -0.102      42.316    -115.086       0.242       0.102 \n     Errors  NewLeagueN \n     -3.041      -5.554"
  },
  {
    "objectID": "Chapter6l.html#the-lasso",
    "href": "Chapter6l.html#the-lasso",
    "title": "Chapter 6 (Lab)",
    "section": "6.6.2 The Lasso",
    "text": "6.6.2 The Lasso\nThe Lasso can be run using the same function glmnet() but with the argument alpha = 1.\n\nlasso.mod &lt;- glmnet(x[train, ], y[train], alpha = 1, lambda = lamvec)\nplot(lasso.mod)\n\n\n\n# Using cross validation to find out the best lambda value\nset.seed(3)\ncv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1, )\nplot(cv.out)\n\n\n\nbestlam &lt;- cv.out$lambda.min\nbestlam\n\n[1] 1.179335\n\n# Finding Lasso predicted values on test set using best lambda value\nlasso.pred &lt;- predict(lasso.mod, s = bestlam, newx = x[test, ])\n\n# Calculating MSE at best lambda value with Lasso\nmean((lasso.pred - y[test])^2)\n\n[1] 141723.2\n\n# Using full data to find coefficient values at best lambda values\nout &lt;- glmnet(x, y, alpha = 1)\ncoeffs &lt;- predict(out, s = bestlam, type = \"coefficients\")[1:20, ]\ncoeffs[coeffs != 0]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n 150.65616208   -1.89777785    6.63473771    1.01307454   -0.73367711 \n        Walks         Years        CAtBat        CHmRun         CRuns \n   5.49534050   -8.26890842   -0.05204784    0.30697694    1.04151736 \n         CRBI        CWalks       LeagueN     DivisionW       PutOuts \n   0.53045013   -0.70560505   43.84887506 -117.16220845    0.28144497 \n      Assists        Errors    NewLeagueN \n   0.27463999   -2.76569697   -7.82822649"
  },
  {
    "objectID": "Chapter6l.html#principal-components-regression",
    "href": "Chapter6l.html#principal-components-regression",
    "title": "Chapter 6 (Lab)",
    "section": "6.7.1 Principal Components Regression",
    "text": "6.7.1 Principal Components Regression\nWe can fit the Principal Components Regression using the pcr() function which is a part pls library.\n\nlibrary(pls)\nlibrary(ISLR)\nset.seed(1)\n\n# Recreating data sets once again\ndata(\"Hitters\")\nHitters &lt;- na.omit(Hitters)\ntrain &lt;- sample(1:nrow(Hitters), size = nrow(Hitters) / 2)\ntest &lt;- -train\nx &lt;- model.matrix(Salary ~ ., data = Hitters)[, -1]\ny &lt;- Hitters$Salary\n\n# Fitting Principal Components Regression on the Hitters data\npcr.fit &lt;- pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = \"CV\")\n\n# Examining the pcr() object and its summary\nclass(pcr.fit)\n\n[1] \"mvr\"\n\nnames(pcr.fit)\n\n [1] \"coefficients\"  \"scores\"        \"loadings\"      \"Yloadings\"    \n [5] \"projection\"    \"Xmeans\"        \"Ymeans\"        \"fitted.values\"\n [9] \"residuals\"     \"Xvar\"          \"Xtotvar\"       \"fit.time\"     \n[13] \"ncomp\"         \"method\"        \"center\"        \"scale\"        \n[17] \"validation\"    \"call\"          \"terms\"         \"model\"        \n\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV             452    350.6    352.3    352.4    349.7    344.7    342.7\nadjCV          452    350.3    351.9    351.9    349.1    344.1    342.0\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       344.3    345.1    345.7     346.5     347.2     349.4     349.1\nadjCV    343.4    344.2    344.8     345.4     346.1     348.1     347.8\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        342.8     342.7     335.4     337.3     336.9     340.5\nadjCV     341.2     341.3     333.9     335.6     335.1     338.4\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.28     97.26     97.98     98.65     99.15     99.47     99.75\nSalary    46.86     47.76     47.82     47.85     48.10     50.40     50.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.89     99.97     99.99    100.00\nSalary     53.01     53.85     54.61     54.61\n\n# Using validationplot() to see the results of pcr() - plots with RMSE and MSE\npar(mfrow = c(1, 2))\nvalidationplot(pcr.fit)\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n\n\n# Performing PCR on training set and evaluating MSE on test set\npcr.fit &lt;- pcr(Salary ~ .,\n  data = Hitters, subset = train,\n  scale = TRUE, validation = \"CV\"\n)\nvalidationplot(pcr.fit, val.type = \"MSEP\")\n\n# Using ncomp = 7, to find predicted values and MSE\npcr.pred &lt;- predict(pcr.fit, newdata = x[test, ], ncomp = 7)\nmean((pcr.pred - y[test])^2)\n\n[1] 140751.3\n\n# Finally, we fit the PCR model with M=7 to the entire data set\npcr.fit &lt;- pcr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 7)\nsummary(pcr.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: svdpc\nNumber of components considered: 7\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps\nX         38.31    60.16    70.84    79.03    84.29    88.63    92.26\nSalary    40.63    41.58    42.17    43.22    44.90    46.48    46.69"
  },
  {
    "objectID": "Chapter6l.html#partial-least-squares",
    "href": "Chapter6l.html#partial-least-squares",
    "title": "Chapter 6 (Lab)",
    "section": "6.7.2 Partial Least Squares",
    "text": "6.7.2 Partial Least Squares\nWe now implement the Partial Least Squares method using the plsr() function in the pls library. The syntax is similar to the pcr() function.\n\npls.fit &lt;- plsr(Salary ~ .,\n  data = Hitters, subset = train,\n  scale = TRUE, validation = \"CV\"\n)\nsummary(pls.fit)\n\nData:   X dimension: 131 19 \n    Y dimension: 131 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV           428.3    326.7    330.3    326.4    335.6    336.1    338.9\nadjCV        428.3    326.0    328.4    324.7    333.4    333.6    335.7\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       342.7    354.1    351.6     349.6     343.7     347.9     351.2\nadjCV    339.7    349.9    347.7     346.0     340.3     344.1     346.2\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        349.6     348.4     350.2     344.8     344.7     345.3\nadjCV     345.4     344.4     346.2     341.0     340.9     341.4\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71\nSalary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         93.17     96.05     97.08     97.61     97.97     98.70     99.12\nSalary    55.95     56.12     56.47     56.68     57.37     57.76     58.08\n        16 comps  17 comps  18 comps  19 comps\nX          99.61     99.70     99.95    100.00\nSalary     58.17     58.49     58.56     58.62\n\n# Creating Validation Plots to see MSE or RMSE versus M-value\npar(mfrow = c(1, 2))\nvalidationplot(pls.fit)\nvalidationplot(pls.fit, val.type = \"MSEP\")\n\n\n\n# Lowest CV-MSE at M=2, so we compute Test MSE at M=2\npred.pls &lt;- predict(pls.fit, newx = x[test, ], ncomp = 2)\nmean((pred.pls - mean(y[test]))^2)\n\n[1] 91696.61\n\n# Finally, perform PLS on the entire data set, with M=2\npls.fit &lt;- plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 2)\nsummary(pls.fit)\n\nData:   X dimension: 263 19 \n    Y dimension: 263 1\nFit method: kernelpls\nNumber of components considered: 2\nTRAINING: % variance explained\n        1 comps  2 comps\nX         38.08    51.03\nSalary    43.05    46.40"
  },
  {
    "objectID": "Chapter7l.html",
    "href": "Chapter7l.html",
    "title": "Chapter 7 (Lab)",
    "section": "",
    "text": "We will analyse the Wage data set from the ISLR library. We also examine the data set and remove missing values.\n\nlibrary(ISLR)\ndata(Wage)\nsum(is.na(Wage))\n\n[1] 0\n\ndim(Wage)\n\n[1] 3000   11\n\nattach(Wage)\n\n\n\nWe first fit the polynomial regression (up to 4th degree) of wage onto age. Then, we plot the Figure 7.1.\n\nattach(Wage)\n# Method 1: Using a matrix whose columns are the basis of orthogonal polynomials\nfit1 &lt;- lm(wage ~ poly(age, 4), data = Wage)\ncoef(summary(fit1))\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    111.70361  0.7287409 153.283015 0.000000e+00\npoly(age, 4)1  447.06785 39.9147851  11.200558 1.484604e-28\npoly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32\npoly(age, 4)3  125.52169 39.9147851   3.144742 1.678622e-03\npoly(age, 4)4  -77.91118 39.9147851  -1.951938 5.103865e-02\n\n# Method 2: Using actual polynomials of `age` as predictors\nfit2 &lt;- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)\nfit2 &lt;- lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage)\nfit2 &lt;- lm(wage ~ poly(age, 4, raw = TRUE), data = Wage)\n# All three produce the exact same model\n\ncoef(summary(fit2))\n\n                               Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)               -1.841542e+02 6.004038e+01 -3.067172 0.0021802539\npoly(age, 4, raw = TRUE)1  2.124552e+01 5.886748e+00  3.609042 0.0003123618\npoly(age, 4, raw = TRUE)2 -5.638593e-01 2.061083e-01 -2.735743 0.0062606446\npoly(age, 4, raw = TRUE)3  6.810688e-03 3.065931e-03  2.221409 0.0263977518\npoly(age, 4, raw = TRUE)4 -3.203830e-05 1.641359e-05 -1.951938 0.0510386498\n\n# Creating a grid of values of age for which to predict wage\nagelims &lt;- range(age)\nage.grid &lt;- seq(agelims[1], agelims[2], by = 1)\n\n# Predicting wage for each age, with standard errors\npred.wage &lt;- predict(fit1, newdata = data.frame(age = age.grid), se = TRUE)\nse.bands &lt;- cbind(\n  pred.wage$fit - 2 * pred.wage$se.fit,\n  pred.wage$fit + 2 * pred.wage$se.fit\n)\n\n# Predicting I(Wage&gt;250) based on up-to 4th polynomial of age\n# I(wage&gt;250) creates a boolean vector which glm() coerces into 0 and 1\ntable(I(wage &gt; 250))\n\n\nFALSE  TRUE \n 2921    79 \n\n# Fitting a logistic regression: Methods 1 and 2\nfit3 &lt;- glm(I(wage &gt; 250) ~ poly(age, 4), data = Wage, family = \"binomial\")\nfit4 &lt;- glm(I(wage &gt; 250) ~ poly(age, 4, raw = TRUE), data = Wage, family = \"binomial\")\n\n# Predicting values (not using type = \"response\", bcoz that can't create proper SE)\npred.I.wage &lt;- predict(fit3, newdata = data.frame(age = age.grid), se = TRUE)\n# Converting logit into probabilities\nfit.I.wage &lt;- exp(pred.I.wage$fit) / (1 + exp(pred.I.wage$fit))\nse.I.wage &lt;- cbind(\n  pred.I.wage$fit - 2 * pred.I.wage$se.fit,\n  pred.I.wage$fit + 2 * pred.I.wage$se.fit\n)\nse.I.wage &lt;- exp(se.I.wage) / (1 + exp(se.I.wage))\n\n\n# Creating the left hand side plot of Fig 7.1\npar(\n  mfrow = c(1, 2),\n  mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0)\n)\n# Left Hand Side Plot\nplot(x = age, y = wage, xlim = agelims, cex = 0.5, col = \"darkgrey\")\ntitle(\"Degree 4 Polynomial\", outer = TRUE)\nlines(age.grid, pred.wage$fit, col = \"blue\", lwd = 2)\nmatlines(age.grid, se.bands, col = \"blue\", lty = 3, lwd = 1)\n\n# Right Hand Side Plot\nplot(x = age, y = I(wage &gt; 250), type = \"n\", ylim = c(0, 0.2))\npoints(\n  x = jitter(age), y = I(wage &gt; 250) / 5, cex = 0.5, pch = \"|\",\n  col = \"darkgrey\"\n)\nlines(x = age.grid, y = fit.I.wage, col = \"blue\", lwd = 2)\nmatlines(x = age.grid, se.I.wage, col = \"blue\", lty = 3)\n\n\n\n# Now, we examine similarity between method 1 and 2\n# Demonstrating that fitted values from both are the same\npred.wage.2 &lt;- predict(fit2, newdata = data.frame(age = age.grid), se = TRUE)\nmax(pred.wage$fit - pred.wage.2$fit) # Showing max. difference between two\n\n[1] -2.515321e-12\n\npred.I.wage.2 &lt;- predict(fit4, newdata = data.frame(age = age.grid), se = TRUE)\nmax(pred.I.wage$fit - pred.I.wage.2$fit) # Showing max. difference between two\n\n[1] 3.792522e-12\n\n# Lastly, we find which level of polynomials fit the data best\n# Method 1\nfit &lt;- list()\nfor (i in 1:5) {\n  model &lt;- paste(\"M\", i, sep = \"\")\n  fit[[model]] &lt;- lm(wage ~ poly(age, i), data = Wage)\n}\nanova(fit$M1, fit$M2, fit$M3, fit$M4, fit$M5)\n\nAnalysis of Variance Table\n\nModel 1: wage ~ poly(age, i)\nModel 2: wage ~ poly(age, i)\nModel 3: wage ~ poly(age, i)\nModel 4: wage ~ poly(age, i)\nModel 5: wage ~ poly(age, i)\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1   2998 5022216                                    \n2   2997 4793430  1    228786 143.5931 &lt; 2.2e-16 ***\n3   2996 4777674  1     15756   9.8888  0.001679 ** \n4   2995 4771604  1      6070   3.8098  0.051046 .  \n5   2994 4770322  1      1283   0.8050  0.369682    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Since we are using orgonal polynomials, we could equally well use\ncoef(summary(fit$M5))\n\n                Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept)    111.70361  0.7287647 153.2780243 0.000000e+00\npoly(age, i)1  447.06785 39.9160847  11.2001930 1.491111e-28\npoly(age, i)2 -478.31581 39.9160847 -11.9830341 2.367734e-32\npoly(age, i)3  125.52169 39.9160847   3.1446392 1.679213e-03\npoly(age, i)4  -77.91118 39.9160847  -1.9518743 5.104623e-02\npoly(age, i)5  -35.81289 39.9160847  -0.8972045 3.696820e-01\n\n# Method 2 (where we have extra variables like education) [Same Results]\nfit &lt;- list()\nfor (i in 1:5) {\n  model &lt;- paste(\"M\", i, sep = \"\")\n  fit[[model]] &lt;- lm(wage ~ poly(age, i, raw = TRUE), data = Wage)\n}\nanova(fit$M1, fit$M2, fit$M3, fit$M4, fit$M5)\n\nAnalysis of Variance Table\n\nModel 1: wage ~ poly(age, i, raw = TRUE)\nModel 2: wage ~ poly(age, i, raw = TRUE)\nModel 3: wage ~ poly(age, i, raw = TRUE)\nModel 4: wage ~ poly(age, i, raw = TRUE)\nModel 5: wage ~ poly(age, i, raw = TRUE)\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1   2998 5022216                                    \n2   2997 4793430  1    228786 143.5931 &lt; 2.2e-16 ***\n3   2996 4777674  1     15756   9.8888  0.001679 ** \n4   2995 4771604  1      6070   3.8098  0.051046 .  \n5   2994 4770322  1      1283   0.8050  0.369682    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nIn this section, we use the splines library. We could also use the locfit library. We will fit (Part A) splines with basis function bs(), (Part B) natural splines using the ns() function, (Part C) smoothing splines using the smooth.spline() function and (Part D) local regression using the loess() function. We will try to predict wage from age in the Wage data set.\n\nlibrary(splines)\nattach(Wage)\n# Understanding the functions bs(), ns()\n# We can use wither df (degrees of freedom) or specify \"knots\"\ndim(bs(age, knots = c(20,40,60)))\n\n[1] 3000    6\n\ndim(bs(age, df = 6))\n\n[1] 3000    6\n\nattr(x = bs(age, df = 6), which = \"knots\")\n\n[1] 33.75 42.00 51.00\n\n# We can use option degree to specify the level of polynomial (eg. x^5)\ndim(bs(age, degree = 5, knots = c(20,40,60)))\n\n[1] 3000    8\n\ndim(bs(age, df = 6, degree = 5))\n\n[1] 3000    6\n\nattr(x = bs(age, df = 6, degree = 5), which = \"knots\")\n\n[1] 42\n\n# Part A: REGRESSION SPLINES (CUBIC)\nfits = lm(wage ~ bs(age, knots = c(25,40,60)), data = Wage)\n\n# Calculate predicted values\npreds = predict(fits, newdata = data.frame(age = age.grid), se = TRUE)\nse.preds = cbind(preds$fit - 2*preds$se.fit,\n                preds$fit + 2*preds$se.fit)\n\n# Plotting the results - Blue lines for Cubic Splines\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5)\nlines(x = age.grid, y = preds$fit, col = \"blue\", lwd = 1.5)\nmatlines(x = age.grid, se.preds, col = \"blue\", lty = 3, lwd = 0.8)\n\n\n\n# Part B: NATURAL SPLINES\nfit.ns = lm(wage ~ ns(age, df = 4), data = Wage)\npred.ns = predict(fit.ns, newdata = data.frame(age = age.grid), se = TRUE)\nse.pred.ns = cbind(pred.ns$fit - 2*pred.ns$se.fit,\n                   pred.ns$fit + 2*pred.ns$se.fit)\n\n# Plotting the results (Blue - Cubic Spline) (Red - Natural Cubic Spline)\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5)\nlines(x = age.grid, y = preds$fit, col = \"blue\", lwd = 1.5)\nmatlines(x = age.grid, se.preds, col = \"blue\", lty = 3, lwd = 0.8)\nlines(x = age.grid, y = pred.ns$fit, col = \"red\", lwd = 1.5)\nmatlines(x = age.grid, se.pred.ns, col = \"red\", lty = 3, lwd = 0.8)\nlegend(\"topright\", c(\"Cubic Spline\", \"Natural Spline\"), lty = c(1,1),\n       col = c(\"blue\", \"red\"), lwd = c(1.5, 1.5), cex = 0.8)\n\n\n\n# Part C: SMOOTHING SPLINES\nfit.ss1 = smooth.spline(age, wage, df = 16)\nfit.ss2 = smooth.spline(age, wage, cv = TRUE)\nfit.ss2$df # Viewing the CV-selected degrees of freedom\n\n[1] 6.794596\n\n# Plotting Smoothing Splines directly (no need to calculate predicted values)\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5, main = \"Smoothing Splines\")\nlines(fit.ss1, col = \"orange\", lwd = 1)\nlines(fit.ss2, col = \"brown\", lwd = 1)\nlegend(\"topright\", c(\"SS with df = 16\", \"SS with df = 6.8\"),\n       lty = c(1,1), lwd = c(1,1), col = c(\"orange\", \"brown\"))\n\n\n\n# Part D: LOESS (Local Regression)\nfit.L1 = loess(wage ~ age, span = 0.2, data = Wage)\nfit.L2 = loess(wage ~ age, span = 0.5, data = Wage)\n\n# Plotting the Local Regression with two different spans\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5, main = \"Local Regression\")\nlines(x = age.grid, y = predict(fit.L1, newdata = data.frame(age = age.grid)),\n      col = \"red\", lty = 1, lwd = 1.5)\nlines(x = age.grid, y = predict(fit.L2, newdata = data.frame(age = age.grid)),\n      col = \"blue\", lty = 1, lwd = 1.5)\nlegend(\"topright\", c(\"Span 0.2\", \"Span 0.5\"), lty = c(1,1),\n       lwd = c(1,1), col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\nWe now use Generalized Additive Models to predict wage from age, year and education from the Wage data set.\n\n# Using lm() for natural splines\nlibrary(splines)\nlibrary(gam)\ngam1 &lt;- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)\npar(mfrow = c(1, 3))\nplot.Gam(gam1, se = TRUE, col = \"red\")\n\n\n\n# Using gam() for more complicated stuff: Smoothing Splines\nlibrary(gam)\ngam.m3 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)\npar(mfrow = c(1, 3))\nplot(gam.m3, se = TRUE, col = \"blue\")\n\n\n\n# Comparing the 3 models m1: no year included, m2 = linear in year,\n# m3 = smoothing spline in year with df = 4\ngam.m1 &lt;- gam(wage ~ s(age, 5) + education, data = Wage)\ngam.m2 &lt;- gam(wage ~ year + s(age, 5) + education, data = Wage)\nanova(gam.m1, gam.m2, gam.m3)\n\nAnalysis of Deviance Table\n\nModel 1: wage ~ s(age, 5) + education\nModel 2: wage ~ year + s(age, 5) + education\nModel 3: wage ~ s(year, 4) + s(age, 5) + education\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      2990    3711731                          \n2      2989    3693842  1  17889.2 0.0001419 ***\n3      2986    3689770  3   4071.1 0.3483897    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Thus gam.m2, with linear fitting for year is the best model\nsummary(gam.m2)\n\n\nCall: gam(formula = wage ~ year + s(age, 5) + education, data = Wage)\nDeviance Residuals:\n     Min       1Q   Median       3Q      Max \n-119.959  -19.647   -3.199   13.969  213.562 \n\n(Dispersion Parameter for gaussian family taken to be 1235.812)\n\n    Null Deviance: 5222086 on 2999 degrees of freedom\nResidual Deviance: 3693842 on 2989 degrees of freedom\nAIC: 29885.06 \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nyear         1   27154   27154  21.973  2.89e-06 ***\ns(age, 5)    1  194535  194535 157.415 &lt; 2.2e-16 ***\neducation    4 1069081  267270 216.271 &lt; 2.2e-16 ***\nResiduals 2989 3693842    1236                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n            Npar Df Npar F     Pr(F)    \n(Intercept)                             \nyear                                    \ns(age, 5)         4  32.46 &lt; 2.2e-16 ***\neducation                               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Predicting values from gam()\npreds &lt;- predict(gam.m2, newdata = Wage)\n\n# Using gam with Local Regression on only 1 variable\ngam.lo &lt;- gam(wage ~ s(year, 4) + lo(age, span = 0.7) + education,\n  data = Wage\n)\npar(mfrow = c(1, 3))\nplot(gam.lo, se = TRUE, col = \"green\")\n\n# Using gam with local regression on interaction of age and year\ngam.lo.i &lt;- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage)\n# Plotting this model in 3-D using akima\nlibrary(akima)\n\n\n\n# install.packages(\"interp\")\nlibrary(interp)\npar(mfrow = c(1, 2))\nplot(gam.lo.i)\n\n\n\n# Using gam for logistic regression\ngam.lr &lt;- gam(I(wage &gt; 250) ~ year + s(age, 5) + education, data = Wage, family = \"binomial\")\npar(mfrow = c(1, 3))\nplot(gam.lr, se = TRUE, col = \"green\")\n\n\n\n# Removing &lt;HS category in education to remove the surprising results\ngam.lr &lt;- gam(I(wage &gt; 250) ~ year + s(age, 5) + education,\n  data = Wage,\n  family = \"binomial\", subset = (education != \"1. &lt; HS Grad\")\n)\npar(mfrow = c(1, 3))\nplot(gam.lr, se = TRUE, col = \"green\")"
  },
  {
    "objectID": "Chapter7l.html#polynomial-regression-and-step-functions",
    "href": "Chapter7l.html#polynomial-regression-and-step-functions",
    "title": "Chapter 7 (Lab)",
    "section": "",
    "text": "We first fit the polynomial regression (up to 4th degree) of wage onto age. Then, we plot the Figure 7.1.\n\nattach(Wage)\n# Method 1: Using a matrix whose columns are the basis of orthogonal polynomials\nfit1 &lt;- lm(wage ~ poly(age, 4), data = Wage)\ncoef(summary(fit1))\n\n                Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    111.70361  0.7287409 153.283015 0.000000e+00\npoly(age, 4)1  447.06785 39.9147851  11.200558 1.484604e-28\npoly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32\npoly(age, 4)3  125.52169 39.9147851   3.144742 1.678622e-03\npoly(age, 4)4  -77.91118 39.9147851  -1.951938 5.103865e-02\n\n# Method 2: Using actual polynomials of `age` as predictors\nfit2 &lt;- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage)\nfit2 &lt;- lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage)\nfit2 &lt;- lm(wage ~ poly(age, 4, raw = TRUE), data = Wage)\n# All three produce the exact same model\n\ncoef(summary(fit2))\n\n                               Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept)               -1.841542e+02 6.004038e+01 -3.067172 0.0021802539\npoly(age, 4, raw = TRUE)1  2.124552e+01 5.886748e+00  3.609042 0.0003123618\npoly(age, 4, raw = TRUE)2 -5.638593e-01 2.061083e-01 -2.735743 0.0062606446\npoly(age, 4, raw = TRUE)3  6.810688e-03 3.065931e-03  2.221409 0.0263977518\npoly(age, 4, raw = TRUE)4 -3.203830e-05 1.641359e-05 -1.951938 0.0510386498\n\n# Creating a grid of values of age for which to predict wage\nagelims &lt;- range(age)\nage.grid &lt;- seq(agelims[1], agelims[2], by = 1)\n\n# Predicting wage for each age, with standard errors\npred.wage &lt;- predict(fit1, newdata = data.frame(age = age.grid), se = TRUE)\nse.bands &lt;- cbind(\n  pred.wage$fit - 2 * pred.wage$se.fit,\n  pred.wage$fit + 2 * pred.wage$se.fit\n)\n\n# Predicting I(Wage&gt;250) based on up-to 4th polynomial of age\n# I(wage&gt;250) creates a boolean vector which glm() coerces into 0 and 1\ntable(I(wage &gt; 250))\n\n\nFALSE  TRUE \n 2921    79 \n\n# Fitting a logistic regression: Methods 1 and 2\nfit3 &lt;- glm(I(wage &gt; 250) ~ poly(age, 4), data = Wage, family = \"binomial\")\nfit4 &lt;- glm(I(wage &gt; 250) ~ poly(age, 4, raw = TRUE), data = Wage, family = \"binomial\")\n\n# Predicting values (not using type = \"response\", bcoz that can't create proper SE)\npred.I.wage &lt;- predict(fit3, newdata = data.frame(age = age.grid), se = TRUE)\n# Converting logit into probabilities\nfit.I.wage &lt;- exp(pred.I.wage$fit) / (1 + exp(pred.I.wage$fit))\nse.I.wage &lt;- cbind(\n  pred.I.wage$fit - 2 * pred.I.wage$se.fit,\n  pred.I.wage$fit + 2 * pred.I.wage$se.fit\n)\nse.I.wage &lt;- exp(se.I.wage) / (1 + exp(se.I.wage))\n\n\n# Creating the left hand side plot of Fig 7.1\npar(\n  mfrow = c(1, 2),\n  mar = c(4.5, 4.5, 1, 1), oma = c(0, 0, 4, 0)\n)\n# Left Hand Side Plot\nplot(x = age, y = wage, xlim = agelims, cex = 0.5, col = \"darkgrey\")\ntitle(\"Degree 4 Polynomial\", outer = TRUE)\nlines(age.grid, pred.wage$fit, col = \"blue\", lwd = 2)\nmatlines(age.grid, se.bands, col = \"blue\", lty = 3, lwd = 1)\n\n# Right Hand Side Plot\nplot(x = age, y = I(wage &gt; 250), type = \"n\", ylim = c(0, 0.2))\npoints(\n  x = jitter(age), y = I(wage &gt; 250) / 5, cex = 0.5, pch = \"|\",\n  col = \"darkgrey\"\n)\nlines(x = age.grid, y = fit.I.wage, col = \"blue\", lwd = 2)\nmatlines(x = age.grid, se.I.wage, col = \"blue\", lty = 3)\n\n\n\n# Now, we examine similarity between method 1 and 2\n# Demonstrating that fitted values from both are the same\npred.wage.2 &lt;- predict(fit2, newdata = data.frame(age = age.grid), se = TRUE)\nmax(pred.wage$fit - pred.wage.2$fit) # Showing max. difference between two\n\n[1] -2.515321e-12\n\npred.I.wage.2 &lt;- predict(fit4, newdata = data.frame(age = age.grid), se = TRUE)\nmax(pred.I.wage$fit - pred.I.wage.2$fit) # Showing max. difference between two\n\n[1] 3.792522e-12\n\n# Lastly, we find which level of polynomials fit the data best\n# Method 1\nfit &lt;- list()\nfor (i in 1:5) {\n  model &lt;- paste(\"M\", i, sep = \"\")\n  fit[[model]] &lt;- lm(wage ~ poly(age, i), data = Wage)\n}\nanova(fit$M1, fit$M2, fit$M3, fit$M4, fit$M5)\n\nAnalysis of Variance Table\n\nModel 1: wage ~ poly(age, i)\nModel 2: wage ~ poly(age, i)\nModel 3: wage ~ poly(age, i)\nModel 4: wage ~ poly(age, i)\nModel 5: wage ~ poly(age, i)\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1   2998 5022216                                    \n2   2997 4793430  1    228786 143.5931 &lt; 2.2e-16 ***\n3   2996 4777674  1     15756   9.8888  0.001679 ** \n4   2995 4771604  1      6070   3.8098  0.051046 .  \n5   2994 4770322  1      1283   0.8050  0.369682    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Since we are using orgonal polynomials, we could equally well use\ncoef(summary(fit$M5))\n\n                Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept)    111.70361  0.7287647 153.2780243 0.000000e+00\npoly(age, i)1  447.06785 39.9160847  11.2001930 1.491111e-28\npoly(age, i)2 -478.31581 39.9160847 -11.9830341 2.367734e-32\npoly(age, i)3  125.52169 39.9160847   3.1446392 1.679213e-03\npoly(age, i)4  -77.91118 39.9160847  -1.9518743 5.104623e-02\npoly(age, i)5  -35.81289 39.9160847  -0.8972045 3.696820e-01\n\n# Method 2 (where we have extra variables like education) [Same Results]\nfit &lt;- list()\nfor (i in 1:5) {\n  model &lt;- paste(\"M\", i, sep = \"\")\n  fit[[model]] &lt;- lm(wage ~ poly(age, i, raw = TRUE), data = Wage)\n}\nanova(fit$M1, fit$M2, fit$M3, fit$M4, fit$M5)\n\nAnalysis of Variance Table\n\nModel 1: wage ~ poly(age, i, raw = TRUE)\nModel 2: wage ~ poly(age, i, raw = TRUE)\nModel 3: wage ~ poly(age, i, raw = TRUE)\nModel 4: wage ~ poly(age, i, raw = TRUE)\nModel 5: wage ~ poly(age, i, raw = TRUE)\n  Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    \n1   2998 5022216                                    \n2   2997 4793430  1    228786 143.5931 &lt; 2.2e-16 ***\n3   2996 4777674  1     15756   9.8888  0.001679 ** \n4   2995 4771604  1      6070   3.8098  0.051046 .  \n5   2994 4770322  1      1283   0.8050  0.369682    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Chapter7l.html#splines",
    "href": "Chapter7l.html#splines",
    "title": "Chapter 7 (Lab)",
    "section": "",
    "text": "In this section, we use the splines library. We could also use the locfit library. We will fit (Part A) splines with basis function bs(), (Part B) natural splines using the ns() function, (Part C) smoothing splines using the smooth.spline() function and (Part D) local regression using the loess() function. We will try to predict wage from age in the Wage data set.\n\nlibrary(splines)\nattach(Wage)\n# Understanding the functions bs(), ns()\n# We can use wither df (degrees of freedom) or specify \"knots\"\ndim(bs(age, knots = c(20,40,60)))\n\n[1] 3000    6\n\ndim(bs(age, df = 6))\n\n[1] 3000    6\n\nattr(x = bs(age, df = 6), which = \"knots\")\n\n[1] 33.75 42.00 51.00\n\n# We can use option degree to specify the level of polynomial (eg. x^5)\ndim(bs(age, degree = 5, knots = c(20,40,60)))\n\n[1] 3000    8\n\ndim(bs(age, df = 6, degree = 5))\n\n[1] 3000    6\n\nattr(x = bs(age, df = 6, degree = 5), which = \"knots\")\n\n[1] 42\n\n# Part A: REGRESSION SPLINES (CUBIC)\nfits = lm(wage ~ bs(age, knots = c(25,40,60)), data = Wage)\n\n# Calculate predicted values\npreds = predict(fits, newdata = data.frame(age = age.grid), se = TRUE)\nse.preds = cbind(preds$fit - 2*preds$se.fit,\n                preds$fit + 2*preds$se.fit)\n\n# Plotting the results - Blue lines for Cubic Splines\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5)\nlines(x = age.grid, y = preds$fit, col = \"blue\", lwd = 1.5)\nmatlines(x = age.grid, se.preds, col = \"blue\", lty = 3, lwd = 0.8)\n\n\n\n# Part B: NATURAL SPLINES\nfit.ns = lm(wage ~ ns(age, df = 4), data = Wage)\npred.ns = predict(fit.ns, newdata = data.frame(age = age.grid), se = TRUE)\nse.pred.ns = cbind(pred.ns$fit - 2*pred.ns$se.fit,\n                   pred.ns$fit + 2*pred.ns$se.fit)\n\n# Plotting the results (Blue - Cubic Spline) (Red - Natural Cubic Spline)\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5)\nlines(x = age.grid, y = preds$fit, col = \"blue\", lwd = 1.5)\nmatlines(x = age.grid, se.preds, col = \"blue\", lty = 3, lwd = 0.8)\nlines(x = age.grid, y = pred.ns$fit, col = \"red\", lwd = 1.5)\nmatlines(x = age.grid, se.pred.ns, col = \"red\", lty = 3, lwd = 0.8)\nlegend(\"topright\", c(\"Cubic Spline\", \"Natural Spline\"), lty = c(1,1),\n       col = c(\"blue\", \"red\"), lwd = c(1.5, 1.5), cex = 0.8)\n\n\n\n# Part C: SMOOTHING SPLINES\nfit.ss1 = smooth.spline(age, wage, df = 16)\nfit.ss2 = smooth.spline(age, wage, cv = TRUE)\nfit.ss2$df # Viewing the CV-selected degrees of freedom\n\n[1] 6.794596\n\n# Plotting Smoothing Splines directly (no need to calculate predicted values)\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5, main = \"Smoothing Splines\")\nlines(fit.ss1, col = \"orange\", lwd = 1)\nlines(fit.ss2, col = \"brown\", lwd = 1)\nlegend(\"topright\", c(\"SS with df = 16\", \"SS with df = 6.8\"),\n       lty = c(1,1), lwd = c(1,1), col = c(\"orange\", \"brown\"))\n\n\n\n# Part D: LOESS (Local Regression)\nfit.L1 = loess(wage ~ age, span = 0.2, data = Wage)\nfit.L2 = loess(wage ~ age, span = 0.5, data = Wage)\n\n# Plotting the Local Regression with two different spans\nplot(x = age, y = wage, col = \"darkgrey\", cex = 0.5, main = \"Local Regression\")\nlines(x = age.grid, y = predict(fit.L1, newdata = data.frame(age = age.grid)),\n      col = \"red\", lty = 1, lwd = 1.5)\nlines(x = age.grid, y = predict(fit.L2, newdata = data.frame(age = age.grid)),\n      col = \"blue\", lty = 1, lwd = 1.5)\nlegend(\"topright\", c(\"Span 0.2\", \"Span 0.5\"), lty = c(1,1),\n       lwd = c(1,1), col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "Chapter7l.html#gams",
    "href": "Chapter7l.html#gams",
    "title": "Chapter 7 (Lab)",
    "section": "",
    "text": "We now use Generalized Additive Models to predict wage from age, year and education from the Wage data set.\n\n# Using lm() for natural splines\nlibrary(splines)\nlibrary(gam)\ngam1 &lt;- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)\npar(mfrow = c(1, 3))\nplot.Gam(gam1, se = TRUE, col = \"red\")\n\n\n\n# Using gam() for more complicated stuff: Smoothing Splines\nlibrary(gam)\ngam.m3 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)\npar(mfrow = c(1, 3))\nplot(gam.m3, se = TRUE, col = \"blue\")\n\n\n\n# Comparing the 3 models m1: no year included, m2 = linear in year,\n# m3 = smoothing spline in year with df = 4\ngam.m1 &lt;- gam(wage ~ s(age, 5) + education, data = Wage)\ngam.m2 &lt;- gam(wage ~ year + s(age, 5) + education, data = Wage)\nanova(gam.m1, gam.m2, gam.m3)\n\nAnalysis of Deviance Table\n\nModel 1: wage ~ s(age, 5) + education\nModel 2: wage ~ year + s(age, 5) + education\nModel 3: wage ~ s(year, 4) + s(age, 5) + education\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      2990    3711731                          \n2      2989    3693842  1  17889.2 0.0001419 ***\n3      2986    3689770  3   4071.1 0.3483897    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Thus gam.m2, with linear fitting for year is the best model\nsummary(gam.m2)\n\n\nCall: gam(formula = wage ~ year + s(age, 5) + education, data = Wage)\nDeviance Residuals:\n     Min       1Q   Median       3Q      Max \n-119.959  -19.647   -3.199   13.969  213.562 \n\n(Dispersion Parameter for gaussian family taken to be 1235.812)\n\n    Null Deviance: 5222086 on 2999 degrees of freedom\nResidual Deviance: 3693842 on 2989 degrees of freedom\nAIC: 29885.06 \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nyear         1   27154   27154  21.973  2.89e-06 ***\ns(age, 5)    1  194535  194535 157.415 &lt; 2.2e-16 ***\neducation    4 1069081  267270 216.271 &lt; 2.2e-16 ***\nResiduals 2989 3693842    1236                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n            Npar Df Npar F     Pr(F)    \n(Intercept)                             \nyear                                    \ns(age, 5)         4  32.46 &lt; 2.2e-16 ***\neducation                               \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Predicting values from gam()\npreds &lt;- predict(gam.m2, newdata = Wage)\n\n# Using gam with Local Regression on only 1 variable\ngam.lo &lt;- gam(wage ~ s(year, 4) + lo(age, span = 0.7) + education,\n  data = Wage\n)\npar(mfrow = c(1, 3))\nplot(gam.lo, se = TRUE, col = \"green\")\n\n# Using gam with local regression on interaction of age and year\ngam.lo.i &lt;- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage)\n# Plotting this model in 3-D using akima\nlibrary(akima)\n\n\n\n# install.packages(\"interp\")\nlibrary(interp)\npar(mfrow = c(1, 2))\nplot(gam.lo.i)\n\n\n\n# Using gam for logistic regression\ngam.lr &lt;- gam(I(wage &gt; 250) ~ year + s(age, 5) + education, data = Wage, family = \"binomial\")\npar(mfrow = c(1, 3))\nplot(gam.lr, se = TRUE, col = \"green\")\n\n\n\n# Removing &lt;HS category in education to remove the surprising results\ngam.lr &lt;- gam(I(wage &gt; 250) ~ year + s(age, 5) + education,\n  data = Wage,\n  family = \"binomial\", subset = (education != \"1. &lt; HS Grad\")\n)\npar(mfrow = c(1, 3))\nplot(gam.lr, se = TRUE, col = \"green\")"
  },
  {
    "objectID": "Chapter9l.html",
    "href": "Chapter9l.html",
    "title": "Chapter 9 (Lab)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gridExtra)"
  },
  {
    "objectID": "Chapter9l.html#support-vector-classifier",
    "href": "Chapter9l.html#support-vector-classifier",
    "title": "Chapter 9 (Lab)",
    "section": "9.6.1 Support Vector Classifier",
    "text": "9.6.1 Support Vector Classifier\nWe could use the support vector classifier from the e1071 library, or the LibLineaR library. Here, in this lab we use e1071 and its function svm() with kernel = \"linear\" argument.\n\nlibrary(e1071)\noptions(digits = 2)\n# Creating a simulated data set\nset.seed(3)\nx &lt;- matrix(data = rnorm(40), ncol = 2)\ny &lt;- c(rep(1, 10), rep(-1, 10))\n# Changing x values in set y=1\nx[y == 1, ] &lt;- x[y == 1, ] + 1\n\n# Forming a data frame from x and y (creating y as a factor, so that svm() can be used)\nsv.data &lt;- data.frame(x = x, y = as.factor(y))\n\n# checking whether the data set generated is linearly separable\nggplot(data = sv.data) +\n  geom_point(aes(x = x.1, y = x.2, col = y),\n    pch = 20, size = 4\n  ) +\n  labs(\n    x = \"x1 (First variable)\", y = \"x2 (Second Variable)\",\n    title = \"Plot of 2-dimensional simulated data set\",\n    col = \"Class (y)\"\n  ) +\n  theme_bw()\n\n\n\n# Simpler Plot of data set with plot()\nplot(\n  x = sv.data$x.1, y = sv.data$x.2, col = (3 - y),\n  xlab = \"x1 (First variable)\", ylab = \"x2 (Second Variable)\",\n  main = \"Plot of 2-dimensional simulated data set\",\n  pch = 20, cex = 1.5\n)\n\n\n\n# Fitting a Support Vector Classifier on the data set\n# (using a fixed arbitrary cost = 10)\nsvm.fit &lt;- svm(y ~ .,\n  data = sv.data, kernel = \"linear\",\n  cost = 10, scale = FALSE\n)\n\n# Examining the Support Vector Classifier model\nclass(svm.fit)\n\n[1] \"svm.formula\" \"svm\"        \n\nnames(svm.fit)\n\n [1] \"call\"            \"type\"            \"kernel\"          \"cost\"           \n [5] \"degree\"          \"gamma\"           \"coef0\"           \"nu\"             \n [9] \"epsilon\"         \"sparse\"          \"scaled\"          \"x.scale\"        \n[13] \"y.scale\"         \"nclasses\"        \"levels\"          \"tot.nSV\"        \n[17] \"nSV\"             \"labels\"          \"SV\"              \"index\"          \n[21] \"rho\"             \"compprob\"        \"probA\"           \"probB\"          \n[25] \"sigma\"           \"coefs\"           \"na.action\"       \"fitted\"         \n[29] \"decision.values\" \"terms\"          \n\nsummary(svm.fit)\n\n\nCall:\nsvm(formula = y ~ ., data = sv.data, kernel = \"linear\", cost = 10, \n    scale = FALSE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  12\n\n ( 6 6 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n# Plotting the fitted Support Vector Classifier\nplot(svm.fit, sv.data)\n\n\n\n# Finding the support vectors in the svm.fit model\nsvm.fit$index\n\n [1]  1  2  4  5  6  9 11 14 15 16 19 20\n\n# Trying differen values of Cost Parameter\n\n# Imposing lower cost: we expect wider margin, and more support vector\nsvm.fit.1 &lt;- svm(y ~ .,\n  data = sv.data, kernel = \"linear\",\n  cost = 0.001, scale = FALSE\n)\nlength(svm.fit.1$index)\n\n[1] 20\n\n# Finding the best Cost value in Cross-validation using tune()\nset.seed(3)\n# Making a vector of different cost values\ncosts &lt;- 10^(-5:5)\n# Running Cross Validation\ncv.svm &lt;- tune(svm, y ~ .,\n  data = sv.data, kernel = \"linear\",\n  ranges = list(cost = costs)\n)\nsummary(cv.svm)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost\n    1\n\n- best performance: 0.25 \n\n- Detailed performance results:\n    cost error dispersion\n1  1e-05  0.80       0.35\n2  1e-04  0.80       0.35\n3  1e-03  0.80       0.35\n4  1e-02  0.80       0.35\n5  1e-01  0.30       0.35\n6  1e+00  0.25       0.35\n7  1e+01  0.25       0.35\n8  1e+02  0.25       0.35\n9  1e+03  0.25       0.35\n10 1e+04  0.25       0.35\n11 1e+05  0.25       0.35\n\n# Selecting the best model in cross-validation\nbest.svc &lt;- cv.svm$best.model\nsummary(best.svc)\n\n\nCall:\nbest.tune(METHOD = svm, train.x = y ~ ., data = sv.data, ranges = list(cost = costs), \n    kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  13\n\n ( 6 7 )\n\n\nNumber of Classes:  2 \n\nLevels: \n -1 1\n\n# Making predictions on a new data set using the best model\n# Create a new data set\nx.test &lt;- matrix(rnorm(40), ncol = 2)\ny.test &lt;- sample(c(-1, 1), size = 20, replace = TRUE)\nx.test[y.test == 1, ] &lt;- x.test[y.test == 1, ] + 1\nsv.test.data &lt;- data.frame(\n  x = x.test,\n  y = as.factor(y.test)\n)\n# Plotting the test data set for confirmation that we did it right\nplot(\n  x = sv.test.data$x.1,\n  y = sv.test.data$x.2,\n  col = 3 - y, pch = 20, cex = 1.5\n)\n\n\n\n# Predicting the class for the new data set\npred &lt;- predict(best.svc, newdata = sv.test.data)\nresult1 &lt;- table(\n  predicted = pred,\n  truth = sv.test.data$y\n)\nresult1\n\n         truth\npredicted -1 1\n       -1  7 2\n       1   3 8\n\n# Percentage Correct\n100 * (result1[1, 1] + result1[2, 2]) / sum(result1)\n\n[1] 75\n\n# New case where two classes are linearly separable (completely)\nset.seed(3)\nx &lt;- matrix(data = rnorm(40), ncol = 2)\ny &lt;- c(rep(1, 10), rep(-1, 10))\nx[y == 1, ] &lt;- x[y == 1, ] + 2\nsv.data &lt;- data.frame(x = x, y = as.factor(y))\nplot(\n  x = sv.data$x.1, y = sv.data$x.2, col = (3 - y),\n  xlab = \"x1 (First variable)\", ylab = \"x2 (Second Variable)\",\n  main = \"Plot of 2-dimensional simulated data set\",\n  pch = 20, cex = 1.5\n)\n\n\n\nsvm.fit &lt;- svm(y ~ .,\n  data = sv.data, kernel = \"linear\",\n  cost = 10, scale = FALSE\n)\nplot(svm.fit, sv.data)\n\n\n\ncosts &lt;- 10^(-5:5)\ncv.svm &lt;- tune(svm, y ~ .,\n  data = sv.data, kernel = \"linear\",\n  ranges = list(cost = costs)\n)\nbest.svc &lt;- cv.svm$best.model\nx.test &lt;- matrix(rnorm(40), ncol = 2)\ny.test &lt;- sample(c(-1, 1), size = 20, replace = TRUE)\nx.test[y.test == 1, ] &lt;- x.test[y.test == 1, ] + 2\nsv.test.data &lt;- data.frame(\n  x = x.test,\n  y = as.factor(y.test)\n)\npred &lt;- predict(best.svc, newdata = sv.test.data)\nresult2 &lt;- table(\n  predicted = pred,\n  truth = sv.test.data$y\n)\nresult2\n\n         truth\npredicted -1  1\n       -1  7  0\n       1   1 12\n\n100 * (result2[1, 1] + result2[2, 2]) / sum(result2)\n\n[1] 95\n\n# Lastly, we fit a hyperplane with no misclassified observations\n# using a very high value of cost\nmmc.fit &lt;- svm(y ~ .,\n  data = sv.data, kernel = \"linear\",\n  cost = 1e5, scale = FALSE\n)\nplot(mmc.fit, data = sv.data)\n\n\n\n# Using a low value of cost\nsv1.fit &lt;- svm(y ~ .,\n  data = sv.data, kernel = \"linear\",\n  cost = 1, scale = FALSE\n)\nplot(sv1.fit, data = sv.data)"
  },
  {
    "objectID": "Chapter9l.html#support-vector-machine",
    "href": "Chapter9l.html#support-vector-machine",
    "title": "Chapter 9 (Lab)",
    "section": "9.6.2 Support Vector Machine",
    "text": "9.6.2 Support Vector Machine\nWe not fit a support vector machine model for classifying a data which decidedly has non-linear boundary between the two classes.\n\nlibrary(e1071)\n# Set seed and options\noptions(digits = 2)\nset.seed(3)\n\n# Creating a new data set with non-linear boundary\nx &lt;- matrix(rnorm(400), ncol = 2)\ny &lt;- c(rep(1, 150), rep(2, 50))\nx[1:100, ] &lt;- x[1:100, ] + 2\nx[101:150, ] &lt;- x[101:150, ] - 2\nsvm.data &lt;- data.frame(x = x, y = as.factor(y))\n\n# Plotting the data to see whether it has a non-linear boundary\nplot(\n  x = svm.data$x.1, y = svm.data$x.2, col = svm.data$y,\n  xlab = \"x1 (First variable)\", ylab = \"x2 (Second Variable)\",\n  main = \"Plot of 2-dimensional simulated data set (non-linear boundary)\",\n  pch = 20, cex = 1.5\n)\n\n\n\n# Plotting with ggplot2\nggplot(data = svm.data) +\n  geom_point(aes(x = x.1, y = x.2, col = y),\n    pch = 20, size = 4\n  ) +\n  labs(\n    x = \"x1 (First variable)\", y = \"x2 (Second Variable)\",\n    title = \"Plot of 2-dimensional simulated data set (non-linear separation boundary)\",\n    col = \"Class (y)\"\n  ) +\n  theme_bw()\n\n\n\n# Creating a training and test set using a boolean vector\ntrain &lt;- sample(c(TRUE, FALSE), size = 200, replace = TRUE)\ntest &lt;- !train\n\n# Fitting a Support Vector Machine model on the training set\nsvm.fit &lt;- svm(y ~ ., data = svm.data[train, ], kernel = \"radial\", gamma = 1, cost = 1)\nsummary(svm.fit)\n\n\nCall:\nsvm(formula = y ~ ., data = svm.data[train, ], kernel = \"radial\", \n    gamma = 1, cost = 1)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  36\n\n ( 21 15 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 1 2\n\nplot(svm.fit, data = svm.data[train, ])\n\n\n\n# Fitting a Support Vector Machine with high cost (minimal training errors)\nsvm.fit &lt;- svm(y ~ ., data = svm.data[train, ], kernel = \"radial\", gamma = 1, cost = 1e5)\nplot(svm.fit, data = svm.data[train, ])\n\n\n\n# Selecting the best value of cost and gamma using cross validation\ncv.svm.fit &lt;- tune(svm, y ~ .,\n  data = svm.data[train, ], kernel = \"radial\",\n  ranges = list(\n    cost = 10^(-2:3),\n    gamma = seq(from = 0.1, to = 10, length = 5)\n  )\n)\nbest.svm &lt;- cv.svm.fit$best.model\n\n# Predicting classes in test data using best model selected by cross validation\npred2 &lt;- predict(best.svm, newdata = svm.data[test, ])\ntab2 &lt;- table(predicted = pred2, truth = svm.data$y[test])\ntab2\n\n         truth\npredicted  1  2\n        1 68  6\n        2  7 23\n\n# Percentage mis-classified observations\n100 * (tab2[1, 2] + tab2[2, 1]) / sum(tab2)\n\n[1] 12"
  },
  {
    "objectID": "Chapter9l.html#roc-curves",
    "href": "Chapter9l.html#roc-curves",
    "title": "Chapter 9 (Lab)",
    "section": "9.6.3 ROC Curves",
    "text": "9.6.3 ROC Curves\nWe can now create ROC curves using the ROCR package in R. For this, we need to create a customized function rocplot() and use it to plot a support vector machine model from previous section with differing values of \\(\\gamma\\). We expect that at a high \\(\\gamma\\), the radial kernel will fit the training data very closely producing a near perfect ROC curve.\n\nlibrary(ROCR)\n# This package provides us two important functions:--\n# prediction() - Every classifier evaluation using ROCR starts with creating a\n# prediction object. This function is used to transform the input data (which\n# can be in vector, matrix, data frame, or list form) into a standardized format.\n#\n# performance() - All kinds of predictor evaluations are performed using this function.\n\n# Create a customized function to plot an ROC curve\nrocplot &lt;- function(pred, truth, ...) {\n  predob &lt;- prediction(pred, truth)\n  perf &lt;- performance(predob, \"tpr\", \"fpr\")\n  plot(perf, ...)\n}\n\n# Use fitted values in a new svm() object\nsvm.fit.roc1 &lt;- svm(y ~ .,\n  data = svm.data[train, ], kernel = \"radial\",\n  gamma = 2, cost = 1, decision.values = TRUE\n)\npred &lt;- attributes(predict(svm.fit.roc1,\n  newdata = svm.data[train, ],\n  decision.values = TRUE\n))$decision.values\npar(mfrow = c(1, 2))\nrocplot(\n  pred = pred, truth = svm.data[train, \"y\"],\n  main = \"Training Data\"\n)\n# Fitting training data with high value of gamma (more flexible fit)\nsvm.fit.roc2 &lt;- svm(y ~ .,\n  data = svm.data[train, ], kernel = \"radial\",\n  gamma = 50, cost = 1, decision.values = TRUE\n)\npred &lt;- attributes(predict(svm.fit.roc2,\n  newdata = svm.data[train, ],\n  decision.values = TRUE\n))$decision.values\nrocplot(\n  pred = pred, truth = svm.data[train, \"y\"],\n  add = TRUE, col = \"red\"\n)\nlegend(\"topleft\",\n  lty = c(1, 1), col = c(\"black\", \"red\"),\n  legend = c(\"gamma = 2\", \"gamma = 50\")\n)\n\n# Plotting ROC Curves for test data\npred &lt;- attributes(predict(svm.fit.roc1,\n  newdata = svm.data[test, ],\n  decision.values = TRUE\n))$decision.values\nrocplot(\n  pred = pred, truth = svm.data[test, \"y\"],\n  main = \"Test Data\"\n)\npred &lt;- attributes(predict(svm.fit.roc2,\n  newdata = svm.data[test, ],\n  decision.values = TRUE\n))$decision.values\nrocplot(\n  pred = pred, truth = svm.data[test, \"y\"],\n  add = TRUE, col = \"red\"\n)\nlegend(\"topleft\",\n  lty = c(1, 1), col = c(\"black\", \"red\"),\n  legend = c(\"gamma = 2\", \"gamma = 50\")\n)"
  },
  {
    "objectID": "Chapter9l.html#svm-with-multiple-classes",
    "href": "Chapter9l.html#svm-with-multiple-classes",
    "title": "Chapter 9 (Lab)",
    "section": "9.6.4 SVM with Multiple Classes",
    "text": "9.6.4 SVM with Multiple Classes\nWe can fit one-on-one approach based Support Vector Machines using the same svm() function from e1071 library.\n\n# Creating a three class data (adding 50 new observations to svm.data's 200 existing observations)\nx &lt;- matrix(rnorm(100), ncol = 2)\nx[, 1] &lt;- x[, 1] - 2\nx[, 2] &lt;- x[, 2] + 2\ny &lt;- rep(3, 10)\ndf1 &lt;- data.frame(x = x, y = as.factor(y))\nsvm.3.data &lt;- rbind(svm.data, df1)\n\n# Plotting the new data set\nplot(\n  x = svm.3.data$x.1, y = svm.3.data$x.2, col = svm.3.data$y,\n  xlab = \"x1 (First variable)\", ylab = \"x2 (Second Variable)\",\n  main = \"Plot of 2-dimensional simulated data set (non-linear boundary)\",\n  pch = 20, cex = 1.5\n)\n\n\n\n# Plotting with ggplot2\nggplot(data = svm.3.data) +\n  geom_point(aes(x = x.1, y = x.2, col = y),\n    pch = 20, size = 4\n  ) +\n  labs(\n    x = \"x1 (First variable)\", y = \"x2 (Second Variable)\",\n    title = \"Plot of 2-dimensional simulated data set (non-linear separation boundary)\",\n    col = \"Class (y)\"\n  ) +\n  theme_bw()\n\n\n\n# Fitting a Support Vector Machine model (plot exchanges x.1 and x.2)\nsvm.3.fit &lt;- svm(y ~ .,\n  data = svm.3.data, kernel = \"radial\", cost = 10,\n  gamma = 1\n)\nplot(svm.3.fit, data = svm.3.data)"
  },
  {
    "objectID": "Chapter9l.html#application-to-gene-expression-data",
    "href": "Chapter9l.html#application-to-gene-expression-data",
    "title": "Chapter 9 (Lab)",
    "section": "9.6.5 Application to Gene Expression Data",
    "text": "9.6.5 Application to Gene Expression Data\nWe will now use svm() on the Khan data set of gene expression from ISLR library.\n\n# Loading Data Set and examining it\nlibrary(ISLR)\ndata(\"Khan\")\nnames(Khan)\n\n[1] \"xtrain\" \"xtest\"  \"ytrain\" \"ytest\" \n\ndim(Khan$xtrain)\n\n[1]   63 2308\n\nlength(Khan$ytrain)\n\n[1] 63\n\ndim(Khan$xtest)\n\n[1]   20 2308\n\nlength(Khan$ytest)\n\n[1] 20\n\n# Checking the number of classes of cells in two data sets: Testing and Training\ntable(Khan$ytrain)\n\n\n 1  2  3  4 \n 8 23 12 20 \n\ntable(Khan$ytest)\n\n\n1 2 3 4 \n3 6 6 5 \n\n# Fitting an svm() with linear kernel (as there are already more variables than obsv.)\nkhan.train &lt;- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))\nsvm.khan &lt;- svm(y ~ ., data = khan.train, kernel = \"linear\", cost = 10)\nsummary(svm.khan)\n\n\nCall:\nsvm(formula = y ~ ., data = khan.train, kernel = \"linear\", cost = 10)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  58\n\n ( 20 20 11 7 )\n\n\nNumber of Classes:  4 \n\nLevels: \n 1 2 3 4\n\n# Confusion matrix of predictions on training data - perfect match\ntable(svm.khan$fitted, khan.train$y)\n\n   \n     1  2  3  4\n  1  8  0  0  0\n  2  0 23  0  0\n  3  0  0 12  0\n  4  0  0  0 20\n\n# Checking performance on test data\nkhan.test &lt;- data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))\npred &lt;- predict(svm.khan, newdata = khan.test)\nt &lt;- table(pred, khan.test$y)\n\n# Error rate on test observations\n(1 - sum(diag(t)) / sum(t))\n\n[1] 0.1"
  },
  {
    "objectID": "Chapter8l.html",
    "href": "Chapter8l.html",
    "title": "Chapter 8 (Lab)",
    "section": "",
    "text": "# Loading libraries and data set\nlibrary(tree)\nlibrary(ISLR)\nlibrary(tidyverse)\ndata(\"Carseats\")\noptions(digits = 2)\n# Creating a binary variable for Sales; removing Sales variable from data set\nCarseats &lt;- Carseats %&gt;%\n  mutate(SalesHigh = as.factor(ifelse(Sales &gt; 8, yes = \"Yes\", no = \"No\"))) %&gt;%\n  select(-Sales)\n\n# Fitting a classification tree model to the data set for predicting SalesHigh\ntree.carseats &lt;- tree(SalesHigh ~ ., data = Carseats)\n\n# Displaying Summary fo Classification Tree\nsummary(tree.carseats)\n\n\nClassification tree:\ntree(formula = SalesHigh ~ ., data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.46 = 171 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n\n# Plotting the Classification tree\nplot(tree.carseats, col = \"brown\", type = \"proportional\")\ntext(tree.carseats, pretty = 0, cex = 0.5)\n\n\n\n# Examining the output from tree object print\nnames(tree.carseats)\n\n[1] \"frame\"   \"where\"   \"terms\"   \"call\"    \"y\"       \"weights\"\n\nnames(tree.carseats$frame)\n\n[1] \"var\"    \"n\"      \"dev\"    \"yval\"   \"splits\" \"yprob\" \n\n# Creating a testing and training set in Carseats Data Set\nnrow(Carseats)\n\n[1] 400\n\nset.seed(3)\ntrain &lt;- sample(c(TRUE, FALSE), size = 200, replace = TRUE)\nTest.Carseats &lt;- Carseats[!train, ]\nTruth &lt;- Carseats[!train, \"SalesHigh\"]\n\n# Fitting a classification tree to the training data\ntree.carseats &lt;- tree(SalesHigh ~ ., data = Carseats, subset = train)\n\n# Obtaining predicted values in test data\npred &lt;- predict(tree.carseats, newdata = Test.Carseats, type = \"class\")\n\n# Creating Confusion Matrix; calculating test error rate\ntable(Predicted = pred, Truth)\n\n         Truth\nPredicted No Yes\n      No  95  55\n      Yes 22  42\n\n# Prediction Accuracy\nmean(pred == Truth)\n\n[1] 0.64\n\n# Test error rate\nmean(pred != Truth)\n\n[1] 0.36\n\n# Now, we prune the Classification Tree, finding the optimum pruning by C.V.\ncv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass)\n\n# Examining the Cross Validation Pruning Tree Object\nnames(cv.carseats)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.carseats\n\n$size\n[1] 18 13  8  7  5  4  2  1\n\n$dev\n[1] 65 65 64 63 65 71 78 68\n\n$k\n[1] -Inf  0.0  1.0  2.0  3.0  5.0  6.5 13.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n# Plotting the error rate as a fucntion of k (alpha) and size of tree (size)\npar(mfrow = c(1, 2))\nplot(\n  x = cv.carseats$size, y = cv.carseats$dev, type = \"b\",\n  ylab = \"Error Rate\", xlab = \"Size of pruned tree\"\n)\npoints(\n  x = 7, y = min(cv.carseats$dev), col = \"red\",\n  pch = 20, cex = 1.5\n)\nabline(v = 7, col = \"red\", lty = 2)\nplot(\n  x = cv.carseats$k, y = cv.carseats$dev, type = \"b\",\n  ylab = \"Error Rate\", xlab = \"alpha (k)\"\n)\npoints(\n  x = 2, y = min(cv.carseats$dev), col = \"red\",\n  pch = 20, cex = 1.5\n)\nabline(v = 2, col = \"red\", lty = 2)\n\n\n\n# Pruning the tree at the best number of variables (i.e. 7)\nprune.carseats &lt;- prune.tree(tree.carseats, best = 7)\n\n# Plotting the best (as per C.V.) pruned classification tree\nplot(prune.carseats, col = \"brown\")\ntext(prune.carseats, cex = 0.9, pretty = 0)\n\n# Calculating the test error rate for the best pruned classification tree\npred &lt;- predict(prune.carseats, newdata = Test.Carseats, type = \"class\")\ntable(Predicted = pred, Truth)\n\n         Truth\nPredicted  No Yes\n      No  107  49\n      Yes  10  48\n\n# Prediction Accuracy\nmean(pred == Truth)\n\n[1] 0.72\n\n# Test error rate\nmean(pred != Truth)\n\n[1] 0.28\n\n\n\n\n\n\n\n\n\n# Loading libraries and data set\nlibrary(MASS)\nlibrary(tree)\ndata(Boston)\ndim(Boston)\n\n[1] 506  14\n\n# creating a training and test subset of the data\nset.seed(3)\ntrain &lt;- sample(c(TRUE, FALSE), size = nrow(Boston) / 2, replace = TRUE)\nTruth &lt;- Boston[!train, \"medv\"]\n\n# Fitting a regression tree on the training data set\ntree.boston &lt;- tree(medv ~ ., data = Boston, subset = train)\n\n# Displaying the fitted regression tree\nsummary(tree.boston)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"lstat\" \"rm\"    \"nox\"   \"crim\" \nNumber of terminal nodes:  9 \nResidual mean deviance:  13 = 2910 / 223 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -12.5    -2.0    -0.1     0.0     2.0    15.0 \n\nnames(tree.boston)\n\n[1] \"frame\"   \"where\"   \"terms\"   \"call\"    \"y\"       \"weights\"\n\nnames(tree.boston$frame)\n\n[1] \"var\"    \"n\"      \"dev\"    \"yval\"   \"splits\"\n\n# Plotting the regression tree\nplot(tree.boston, col = \"brown\")\ntext(tree.boston, pretty = 0, cex = 0.8)\n\n\n\n# Finding the best pruned regression tree using cross validation\ncv.boston &lt;- cv.tree(tree.boston, K = 10)\ncv.boston\n\n$size\n[1] 9 8 7 6 5 4 3 2 1\n\n$dev\n[1]  6092  6337  6701  6950  7558  7781  9069 12520 18862\n\n$k\n[1] -Inf  247  329  460  664  783 1588 3077 8702\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\nplot(\n  x = cv.boston$size, y = cv.boston$dev, xlab = \"No. of variables\",\n  ylab = \"Cross-Validation Deviance\", type = \"b\"\n)\n\n\n\n# Using the unpruned tree to make preductions on the test data set\npred &lt;- predict(tree.boston, newdata = Boston[!train, ])\n\n# Plotting Predicted vs. True values of medv\nplot(\n  x = Truth, y = pred, xlab = \"True values of medv\",\n  ylab = \"Predicted values of medv\"\n)\nabline(a = 0, b = 1, col = \"red\")\n\n\n\n# Calculating Test MSE\noptions(digits = 4)\nmean((pred - Truth)^2)\n\n[1] 21.18\n\nsqrt(mean((pred - Truth)^2))\n\n[1] 4.602\n\n\n\n\n\n\n# Loading libraries and Data Set\nlibrary(MASS)\nlibrary(randomForest)\ndata(Boston)\nset.seed(3)\n\n# Fitting a bagging model with randomForest() with m = p\nlength(names(Boston))\n\n[1] 14\n\nbag.boston &lt;- randomForest(medv ~ .,\n  data = Boston, subset = train,\n  mtry = 13, importance = TRUE\n)\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 13, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 13\n\n          Mean of squared residuals: 15.6\n                    % Var explained: 80.71\n\n# Calculating test set predictions and plotting them\npred &lt;- predict(bag.boston, newdata = Boston[!train, ])\nplot(\n  x = Truth, y = pred, xlab = \"True response values (medv)\",\n  ylab = \"Predicted response values (medv)\"\n)\nabline(a = 0, b = 1, col = \"red\")\n\n\n\n# Calculating the Test MSE\nmean((pred - Truth)^2)\n\n[1] 13.26\n\n# Fitting a Random Forest Model with 6 variables mtry\nrf.boston &lt;- randomForest(medv ~ .,\n  data = Boston, subset = train,\n  mtry = 6, importance = TRUE\n)\nrf.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 6, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 6\n\n          Mean of squared residuals: 13.74\n                    % Var explained: 83.01\n\n# Calculating test set predictions and plotting them\npred &lt;- predict(rf.boston, newdata = Boston[!train, ])\nplot(\n  x = Truth, y = pred, xlab = \"True response values (medv)\",\n  ylab = \"Predicted response values (medv)\"\n)\nabline(a = 0, b = 1, col = \"red\")\n\n\n\n# Calculating the Test MSE\nmean((pred - Truth)^2)\n\n[1] 12.44\n\n# Showing the importance of variables as table and plot\nround(importance(rf.boston), 2)\n\n        %IncMSE IncNodePurity\ncrim      11.18       1082.37\nzn         3.25        117.71\nindus      8.77        866.36\nchas       0.01         21.87\nnox       13.55       1022.72\nrm        30.67       5883.64\nage       10.69        504.33\ndis       10.23        760.49\nrad        3.93        124.16\ntax        7.53        637.97\nptratio    9.17        654.75\nblack      8.66        330.35\nlstat     31.83       6239.90\n\nvarImpPlot(rf.boston)\n\n\n\n\n\n\n\n\n# Loading libraries\nlibrary(gbm)\nset.seed(3)\n\n# Fitting a boosting model\nboost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\",\n                    n.trees = 5000, interaction.depth = 4)\nboost.boston\n\ngbm(formula = medv ~ ., distribution = \"gaussian\", data = Boston[train, \n    ], n.trees = 5000, interaction.depth = 4)\nA gradient boosted model with gaussian loss function.\n5000 iterations were performed.\nThere were 13 predictors of which 13 had non-zero influence.\n\nnames(boost.boston)\n\n [1] \"initF\"             \"fit\"               \"train.error\"      \n [4] \"valid.error\"       \"oobag.improve\"     \"trees\"            \n [7] \"c.splits\"          \"bag.fraction\"      \"distribution\"     \n[10] \"interaction.depth\" \"n.minobsinnode\"    \"num.classes\"      \n[13] \"n.trees\"           \"nTrain\"            \"train.fraction\"   \n[16] \"response.name\"     \"shrinkage\"         \"var.levels\"       \n[19] \"var.monotone\"      \"var.names\"         \"var.type\"         \n[22] \"verbose\"           \"data\"              \"Terms\"            \n[25] \"cv.folds\"          \"call\"              \"m\"                \n\nsummary(boost.boston)\n\n\n\n\n            var  rel.inf\nlstat     lstat 35.06153\nrm           rm 32.09065\ndis         dis  6.92124\ncrim       crim  6.19716\nnox         nox  4.33675\nage         age  4.31233\nblack     black  3.94817\nptratio ptratio  2.51777\ntax         tax  2.22554\nindus     indus  1.18808\nrad         rad  0.86941\nzn           zn  0.25900\nchas       chas  0.07236\n\n# Plotting the marginal effects of selected variables on the response\npar(mfrow = c(1,2))\nplot(boost.boston, i = \"rm\")\n\n\n\nplot(boost.boston, i = \"lstat\")\n\n\n\n# Use boosted model to predict the medv on test data set\npred &lt;- predict(boost.boston, newdata = Boston[!train,])\n\n# Calculating the test MSE\nmean((pred - Truth)^2)\n\n[1] 13.89\n\n# Using boosting model with differnt lambda value = 0.2 (instead of default 0.001)\nboost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\",\n                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.2)\npred &lt;- predict(boost.boston, newdata = Boston[!train,])\nmean((pred - Truth)^2)\n\n[1] 13.32"
  },
  {
    "objectID": "Chapter8l.html#fitting-classification-trees",
    "href": "Chapter8l.html#fitting-classification-trees",
    "title": "Chapter 8 (Lab)",
    "section": "",
    "text": "# Loading libraries and data set\nlibrary(tree)\nlibrary(ISLR)\nlibrary(tidyverse)\ndata(\"Carseats\")\noptions(digits = 2)\n# Creating a binary variable for Sales; removing Sales variable from data set\nCarseats &lt;- Carseats %&gt;%\n  mutate(SalesHigh = as.factor(ifelse(Sales &gt; 8, yes = \"Yes\", no = \"No\"))) %&gt;%\n  select(-Sales)\n\n# Fitting a classification tree model to the data set for predicting SalesHigh\ntree.carseats &lt;- tree(SalesHigh ~ ., data = Carseats)\n\n# Displaying Summary fo Classification Tree\nsummary(tree.carseats)\n\n\nClassification tree:\ntree(formula = SalesHigh ~ ., data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.46 = 171 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n\n# Plotting the Classification tree\nplot(tree.carseats, col = \"brown\", type = \"proportional\")\ntext(tree.carseats, pretty = 0, cex = 0.5)\n\n\n\n# Examining the output from tree object print\nnames(tree.carseats)\n\n[1] \"frame\"   \"where\"   \"terms\"   \"call\"    \"y\"       \"weights\"\n\nnames(tree.carseats$frame)\n\n[1] \"var\"    \"n\"      \"dev\"    \"yval\"   \"splits\" \"yprob\" \n\n# Creating a testing and training set in Carseats Data Set\nnrow(Carseats)\n\n[1] 400\n\nset.seed(3)\ntrain &lt;- sample(c(TRUE, FALSE), size = 200, replace = TRUE)\nTest.Carseats &lt;- Carseats[!train, ]\nTruth &lt;- Carseats[!train, \"SalesHigh\"]\n\n# Fitting a classification tree to the training data\ntree.carseats &lt;- tree(SalesHigh ~ ., data = Carseats, subset = train)\n\n# Obtaining predicted values in test data\npred &lt;- predict(tree.carseats, newdata = Test.Carseats, type = \"class\")\n\n# Creating Confusion Matrix; calculating test error rate\ntable(Predicted = pred, Truth)\n\n         Truth\nPredicted No Yes\n      No  95  55\n      Yes 22  42\n\n# Prediction Accuracy\nmean(pred == Truth)\n\n[1] 0.64\n\n# Test error rate\nmean(pred != Truth)\n\n[1] 0.36\n\n# Now, we prune the Classification Tree, finding the optimum pruning by C.V.\ncv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass)\n\n# Examining the Cross Validation Pruning Tree Object\nnames(cv.carseats)\n\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n\ncv.carseats\n\n$size\n[1] 18 13  8  7  5  4  2  1\n\n$dev\n[1] 65 65 64 63 65 71 78 68\n\n$k\n[1] -Inf  0.0  1.0  2.0  3.0  5.0  6.5 13.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\n# Plotting the error rate as a fucntion of k (alpha) and size of tree (size)\npar(mfrow = c(1, 2))\nplot(\n  x = cv.carseats$size, y = cv.carseats$dev, type = \"b\",\n  ylab = \"Error Rate\", xlab = \"Size of pruned tree\"\n)\npoints(\n  x = 7, y = min(cv.carseats$dev), col = \"red\",\n  pch = 20, cex = 1.5\n)\nabline(v = 7, col = \"red\", lty = 2)\nplot(\n  x = cv.carseats$k, y = cv.carseats$dev, type = \"b\",\n  ylab = \"Error Rate\", xlab = \"alpha (k)\"\n)\npoints(\n  x = 2, y = min(cv.carseats$dev), col = \"red\",\n  pch = 20, cex = 1.5\n)\nabline(v = 2, col = \"red\", lty = 2)\n\n\n\n# Pruning the tree at the best number of variables (i.e. 7)\nprune.carseats &lt;- prune.tree(tree.carseats, best = 7)\n\n# Plotting the best (as per C.V.) pruned classification tree\nplot(prune.carseats, col = \"brown\")\ntext(prune.carseats, cex = 0.9, pretty = 0)\n\n# Calculating the test error rate for the best pruned classification tree\npred &lt;- predict(prune.carseats, newdata = Test.Carseats, type = \"class\")\ntable(Predicted = pred, Truth)\n\n         Truth\nPredicted  No Yes\n      No  107  49\n      Yes  10  48\n\n# Prediction Accuracy\nmean(pred == Truth)\n\n[1] 0.72\n\n# Test error rate\nmean(pred != Truth)\n\n[1] 0.28"
  },
  {
    "objectID": "Chapter8l.html#fitting-regression-trees",
    "href": "Chapter8l.html#fitting-regression-trees",
    "title": "Chapter 8 (Lab)",
    "section": "",
    "text": "# Loading libraries and data set\nlibrary(MASS)\nlibrary(tree)\ndata(Boston)\ndim(Boston)\n\n[1] 506  14\n\n# creating a training and test subset of the data\nset.seed(3)\ntrain &lt;- sample(c(TRUE, FALSE), size = nrow(Boston) / 2, replace = TRUE)\nTruth &lt;- Boston[!train, \"medv\"]\n\n# Fitting a regression tree on the training data set\ntree.boston &lt;- tree(medv ~ ., data = Boston, subset = train)\n\n# Displaying the fitted regression tree\nsummary(tree.boston)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"lstat\" \"rm\"    \"nox\"   \"crim\" \nNumber of terminal nodes:  9 \nResidual mean deviance:  13 = 2910 / 223 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -12.5    -2.0    -0.1     0.0     2.0    15.0 \n\nnames(tree.boston)\n\n[1] \"frame\"   \"where\"   \"terms\"   \"call\"    \"y\"       \"weights\"\n\nnames(tree.boston$frame)\n\n[1] \"var\"    \"n\"      \"dev\"    \"yval\"   \"splits\"\n\n# Plotting the regression tree\nplot(tree.boston, col = \"brown\")\ntext(tree.boston, pretty = 0, cex = 0.8)\n\n\n\n# Finding the best pruned regression tree using cross validation\ncv.boston &lt;- cv.tree(tree.boston, K = 10)\ncv.boston\n\n$size\n[1] 9 8 7 6 5 4 3 2 1\n\n$dev\n[1]  6092  6337  6701  6950  7558  7781  9069 12520 18862\n\n$k\n[1] -Inf  247  329  460  664  783 1588 3077 8702\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\nplot(\n  x = cv.boston$size, y = cv.boston$dev, xlab = \"No. of variables\",\n  ylab = \"Cross-Validation Deviance\", type = \"b\"\n)\n\n\n\n# Using the unpruned tree to make preductions on the test data set\npred &lt;- predict(tree.boston, newdata = Boston[!train, ])\n\n# Plotting Predicted vs. True values of medv\nplot(\n  x = Truth, y = pred, xlab = \"True values of medv\",\n  ylab = \"Predicted values of medv\"\n)\nabline(a = 0, b = 1, col = \"red\")\n\n\n\n# Calculating Test MSE\noptions(digits = 4)\nmean((pred - Truth)^2)\n\n[1] 21.18\n\nsqrt(mean((pred - Truth)^2))\n\n[1] 4.602"
  },
  {
    "objectID": "Chapter8l.html#bagging-and-random-forests",
    "href": "Chapter8l.html#bagging-and-random-forests",
    "title": "Chapter 8 (Lab)",
    "section": "",
    "text": "# Loading libraries and Data Set\nlibrary(MASS)\nlibrary(randomForest)\ndata(Boston)\nset.seed(3)\n\n# Fitting a bagging model with randomForest() with m = p\nlength(names(Boston))\n\n[1] 14\n\nbag.boston &lt;- randomForest(medv ~ .,\n  data = Boston, subset = train,\n  mtry = 13, importance = TRUE\n)\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 13, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 13\n\n          Mean of squared residuals: 15.6\n                    % Var explained: 80.71\n\n# Calculating test set predictions and plotting them\npred &lt;- predict(bag.boston, newdata = Boston[!train, ])\nplot(\n  x = Truth, y = pred, xlab = \"True response values (medv)\",\n  ylab = \"Predicted response values (medv)\"\n)\nabline(a = 0, b = 1, col = \"red\")\n\n\n\n# Calculating the Test MSE\nmean((pred - Truth)^2)\n\n[1] 13.26\n\n# Fitting a Random Forest Model with 6 variables mtry\nrf.boston &lt;- randomForest(medv ~ .,\n  data = Boston, subset = train,\n  mtry = 6, importance = TRUE\n)\nrf.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 6, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 6\n\n          Mean of squared residuals: 13.74\n                    % Var explained: 83.01\n\n# Calculating test set predictions and plotting them\npred &lt;- predict(rf.boston, newdata = Boston[!train, ])\nplot(\n  x = Truth, y = pred, xlab = \"True response values (medv)\",\n  ylab = \"Predicted response values (medv)\"\n)\nabline(a = 0, b = 1, col = \"red\")\n\n\n\n# Calculating the Test MSE\nmean((pred - Truth)^2)\n\n[1] 12.44\n\n# Showing the importance of variables as table and plot\nround(importance(rf.boston), 2)\n\n        %IncMSE IncNodePurity\ncrim      11.18       1082.37\nzn         3.25        117.71\nindus      8.77        866.36\nchas       0.01         21.87\nnox       13.55       1022.72\nrm        30.67       5883.64\nage       10.69        504.33\ndis       10.23        760.49\nrad        3.93        124.16\ntax        7.53        637.97\nptratio    9.17        654.75\nblack      8.66        330.35\nlstat     31.83       6239.90\n\nvarImpPlot(rf.boston)"
  },
  {
    "objectID": "Chapter8l.html#boosting",
    "href": "Chapter8l.html#boosting",
    "title": "Chapter 8 (Lab)",
    "section": "",
    "text": "# Loading libraries\nlibrary(gbm)\nset.seed(3)\n\n# Fitting a boosting model\nboost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\",\n                    n.trees = 5000, interaction.depth = 4)\nboost.boston\n\ngbm(formula = medv ~ ., distribution = \"gaussian\", data = Boston[train, \n    ], n.trees = 5000, interaction.depth = 4)\nA gradient boosted model with gaussian loss function.\n5000 iterations were performed.\nThere were 13 predictors of which 13 had non-zero influence.\n\nnames(boost.boston)\n\n [1] \"initF\"             \"fit\"               \"train.error\"      \n [4] \"valid.error\"       \"oobag.improve\"     \"trees\"            \n [7] \"c.splits\"          \"bag.fraction\"      \"distribution\"     \n[10] \"interaction.depth\" \"n.minobsinnode\"    \"num.classes\"      \n[13] \"n.trees\"           \"nTrain\"            \"train.fraction\"   \n[16] \"response.name\"     \"shrinkage\"         \"var.levels\"       \n[19] \"var.monotone\"      \"var.names\"         \"var.type\"         \n[22] \"verbose\"           \"data\"              \"Terms\"            \n[25] \"cv.folds\"          \"call\"              \"m\"                \n\nsummary(boost.boston)\n\n\n\n\n            var  rel.inf\nlstat     lstat 35.06153\nrm           rm 32.09065\ndis         dis  6.92124\ncrim       crim  6.19716\nnox         nox  4.33675\nage         age  4.31233\nblack     black  3.94817\nptratio ptratio  2.51777\ntax         tax  2.22554\nindus     indus  1.18808\nrad         rad  0.86941\nzn           zn  0.25900\nchas       chas  0.07236\n\n# Plotting the marginal effects of selected variables on the response\npar(mfrow = c(1,2))\nplot(boost.boston, i = \"rm\")\n\n\n\nplot(boost.boston, i = \"lstat\")\n\n\n\n# Use boosted model to predict the medv on test data set\npred &lt;- predict(boost.boston, newdata = Boston[!train,])\n\n# Calculating the test MSE\nmean((pred - Truth)^2)\n\n[1] 13.89\n\n# Using boosting model with differnt lambda value = 0.2 (instead of default 0.001)\nboost.boston &lt;- gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\",\n                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.2)\npred &lt;- predict(boost.boston, newdata = Boston[!train,])\nmean((pred - Truth)^2)\n\n[1] 13.32"
  },
  {
    "objectID": "Chapter10l.html",
    "href": "Chapter10l.html",
    "title": "Chapter 10 (Lab)",
    "section": "",
    "text": "In this lab, we use the USArrests data set in base R and do a principal components analysis using prcomp() function of base R.\n\n# Loading data\ndata(\"USArrests\")\noptions(digits = 2)\n\n# Examining data\ndim(USArrests)\n\n[1] 50  4\n\n# Names of variables\ncolnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n# Names of States\nrow.names(USArrests)\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\n# Computing means and variances of variables\ncolMeans(USArrests)\n\n  Murder  Assault UrbanPop     Rape \n     7.8    170.8     65.5     21.2 \n\napply(X = USArrests, MARGIN = 2, FUN = var)\n\n  Murder  Assault UrbanPop     Rape \n      19     6945      210       88 \n\n# Performing Principal Components Analysis\nprUSA &lt;- prcomp(USArrests, scale = TRUE)\n\n# Examining contents of output\nclass(prUSA)\n\n[1] \"prcomp\"\n\nnames(prUSA)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\nprUSA$center # means of original variables\n\n  Murder  Assault UrbanPop     Rape \n     7.8    170.8     65.5     21.2 \n\nprUSA$scale # standard deviation of original variables\n\n  Murder  Assault UrbanPop     Rape \n     4.4     83.3     14.5      9.4 \n\nprUSA$scale^2 # variance of original variables\n\n  Murder  Assault UrbanPop     Rape \n      19     6945      210       88 \n\n# The Principal Component Loading Vectors (columns of Rotation Matrix)\nprUSA$rotation\n\n           PC1   PC2   PC3    PC4\nMurder   -0.54 -0.42  0.34  0.649\nAssault  -0.58 -0.19  0.27 -0.743\nUrbanPop -0.28  0.87  0.38  0.134\nRape     -0.54  0.17 -0.82  0.089\n\n# The Principal Component Score Vectors (columns of x matrix)\nhead(prUSA$x, 2)\n\n          PC1  PC2   PC3   PC4\nAlabama -0.98 -1.1  0.44  0.15\nAlaska  -1.93 -1.1 -2.02 -0.43\n\n# Standard Deviation of Principal Component Score Vectors\nprUSA$sdev\n\n[1] 1.57 0.99 0.60 0.42\n\n# Computing Proportion of variance explained\npr.var &lt;- prUSA$sdev^2 / sum(prUSA$sdev^2)\npr.var\n\n[1] 0.620 0.247 0.089 0.043\n\n# Creating a Scree Plot and a Cumulative Variance Explained plot\npar(mfrow = c(1, 2))\nplot(pr.var,\n  type = \"b\", xlab = \"Principal Component\",\n  ylab = \"Proportion of Variance Explained\", ylim = c(0, 1)\n)\nplot(cumsum(pr.var),\n  type = \"b\", xlab = \"Principal Component\",\n  ylab = \"Cumulative Prop. of Variance Explained\", ylim = c(0, 1)\n)\n\n\n\n# Creating a biplot\npar(mfrow = c(1, 1))\nbiplot(prUSA, scale = 0)"
  },
  {
    "objectID": "Chapter10l.html#k-means-clustering",
    "href": "Chapter10l.html#k-means-clustering",
    "title": "Chapter 10 (Lab)",
    "section": "10.5.1 K-Means Clustering",
    "text": "10.5.1 K-Means Clustering\n\n# Generate a simulated data set matrix (25 X 2) with two clusters\nset.seed(3)\nx &lt;- matrix(rnorm(100), ncol = 2)\nx[1:25, 1] &lt;- x[1:25, 1] + 3\nx[1:25, 2] &lt;- x[1:25, 2] - 4\n\n# Perform K Means Clustering with K = 2\nkm2 &lt;- kmeans(x = x, centers = 2, nstart = 20)\nkm2\n\nK-means clustering with 2 clusters of sizes 25, 25\n\nCluster means:\n  [,1]  [,2]\n1 0.16  0.12\n2 2.71 -3.95\n\nClustering vector:\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 37 32\n (between_SS / total_SS =  80.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n# Displaying identified clusters and plotting them\nkm2$cluster\n\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1\n\nplot(x,\n  col = (km2$cluster + 1), pch = 20, xlab = \"\", ylab = \"\", cex = 1.5,\n  main = \"K-means clustering results with K = 2\"\n)\n\n\n\n# Performing K-means clustering with K = 3\nkm3 &lt;- kmeans(x = x, centers = 3, nstart = 20)\nkm3\n\nK-means clustering with 3 clusters of sizes 12, 13, 25\n\nCluster means:\n   [,1]   [,2]\n1 -0.68  0.017\n2  0.93  0.221\n3  2.71 -3.951\n\nClustering vector:\n [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 2 2 1 1 2 2 2 2 1 2 2 1\n[39] 1 2 2 1 2 1 2 1 1 2 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 10.7  9.9 32.3\n (between_SS / total_SS =  85.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nplot(x,\n  col = (km3$cluster + 1), pch = 20, xlab = \"\", ylab = \"\", cex = 1.5,\n  main = \"K-means clustering results with K = 3\"\n)\n\n\n\n# Demonstrating the use of nstart argument\nkm3_1 &lt;- kmeans(x = x, centers = 3, nstart = 1)\nkm3$tot.withinss\n\n[1] 53\n\nkm3_1$tot.withinss\n\n[1] 53"
  },
  {
    "objectID": "Chapter10l.html#hierarchical-clustering",
    "href": "Chapter10l.html#hierarchical-clustering",
    "title": "Chapter 10 (Lab)",
    "section": "10.5.2 Hierarchical Clustering",
    "text": "10.5.2 Hierarchical Clustering\n\n# Creating a distance matrix with dist()\noptions(digits = 2)\nclass(dist(x))\n\n[1] \"dist\"\n\n# Using hclust() to do hierarchical clustering in R on same data set\n# Three different linkage methods\nhc.complete &lt;- hclust(d = dist(x), method = \"complete\")\nhc.average &lt;- hclust(d = dist(x), method = \"average\")\nhc.single &lt;- hclust(d = dist(x), method = \"single\")\n\n# Plotting the dendrograms from each linkage method\npar(mfrow = c(1, 3))\nplot(hc.complete,\n  cex = 0.7, xlab = \"\", ylab = \"\", sub = \"\",\n  main = \"Complete Linkage\"\n)\nplot(hc.average,\n  cex = 0.7, xlab = \"\", ylab = \"\", sub = \"\",\n  main = \"Average Linkage\"\n)\nplot(hc.single,\n  cex = 0.7, xlab = \"\", ylab = \"\", sub = \"\",\n  main = \"Single Linkage\"\n)\n\n\n\n# Comparing clusters generated using cutree() function\ncutree(tree = hc.complete, k = 2)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\ncutree(tree = hc.average, k = 2)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\ncutree(tree = hc.single, k = 2)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\ncutree(tree = hc.complete, k = 2) == cutree(tree = hc.average, k = 2)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE\n\ncutree(tree = hc.complete, k = 2) == cutree(tree = hc.single, k = 2)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE\n\n# Scaling variables before performing hierarchical clustering\nxsc &lt;- scale(x)\nplot(hclust(d = dist(xsc), method = \"complete\"),\n  cex = 0.7, xlab = \"\",\n  ylab = \"\", sub = \"\", main = \"Complete Linkage and Scaled Features\"\n)\n# Using correlation based distance\n\n# Simulated data set of 3 dimensions (with three clusters)\nx &lt;- matrix(rnorm(90), ncol = 3)\nx[1:10, ] &lt;- x[1:10, ] - 2\nx[20:30, ] &lt;- x[20:30, ] + 2\n\n# Euclidean distance based clustering (Partial success in both methods)\nkmeans(x = x, centers = 3, nstart = 20)$cluster\n\n [1] 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 2 2 1 2 2 2 2 2 2 2 2 2 2 2\n\ncutree(hclust(d = dist(x), method = \"complete\"), k = 3)\n\n [1] 1 1 1 1 1 1 1 1 1 1 2 3 2 2 2 1 3 3 2 3 3 3 3 3 3 3 3 3 3 2\n\n# Creating a matrix of correlation based distance\ndd &lt;- as.dist(m = 1 - cor(t(x)))\npar(mfrow = c(1, 1))\n\n\n\nplot(hclust(dd, method = \"complete\"),\n  xlab = \"\", sub = \"\",\n  main = \"Complete Linkage with Correlation Based Distance\"\n)"
  },
  {
    "objectID": "Chapter10l.html#pca-on-nci60-data",
    "href": "Chapter10l.html#pca-on-nci60-data",
    "title": "Chapter 10 (Lab)",
    "section": "PCA on NCI60 data",
    "text": "PCA on NCI60 data\nIn this lab, we use th NCI60 cancer cell line micro-array data from the ISLR package. We will perform K-Means Clustering and Hierarchical Clustering on the data set.\n\n# Loading data set and storing it locally\nlibrary(ISLR)\nlibrary(tidyverse)\nnames(NCI60)\n\n[1] \"data\" \"labs\"\n\nnci.data &lt;- NCI60$data\nnci.labs &lt;- NCI60$labs\n\ndim(nci.data)\n\n[1]   64 6830\n\nlength(nci.labs)\n\n[1] 64\n\nnci.data[1:5, 1:5]\n\n      1     2     3     4     5\nV1 0.30  1.18  0.55  1.14 -0.26\nV2 0.68  1.29  0.17  0.38  0.46\nV3 0.94 -0.04 -0.17 -0.04 -0.60\nV4 0.28 -0.31  0.68 -0.81  0.62\nV5 0.48 -0.46  0.40  0.90  0.20\n\nn_distinct(nci.labs)\n\n[1] 14\n\n# Performing PCA on the NCI60 data set\nprnci &lt;- prcomp(x = nci.data, scale = TRUE)\n\n# Plotting the data on first two principal components\n# creating a customized function to color the observation spoints\nCols &lt;- function(vec) {\n  cols &lt;- rainbow(length(unique(vec)))\n  return(cols[as.numeric(as.factor(vec))])\n}\npar(mfrow = c(1, 2))\nplot(prnci$x[, 1:2],\n  col = Cols(nci.labs),\n  pch = 19, xlab = \"Z1\", ylab = \"Z2\"\n)\nplot(prnci$x[, c(1, 3)],\n  col = Cols(nci.labs),\n  pch = 19, xlab = \"Z1\", ylab = \"Z3\"\n)\n\n\n\n# Plotting the Scree Plot and Proportion of Variance Explained\npar(mfrow = c(1, 3))\nplot(prnci, main = \"Default plot : prcomp()\")\n\npve &lt;- 100 * (prnci$sdev^2) / sum(prnci$sdev^2)\nplot(pve,\n  xlab = \"Principal Component\", ylab = \"Proportion Variance Explained\",\n  main = \"Scree Plot\", col = \"blue\", type = \"o\"\n)\nplot(cumsum(pve),\n  xlab = \"Principal Component\", ylab = \"Cumulative Prop. Var. Explained\",\n  main = \"Cum. Prop. Var. Explained\", col = \"brown3\", type = \"o\"\n)\n\n\n\n# Examining summary of a prcomp() object\nsummary(prnci)$importance[1:3, 1:7]\n\n                         PC1    PC2    PC3    PC4    PC5    PC6    PC7\nStandard deviation     27.85 21.481 19.820 17.033 15.972 15.721 14.471\nProportion of Variance  0.11  0.068  0.058  0.042  0.037  0.036  0.031\nCumulative Proportion   0.11  0.181  0.239  0.281  0.318  0.355  0.385"
  },
  {
    "objectID": "Chapter10l.html#clustering-on-observations-of-the-nci60-data",
    "href": "Chapter10l.html#clustering-on-observations-of-the-nci60-data",
    "title": "Chapter 10 (Lab)",
    "section": "Clustering on observations of the NCI60 data",
    "text": "Clustering on observations of the NCI60 data\n\n# Scaling the data\nsd.data &lt;- scale(nci.data)\n\n# Ploting hierarchical clustering on three different linkage methods\ndata.dist &lt;- dist(sd.data)\n\npar(mfrow = c(1, 3))\nplot(hclust(data.dist, method = \"complete\"),\n  labels = nci.labs,\n  main = \"Complete Linkage\", xlab = \"\", ylab = \"\", sub = \"\", cex = 0.6\n)\nplot(hclust(data.dist, method = \"average\"),\n  labels = nci.labs,\n  main = \"Average Linkage\", xlab = \"\", ylab = \"\", sub = \"\", cex = 0.6\n)\nplot(hclust(data.dist, method = \"single\"),\n  labels = nci.labs,\n  main = \"Single Linkage\", xlab = \"\", ylab = \"\", sub = \"\", cex = 0.6\n)\n\n\n\n# Plotting a complete linkage dendrogram cut at 4 clusters\nhcnci &lt;- hclust(data.dist, method = \"complete\")\npar(mfrow = c(1, 1))\nplot(hcnci, labels = nci.labs, cex = 0.6)\nabline(h = 139, col = \"red\")\n\n\n\n# Comparing clusters by hclust() with actual ones\nhc.clusters &lt;- cutree(hcnci, 4)\ntable(hc.clusters, nci.labs)\n\n           nci.labs\nhc.clusters BREAST CNS COLON K562A-repro K562B-repro LEUKEMIA MCF7A-repro\n          1      2   3     2           0           0        0           0\n          2      3   2     0           0           0        0           0\n          3      0   0     0           1           1        6           0\n          4      2   0     5           0           0        0           1\n           nci.labs\nhc.clusters MCF7D-repro MELANOMA NSCLC OVARIAN PROSTATE RENAL UNKNOWN\n          1           0        8     8       6        2     8       1\n          2           0        0     1       0        0     1       0\n          3           0        0     0       0        0     0       0\n          4           1        0     0       0        0     0       0\n\n# Printing output of a hclust() object\nhcnci\n\n\nCall:\nhclust(d = data.dist, method = \"complete\")\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 64 \n\n# Comparing hierarchical clustering with k-means clustering\nset.seed(3)\nkm.clusters &lt;- kmeans(sd.data, centers = 4, nstart = 20)$cluster\ntable(km.clusters, hc.clusters)\n\n           hc.clusters\nkm.clusters  1  2  3  4\n          1 20  7  0  0\n          2  9  0  0  0\n          3 11  0  0  9\n          4  0  0  8  0\n\n# Performing Hierarchical and K-Means clustering only on first 5 principal components\nhc.pr.nci &lt;- hclust(dist(scale(prnci$x[, 1:5])))\nhc.pr.clusters &lt;- cutree(hc.pr.nci, 4)\nkm.pr.clusters &lt;- kmeans(prnci$x[, 1:5], centers = 4, nstart = 20)$cluster\ntable(km.pr.clusters, hc.pr.clusters)\n\n              hc.pr.clusters\nkm.pr.clusters  1  2  3  4\n             1 24  3  0  0\n             2  0  0  5  2\n             3  6  4 11  0\n             4  9  0  0  0\n\n# Plotting dendrogram with 5-Principal Components Hierarchical Clustering\nplot(hc.pr.nci,\n  labels = nci.labs, cex = 0.6, xlab = \"\", ylab = \"\", sub = \"\",\n  main = \"Hierarchical Clustering on first 5 PCs\"\n)"
  }
]