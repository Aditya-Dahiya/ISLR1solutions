---
title: "Chapter 6 (Lab)"
author: "Aditya Dahiya"
subtitle: "Linear Model Selection and Regularization"
date: 2021-02-20
execute: 
  echo: true
  error: false
  warning: false
  cache: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

# Lab 1: Subset Selection Methods

## 6.5.1 Best Subset Selection

Load the libraries `ISLR` and `leaps` to perform Best Subset Selection
on `Hitters` data.

```{r}
library(ISLR)
library(leaps)
library(kableExtra)
data(Hitters)

# Examine the data set
dim(Hitters)
names(Hitters)

# Find and remove missing data
sum(is.na(Hitters))
Hitters <- na.omit(Hitters)
dim(Hitters)

# Create an Object with Best Subset Selection performed on Hitters Data set
regfit.trial <- regsubsets(Salary ~ ., data = Hitters)
# Examine contents of summary of the regsubsets output
names(regfit.trial)
names(summary(regfit.trial))

# Now use Best Subset Selection using all 19 variables
regfit.full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
summary(regfit.full)

# Display R-Squared, Adjusted R-Squared, Mallow's Cp and B.I.C from the best models
names(summary(regfit.full))
summary(regfit.full)$rsq
summary(regfit.full)$adjr2
summary(regfit.full)$cp
summary(regfit.full)$bic
```

Now, we plot the various statistics generated by the Best Subset
Selection using the `regsubsets()` function in the object `regfit.full`.

```{r}
par(mfrow = c(2, 2))
# Plotting Residual Sum of Squares
plot(summary(regfit.full)$rss,
  xlab = "Number of Variables",
  ylab = "Residual Sum of Squares", type = "l"
)
points(
  x = which.min(summary(regfit.full)$rss),
  y = min(summary(regfit.full)$rss),
  col = "red", cex = 2, pch = 20
)

# Plotting Adjusted R-Squared
plot(summary(regfit.full)$adjr2,
  xlab = "Number of variables",
  ylab = "Adjusted R-Squared", type = "l"
)
points(
  x = which.max(summary(regfit.full)$adjr2),
  y = max(summary(regfit.full)$adjr2),
  col = "red", cex = 2, pch = 20
)

# Plotting Mallow's Cp
plot(summary(regfit.full)$cp,
  xlab = "Number of variables",
  ylab = "Mallow's Cp", type = "l"
)
points(
  x = which.min(summary(regfit.full)$cp),
  y = min(summary(regfit.full)$cp),
  col = "red", cex = 2, pch = 20
)

# Plotting Bayesian Information Criterion (B.I.C)
plot(summary(regfit.full)$bic,
  xlab = "Number of variables",
  ylab = "B.I.C", type = "l"
)
points(
  x = which.min(summary(regfit.full)$bic),
  y = min(summary(regfit.full)$bic),
  col = "red", cex = 2, pch = 20
)
```

Now, we use the in-built `plot` function in the `regsubsets()` to
examine the models.

```{r}
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
coef(regfit.full, 6)
```

## 6.5.2 Forward and Backward Step-wise Selection

We can use the `regsubsets()` function to do forward and backward
selection as well. We use the same data set `Hitters` and compare the
selected models of 7 variables from the three approaches: (1) Best
Subset Selection (2) Forward Selection and (3) Backward Selection.

```{r}
# Forward Selection; Listing the variables in 7 variable model
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters, method = "forward")
names(coef(regfit.fwd, 7))

# Backward Selection; Listing out the variables selected in 7 variable model
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters, method = "backward")
names(coef(regfit.bwd, 7))

# Create a nice table to display and compare the coefficients
ComCoef7 <- data.frame(
  BestSubsetSelection = names(coef(regfit.full, 7)),
  ForwardSelection = names(coef(regfit.fwd, 7)),
  BackwardSelection = names(coef(regfit.bwd, 7))
)
ComCoef7
```

## 6.5.3 Choosing among Models using the Validation Set approach and Cross Validation

We create `test` and `train` boolean vectors to be used to subset the
`Hitters` data set into a training subset and a validation subset. The,
we use `regsubsets()` to find the Best Subset Selection model for
$k = 1 to p$ variables in the model. Finally, we will calculate the Mean
Squared Error (MSE) for each of the models. This will allow us to see
which model has the lowest MSE, i.e. Test Error on the Validation Set.

```{r}
# Creating Training and Test boolean vectors
set.seed(5)
train <- sample(c(TRUE, FALSE), size = nrow(Hitters), replace = TRUE)
test <- !train

# Run the Best Subset Selection on the training data
regfit.best <- regsubsets(Salary ~ ., data = Hitters[train, ], nvmax = 19)

# Creating a test matrix (X) with which we can multiply the coefficients of the
# best model to generate predicted values of Y (i.e. Salary). The model.matrix()
# creates matrices based on formulas (eg: including dummy variables etc.)
test.matrix <- model.matrix(Salary ~ ., data = Hitters[test, ])

# Create empty vector to store Validation Set Error Rates for each best model
ValSetErrors <- rep(NA, 19)

# Calculate Validation Set error rate for each model using loops
for (i in 1:19) {
  coeffs <- coef(regfit.best, id = i) # Extract coefficients in ith model
  pred <- test.matrix[, names(coeffs)] %*% coeffs # Calculate predicted Y
  ValSetErrors[i] <- mean((Hitters$Salary[test] - pred)^2)
}

# Find which Model has minimum MSE
which.min(ValSetErrors)

# Display the coefficients used in the model with minimum MSE
coef(regfit.best, id = which.min(ValSetErrors))

# Finally, we obtain coefficient estimates from running the best subset selection
# model on the complete data set. This will allow us to get better coefficient
# estimates.
regfit.best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
round(coef(regfit.best, id = 10),
  digits = 3
)
```

Now, we can create an automated function to calculate MSE for each model
of a `regsubsets()` object, so that we can use it in loops later when we
use Cross-Validation.

```{r}
predict.regsubsets <- function(object, newdata, id, ...){
  formula <- as.formula(object$call[[2]]) # Extracting formula from regsubsets object 
  coeffs <- coef(object, id) # Extracting coefficients with their names
  testmat <- model.matrix(formula, data = newdata) # Create test X matrix
  testmat[, names(coeffs)] %*% coeffs # Predicted values
}

```

We now use **Cross Validation**, instead of Validation Set approach to
find out the Test MSE.

```{r}
# Create k folds in the data-set for k-fold Cross Validation
k <- 10
set.seed(1)
folds <- sample(1:k, size = nrow(Hitters), replace = TRUE)
# table(folds) # To check : Each observation has been assigned one of the k folds

# Create a matrix to store k Test MSEs of each of the 19 models
cv.errors <- matrix(
  data = NA,
  nrow = k, ncol = 19,
  dimnames = list(NULL, 1:19)
)

# Create two loops : (1) Outer Loop to select the cross-validation k fold and
# then run regsubsets (2) Inner loop to find Test MSE of each on 1 to 19 variable
# models in the k-th test fold
for (j in 1:k) {
  reg.fit <- regsubsets(Salary ~ .,
    nvmax = 19,
    data = Hitters[folds != j, ]
  )
  for (i in 1:19) {
    pred <- predict.regsubsets(
      object = reg.fit, id = i,
      newdata = Hitters[folds == j, ]
    )
    cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2) # Store MSE
  }
}

# Calculate mean MSE for each of 1 to 19 variable models. Plot MSEs.
mean.cv.errors <- apply(cv.errors, MARGIN = 2, FUN = mean)
par(mfrow = c(1, 1))
plot(mean.cv.errors,
  type = "b",
  xlab = "Number of variables in the Model",
  ylab = paste(k, "-fold Cross-Validation MSE", collapse = "")
)
points(
  x = which.min(mean.cv.errors),
  y = min(mean.cv.errors),
  col = "red", pch = 20, cex = 2
)

# Now, find coefficients for best model selected by CV, using regsubsets() on
# full data set
round(coef(regsubsets(Salary ~ ., Hitters, nvmax = 19),
  id = which.min(mean.cv.errors)
), 2)
```

# Lab 2 : Ridge Regression and the Lasso

We need to load the `glmnet` library to use the `glmnet()` function
which can perform ridge regression and the Lasso. Further, we need to
create an `x` matrix and a `y` vector to use in the `glmnet()` function.

```{r, message=FALSE}
library(glmnet)
x = model.matrix(Salary ~ ., data = Hitters)[, -1] # remove intercept column with [,-1]
y = Hitters$Salary
```

## 6.6.1 Ridge Regression

We can fit ridge regression using `glmnet()` with `alpha = 0`. First, we
need to create a vector of possible values of lambda, `lamvec` to use in
`glmnet()`.

```{r}
lamvec = 10^seq(from = 10, to = -2, length = 100)

# Fitting ridge regression model
ridge.mod = glmnet(x, y, alpha = 0, lambda = lamvec)

# Examine some of the contents of a ridge regression object 
class(ridge.mod)
names(ridge.mod)

# Matrix containing the Coefficients for each value of lambda in a 20 X 100 matrix
dim(coef(ridge.mod))

# Examining the ell-2 norm of the coefficients at some lambda values to check the
# fact that at high lambda values, this ell-2 norm should be low (near zero)

# Get the 25th Lambda value i.e. a large lambda value (we expect ell-2 norm to be low)
ridge.mod$lambda[25]
sqrt(sum(coef(ridge.mod)[-1, 25]^2))

# Get the 75th Lambda value i.e. a small lambda value (we expect ell-2 norm to be high)
ridge.mod$lambda[75]
sqrt(sum(coef(ridge.mod)[-1, 75]^2))

# Plot the coefficients simply using plot() on the glmnet object
plot(ridge.mod)
```

We can now try to predict the coefficients for a fixed arbitrary value
of `lambda = 50`, which we may not have used in the original ridge
regression fitting using `glmnet()`.

```{r}
predict(ridge.mod, s = 50, type = "coefficients")
```

Now, which is the best value of `lambda` to use for predicting the
coefficients? We can answer this question using cross validation. First,
we split half of sample as training set, and the remaining half as test
set.

```{r}
# Setting seed for replicability, and creating training and testing sets
set.seed(1)
train = sample(c(TRUE, FALSE), size = nrow(x), replace = TRUE)
test = !train

# Running Ridge Regression on Training Set
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = lamvec, 
                   thresh = 1e-12)

# Calculate predicted MSE for some arbitrary lambda value, say 4
ridge.pred = predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y[test])^2)

# Calculate MSE for a very large lambda (i.e. NULL model, all coefficients nearly zero)
ridge.pred = predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y[test])^2)

# Re-check that this MSE is same as using mean of y as predicted value for all of test set
mean( (mean(y[train]) - y[test] )^2 )

# Both values are nearly the same. Further, at lambda = 4, MSE is lower than NULL model

# Calculate MSE for lambda = 0 (i.e. least squares regression)
# ridge.pred = predict(ridge.mod, x = x[test,], y = y[test],
#                      s = 0, newx = x[test,], 
#                      exact = T)
#mean((ridge.pred - y[test])^2)

# Verify that the MSE is same as least squares regression
# lmfit <- lm(Salary ~ ., Hitters, subset = train)
# lm.pred <- predict(lmfit, newdata = Hitters[test,])
# mean((lm.pred - y[test])^2)

```

Now, we use the cross-validation approach to select the `lambda` value
with the lowest MSE. The function `cv.glmnet()` automatically performs
this with a default value of `folds = 10`.

```{r}
set.seed(3)
cv.out = cv.glmnet(x[train, ], y[train], alpha = 0)

# Examine the output of cv.glmnet()
class(cv.out)
names(cv.out)

# Plotting the output of cv.glmnet()
plot(cv.out)

# Finding the test MSE associated with the best lambda value
bestlam = cv.out$lambda.min
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean( (ridge.pred - y[test])^2 )

# Finally, we find out the coefficients for all variables in Hitters for lambda 
# value of bestlam using the full dataset
out = glmnet(x, y, alpha = 0)
round(predict(out, type = "coefficients", s = bestlam),3)[1:20, ]
```

## 6.6.2 The Lasso

The Lasso can be run using the same function `glmnet()` but with the
argument `alpha = 1`.

```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = lamvec)
plot(lasso.mod)

# Using cross validation to find out the best lambda value
set.seed(3)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1, )
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

# Finding Lasso predicted values on test set using best lambda value
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])

# Calculating MSE at best lambda value with Lasso
mean((lasso.pred - y[test])^2)

# Using full data to find coefficient values at best lambda values
out <- glmnet(x, y, alpha = 1)
coeffs <- predict(out, s = bestlam, type = "coefficients")[1:20, ]
coeffs[coeffs != 0]
```

# Lab 3 : PCR and PLS Regression

## 6.7.1 Principal Components Regression

We can fit the Principal Components Regression using the `pcr()`
function which is a part `pls` library.

```{r}
library(pls)
library(ISLR)
set.seed(1)

# Recreating data sets once again
data("Hitters")
Hitters <- na.omit(Hitters)
train <- sample(1:nrow(Hitters), size = nrow(Hitters) / 2)
test <- -train
x <- model.matrix(Salary ~ ., data = Hitters)[, -1]
y <- Hitters$Salary

# Fitting Principal Components Regression on the Hitters data
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = "CV")

# Examining the pcr() object and its summary
class(pcr.fit)
names(pcr.fit)
summary(pcr.fit)

# Using validationplot() to see the results of pcr() - plots with RMSE and MSE
par(mfrow = c(1, 2))
validationplot(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")

# Performing PCR on training set and evaluating MSE on test set
pcr.fit <- pcr(Salary ~ .,
  data = Hitters, subset = train,
  scale = TRUE, validation = "CV"
)
validationplot(pcr.fit, val.type = "MSEP")

# Using ncomp = 7, to find predicted values and MSE
pcr.pred <- predict(pcr.fit, newdata = x[test, ], ncomp = 7)
mean((pcr.pred - y[test])^2)

# Finally, we fit the PCR model with M=7 to the entire data set
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 7)
summary(pcr.fit)
```

## 6.7.2 Partial Least Squares

We now implement the Partial Least Squares method using the `plsr()`
function in the `pls` library. The syntax is similar to the `pcr()`
function.

```{r}
pls.fit <- plsr(Salary ~ .,
  data = Hitters, subset = train,
  scale = TRUE, validation = "CV"
)
summary(pls.fit)

# Creating Validation Plots to see MSE or RMSE versus M-value
par(mfrow = c(1, 2))
validationplot(pls.fit)
validationplot(pls.fit, val.type = "MSEP")

# Lowest CV-MSE at M=2, so we compute Test MSE at M=2
pred.pls <- predict(pls.fit, newx = x[test, ], ncomp = 2)
mean((pred.pls - mean(y[test]))^2)

# Finally, perform PLS on the entire data set, with M=2
pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 2)
summary(pls.fit)
```
